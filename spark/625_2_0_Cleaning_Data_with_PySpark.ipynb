{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "From: https://github.com/ksatola\n",
    "Version: 1.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Data with PySpark\n",
    "\n",
    "Working with data is tricky - working with millions or even billions of rows is worse. Did you receive some data processing code written on a laptop with fairly pristine data? Chances are you’ve probably been put in charge of moving a basic data process from prototype to production. You may have worked with real world datasets, with missing fields, bizarre formatting, and orders of magnitude more data. Even if this is all new to you, this course helps you learn what’s needed to prepare data processes using Python with Apache Spark. You’ll learn terminology, methods, and some best practices to create a performant, maintainable, and understandable data processing platform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "- [Introduction](#intro)\n",
    "- [Manipulating DataFrames in the real wold](#man)\n",
    "- [Improving Performance](#perf)\n",
    "- [Introduction to data pipelines](#pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "path = \"data/dc33/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=local appName=First App>\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local\", \"First App\")\n",
    "print(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('First App').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='intro'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro to data cleaning with Apache Spark\n",
    "\n",
    "<img src=\"images/spark3_001.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark3_002.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark3_003.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark3_004.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark3_005.png\" alt=\"\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a schema\n",
    "\n",
    "Creating a defined schema helps with data quality and import performance. As mentioned during the lesson, we'll create a simple schema to read in the following columns:\n",
    "\n",
    "- Name\n",
    "- Age\n",
    "- City\n",
    "\n",
    "The Name and City columns are `StringType()` and the Age column is an `IntegerType()`.\n",
    "\n",
    "- Import * from the pyspark.sql.types library.\n",
    "- Define a new schema using the StructType method.\n",
    "- Define a StructField for name, age, and city. Each field should correspond to the correct datatype and not be nullable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the pyspark.sql.types library\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Define a new schema using the StructType method\n",
    "people_schema = StructType([\n",
    "  # Define a StructField for each field\n",
    "  StructField('name', StringType(), False),\n",
    "  StructField('age', IntegerType(), False),\n",
    "  StructField('city', StringType(), False)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Immutability and lazy processing\n",
    "\n",
    "<img src=\"images/spark3_006.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark3_007.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark3_008.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark3_009.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "Spark takes advantage of data immutability to efficiently share / create new data representations throughout the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using lazy processing\n",
    "\n",
    "`Lazy processing operations` will usually return in about the same amount of time regardless of the actual quantity of data. Remember that this is due to `Spark not performing any transformations until an action is requested`.\n",
    "\n",
    "For this exercise, we'll be defining a Data Frame (aa_dfw_df) and add a couple transformations. Note the amount of time required for the transformations to complete when defined vs when the data is actually queried. These differences may be short, but they will be noticeable. When working with a full Spark cluster with larger quantities of data the difference will be more apparent.\n",
    "\n",
    "- Load the Data Frame.\n",
    "- Add the transformation for F.lower() to the Destination Airport column.\n",
    "- Drop the Destination Airport column from the Data Frame aa_dfw_df. Note the time for these operations to complete.\n",
    "- Show the Data Frame, noting the time difference for this action to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------+-----------------------------+-------+\n",
      "|Date (MM/DD/YYYY)|Flight Number|Actual elapsed time (Minutes)|airport|\n",
      "+-----------------+-------------+-----------------------------+-------+\n",
      "|       01/01/2017|         0005|                          537|    hnl|\n",
      "|       01/01/2017|         0007|                          498|    ogg|\n",
      "|       01/01/2017|         0037|                          241|    sfo|\n",
      "|       01/01/2017|         0043|                          134|    dtw|\n",
      "|       01/01/2017|         0051|                           88|    stl|\n",
      "|       01/01/2017|         0060|                          149|    mia|\n",
      "|       01/01/2017|         0071|                          203|    lax|\n",
      "|       01/01/2017|         0074|                           76|    mem|\n",
      "|       01/01/2017|         0081|                          123|    den|\n",
      "|       01/01/2017|         0089|                          161|    slc|\n",
      "|       01/01/2017|         0096|                           84|    stl|\n",
      "|       01/01/2017|         0103|                          216|    sjc|\n",
      "|       01/01/2017|         0119|                          514|    ogg|\n",
      "|       01/01/2017|         0123|                          529|    hnl|\n",
      "|       01/01/2017|         0126|                          171|    lga|\n",
      "|       01/01/2017|         0132|                          188|    ewr|\n",
      "|       01/01/2017|         0140|                          231|    sjc|\n",
      "|       01/01/2017|         0174|                          145|    rdu|\n",
      "|       01/01/2017|         0176|                          184|    bos|\n",
      "|       01/01/2017|         0190|                           76|    sat|\n",
      "+-----------------+-------------+-----------------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Load the CSV file\n",
    "aa_dfw_df = spark.read.format('csv').options(Header=True).load(path+'AA_DFW_2017_Departures_Short.csv.gz')\n",
    "\n",
    "# Add the airport column using the F.lower() method\n",
    "aa_dfw_df = aa_dfw_df.withColumn('airport', F.lower(aa_dfw_df['Destination Airport']))\n",
    "\n",
    "# Drop the Destination Airport column\n",
    "aa_dfw_df = aa_dfw_df.drop(aa_dfw_df['Destination Airport'])\n",
    "\n",
    "# Show the DataFrame\n",
    "aa_dfw_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You've just seen how lazy processing works in action. Remember when working with Spark that no transformations take effect until you apply an action. This can be confusing at times, but is one of the underpinnings of Spark's power."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Parquet\n",
    "\n",
    "<img src=\"images/spark3_010.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark3_011.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark3_012.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark3_013.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark3_014.png\" alt=\"\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving a DataFrame in Parquet format\n",
    "\n",
    "When working with Spark, you'll often start with CSV, JSON, or other data sources. This provides a lot of flexibility for the types of data to load, but it is not an optimal format for Spark. The `Parquet format` is a columnar data store, allowing Spark to use `predicate pushdown`. This means Spark will only process the data necessary to complete the operations you define versus reading the entire dataset. This gives Spark more flexibility in accessing the data and often drastically improves performance on large datasets.\n",
    "\n",
    "In this exercise, we're going to practice creating a new Parquet file and then process some data from it.\n",
    "\n",
    "The spark object and the df1 and df2 DataFrames have been setup for you.\n",
    "\n",
    "- View the row count of df1 and df2.\n",
    "- Combine df1 and df2 in a new DataFrame named df3 with the union method.\n",
    "- Save df3 to a parquet file named AA_DFW_ALL.parquet.\n",
    "- Read the AA_DFW_ALL.parquet file and show the count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the row count of df1 and df2\n",
    "print(\"df1 Count: %d\" % df1.count())\n",
    "print(\"df2 Count: %d\" % df2.count())\n",
    "\n",
    "# Combine the DataFrames into one\n",
    "df3 = df1.union(df2)\n",
    "\n",
    "# Save the df3 DataFrame in Parquet format\n",
    "df3.write.parquet('AA_DFW_ALL.parquet', mode='overwrite')\n",
    "\n",
    "# Read the Parquet file into a new DataFrame and run a count\n",
    "print(spark.read.parquet('AA_DFW_ALL.parquet').count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<script.py> output:\n",
    "    df1 Count: 139359\n",
    "    df2 Count: 119911\n",
    "    259270\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL and Parquet\n",
    "\n",
    "`Parquet files are perfect as a backing data store for SQL queries in Spark`. While it is possible to run the same queries directly via Spark's Python functions, sometimes it's easier to run SQL queries alongside the Python options.\n",
    "\n",
    "For this example, we're going to read in the Parquet file we created in the last exercise and register it as a SQL table. Once registered, we'll run a quick query against the table (aka, the Parquet file).\n",
    "\n",
    "The spark object and the AA_DFW_ALL.parquet file are available for you automatically.\n",
    "\n",
    "- Import the AA_DFW_ALL.parquet file into flights_df.\n",
    "- Use the createOrReplaceTempView method to alias the flights table.\n",
    "- Run a Spark SQL query against the flights table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Parquet file into flights_df\n",
    "flights_df = spark.read.parquet('AA_DFW_ALL.parquet')\n",
    "\n",
    "# Register the temp table\n",
    "flights_df.createOrReplaceTempView('flights')\n",
    "\n",
    "# Run a SQL query of the average flight duration\n",
    "avg_duration = spark.sql('SELECT avg(flight_duration) from flights').collect()[0]\n",
    "print('The average flight time is: %d' % avg_duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<script.py> output:\n",
    "    The average flight time is: 151\n",
    "```\n",
    "You've just run a SQL query against a Parquet data source. When building production Spark code, you'll often port SQL operations directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='man'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manipulating DataFrames in the real wold\n",
    "\n",
    "## DataFrame column operations\n",
    "\n",
    "<img src=\"images/spark3_015.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark3_016.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark3_017.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark3_018.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark3_019.png\" alt=\"\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering column content with Python\n",
    "\n",
    "You've looked at using various operations on DataFrame columns - now you can modify a real dataset. The DataFrame `voter_df` contains information regarding the voters on the Dallas City Council from the past few years. This truncated DataFrame contains the date of the vote being cast and the name and position of the voter. Your manager has asked you to clean this data so it can later be integrated into some desired reports. The primary task is to remove any null entries or odd characters and return a specific set of voters where you can validate their information.\n",
    "\n",
    "This is often one of the first steps in data cleaning - removing anything that is obviously outside the format. For this dataset, make sure to look at the original data and see what looks out of place for the VOTER_NAME column.\n",
    "\n",
    "The `pyspark.sql.functions` library is already imported under the alias `F`.\n",
    "\n",
    "- Show the distinct VOTER_NAME entries.\n",
    "- Filter voter_df where the VOTER_NAME is 1-20 characters in length.\n",
    "- Filter out voter_df where the VOTER_NAME contains an _.\n",
    "- Show the distinct VOTER_NAME entries again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the distinct VOTER_NAME entries\n",
    "voter_df.select('VOTER_NAME').distinct().show(40, truncate=False)\n",
    "\n",
    "# Filter voter_df where the VOTER_NAME is 1-20 characters in length\n",
    "voter_df = voter_df.filter('length(VOTER_NAME) > 0 and length(VOTER_NAME) < 20')\n",
    "\n",
    "# Filter out voter_df where the VOTER_NAME contains an underscore\n",
    "voter_df = voter_df.filter(~ F.col('VOTER_NAME').contains('_'))\n",
    "\n",
    "# Show the distinct VOTER_NAME entries again\n",
    "voter_df.select('VOTER_NAME').distinct().show(40, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modifying DataFrame columns\n",
    "\n",
    "Previously, you filtered out any rows that didn't conform to something generally resembling a name. Now based on your earlier work, your manager has asked you to create two new columns - first_name and last_name. She asks you to split the VOTER_NAME column into words on any space character. You'll treat the last word as the last_name, and all other words as the first_name. You'll be using some new functions in this exercise including `.split()`, `.size()`, and `.getItem()`.\n",
    "\n",
    "Please note that these operations are always somewhat specific to the use case. Having your data conform to a format often matters more than the specific details of the format. Rarely is a data cleaning task meant just for one person - matching a defined format allows for easier sharing of the data later (ie, Paul doesn't need to worry about names - Mary already cleaned the dataset).\n",
    "\n",
    "The filtered voter DataFrame from your previous exercise is available as voter_df. The pyspark.sql.functions library is available under the alias F.\n",
    "\n",
    "- Add a new column called splits holding the list of possible names.\n",
    "- Use the getItem() method and create a new column called first_name.\n",
    "- Get the last entry of the splits list and create a column called last_name.\n",
    "- Drop the splits column and show the new voter_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new column called splits separated on whitespace\n",
    "voter_df = voter_df.withColumn('splits', F.split(voter_df.VOTER_NAME, '\\s+'))\n",
    "\n",
    "# Create a new column called first_name based on the first item in splits\n",
    "voter_df = voter_df.withColumn('first_name', voter_df.splits.getItem(0))\n",
    "\n",
    "# Get the last entry of the splits list and create a column called last_name\n",
    "voter_df = voter_df.withColumn('last_name', voter_df.splits.getItem(F.size('splits') - 1))\n",
    "\n",
    "# Drop the splits column\n",
    "voter_df = voter_df.drop('splits')\n",
    "\n",
    "# Show the voter_df DataFrame\n",
    "voter_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This exercise requires some creative thought to determine how best to handle the VOTER_NAME column to get the desired result. You may be wondering about the middle initial. We've left it out of this exercise for clarity, but consider the various ways you could add it to a given column. The string processing functions in Spark allow a wide range of operations to suit most requirements. Always refer to the Spark documentation when you need to modify a string column: you may be surprised at the available options!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional DataFrame column operations\n",
    "\n",
    "<img src=\"images/spark3_020.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark3_021.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark3_022.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark3_023.png\" alt=\"\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## when() example\n",
    "\n",
    "The `when()` clause lets you conditionally modify a Data Frame based on its content. You'll want to modify our voter_df DataFrame to add a random number to any voting member that is defined as a \"Councilmember\".\n",
    "\n",
    "The voter_df DataFrame is defined and available to you. The pyspark.sql.functions library is available as F. You can use F.rand() to generate the random value.\n",
    "\n",
    "- Add a column to voter_df named random_val with the results of the `F.rand()` method for any voter with the title Councilmember.\n",
    "- Show some of the DataFrame rows, noting whether the `.when()` clause worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column to voter_df for any voter with the title **Councilmember**\n",
    "voter_df = voter_df.withColumn('random_val',\n",
    "                               when(voter_df.TITLE == 'Councilmember', F.rand()))\n",
    "\n",
    "# Show some of the DataFrame rows, noting whether the when clause worked\n",
    "voter_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When / Otherwise\n",
    "\n",
    "This requirement is similar to the last, but now you want to add multiple values based on the voter's position. Modify your voter_df DataFrame to add a random number to any voting member that is defined as a Councilmember. Use 2 for the Mayor and 0 for anything other position.\n",
    "\n",
    "The voter_df Data Frame is defined and available to you. The pyspark.sql.functions library is available as F. You can use F.rand() to generate the random value.\n",
    "\n",
    "- Add a column to voter_df named random_val with the results of the F.rand() method for any voter with the title Councilmember. Set random_val to 2 for the Mayor. Set any other title to the value 0.\n",
    "- Show some of the Data Frame rows, noting whether the clauses worked.\n",
    "- Use the .filter clause to find 0 in random_val."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column to voter_df for a voter based on their position\n",
    "voter_df = voter_df.withColumn('random_val',\n",
    "                               when(voter_df.TITLE == 'Councilmember', F.rand())\n",
    "                               .when(voter_df.TITLE == 'Mayor', 2)\n",
    "                               .otherwise(0))\n",
    "\n",
    "# Show some of the DataFrame rows\n",
    "voter_df.show()\n",
    "\n",
    "# Use the .filter() clause with random_val\n",
    "voter_df.filter(voter_df.random_val == 0).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You've successfully used multiple when clauses and the otherwise clause to modify a Data Frame. When clauses can be useful for changing errant data in your Data Frames without extensive work. Make sure to consider using when / otherwise if you ever need to perform conditional steps on your data cleaning processes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User defined functions\n",
    "\n",
    "<img src=\"images/spark3_024.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark3_025.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark3_026.png\" alt=\"\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using user defined functions in Spark\n",
    "\n",
    "You've seen some of the power behind Spark's built-in string functions when it comes to manipulating DataFrames. However, once you reach a certain point, it becomes difficult to process the data in a without creating a rat's nest of function calls. Here's one place where you can use User Defined Functions to manipulate our DataFrames.\n",
    "\n",
    "For this exercise, we'll use our voter_df DataFrame, but you're going to replace the first_name column with the first and middle names.\n",
    "\n",
    "The pyspark.sql.functions library is available under the alias F. The classes from pyspark.sql.types are already imported.\n",
    "\n",
    "- Edit the getFirstAndMiddle() function to return a space separated string of names, except the last entry in the names list.\n",
    "- Define the function as a user-defined function. It should return a string type.\n",
    "- Create a new column on voter_df called first_and_middle_name using your UDF.\n",
    "- Drop the \"first_name\" and \"splits\" columns (on separate lines), then show the Data Frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFirstAndMiddle(names):\n",
    "  # Return a space separated string of names\n",
    "  return ' '.join(names[-1])\n",
    "\n",
    "# Define the method as a UDF\n",
    "udfFirstAndMiddle = F.udf(getFirstAndMiddle, StringType())\n",
    "\n",
    "# Create a new column using your UDF\n",
    "voter_df = voter_df.withColumn('first_and_middle_name', udfFirstAndMiddle(voter_df.splits))\n",
    "\n",
    "# Drop the unecessary columns then show the DataFrame\n",
    "voter_df = voter_df.drop('first_name')\n",
    "voter_df = voter_df.drop('splits')\n",
    "voter_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partitioning and lazy processing\n",
    "\n",
    "<img src=\"images/spark3_027.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark3_028.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark3_029.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark3_030.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark3_031.png\" alt=\"\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding an ID Field\n",
    "\n",
    "When working with data, you sometimes only want to access certain fields and perform various operations. In this case, find all the unique voter names from the DataFrame and add a unique ID number. Remember that `Spark IDs are assigned based on the DataFrame partition - as such the ID values may be much greater than the actual number of rows in the DataFrame`.\n",
    "\n",
    "With Spark's lazy processing, `the IDs are not actually generated until an action is performed and can be somewhat random depending on the size of the dataset`.\n",
    "\n",
    "The spark session and a Spark DataFrame df containing the DallasCouncilVotes.csv.gz file are available in your workspace. The pyspark.sql.functions library is available under the alias F.\n",
    "\n",
    "- Select the unique entries from the column VOTER NAME and create a new DataFrame called voter_df.\n",
    "- Count the rows in the voter_df DataFrame.\n",
    "- Add a ROW_ID column using Spark's id function.\n",
    "- Show the rows with the 10 highest ROW_IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select all the unique council voters\n",
    "voter_df = df.select(df[\"VOTER NAME\"]).distinct()\n",
    "\n",
    "# Count the rows in voter_df\n",
    "print(\"\\nThere are %d rows in the voter_df DataFrame.\\n\" % voter_df.count())\n",
    "\n",
    "# Add a ROW_ID\n",
    "voter_df = voter_df.withColumn('ROW_ID', F.monotonically_increasing_id())\n",
    "\n",
    "# Show the rows with 10 highest IDs in the set\n",
    "voter_df.orderBy(voter_df.ROW_ID.desc()).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IDs with different partitions\n",
    "\n",
    "You've just completed adding an ID field to a DataFrame. Now, take a look at what happens when you do the same thing on DataFrames containing a different number of partitions.\n",
    "\n",
    "To check the number of partitions, use the method `.rdd.getNumPartitions()` on a DataFrame.\n",
    "\n",
    "The spark session and two DataFrames, voter_df and voter_df_single, are available in your workspace. The instructions will help you discover the difference between the DataFrames. The pyspark.sql.functions library is available under the alias F.\n",
    "\n",
    "- Print the number of partitions on each DataFrame.\n",
    "- Add a ROW_ID field to each DataFrame.\n",
    "- Show the top 10 IDs in each DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the number of partitions in each DataFrame\n",
    "print(\"\\nThere are %d partitions in the voter_df DataFrame.\\n\" % voter_df.rdd.getNumPartitions())\n",
    "print(\"\\nThere are %d partitions in the voter_df_single DataFrame.\\n\" % voter_df_single.rdd.getNumPartitions())\n",
    "\n",
    "# Add a ROW_ID field to each DataFrame\n",
    "voter_df = voter_df.withColumn('ROW_ID', F.monotonically_increasing_id())\n",
    "voter_df_single = voter_df_single.withColumn('ROW_ID', F.monotonically_increasing_id())\n",
    "\n",
    "# Show the top 10 IDs in each DataFrame \n",
    "voter_df.orderBy(voter_df.ROW_ID.desc()).show(10)\n",
    "voter_df_single.orderBy(voter_df_single.ROW_ID.desc()).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<script.py> output:\n",
    "    \n",
    "    There are 200 partitions in the voter_df DataFrame.\n",
    "    \n",
    "    \n",
    "    There are 1 partitions in the voter_df_single DataFrame.\n",
    "    \n",
    "    +--------------------+-------------+\n",
    "    |          VOTER NAME|       ROW_ID|\n",
    "    +--------------------+-------------+\n",
    "    |        Lee Kleinman|1709396983808|\n",
    "    |  the  final  201...|1700807049217|\n",
    "    |         Erik Wilson|1700807049216|\n",
    "    |  the  final   20...|1683627180032|\n",
    "    | Carolyn King Arnold|1632087572480|\n",
    "    | Rickey D.  Callahan|1597727834112|\n",
    "    |   the   final  2...|1443109011456|\n",
    "    |    Monica R. Alonzo|1382979469312|\n",
    "    |     Lee M. Kleinman|1228360646656|\n",
    "    |   Jennifer S. Gates|1194000908288|\n",
    "    +--------------------+-------------+\n",
    "    only showing top 10 rows\n",
    "    \n",
    "    +--------------------+------+\n",
    "    |          VOTER NAME|ROW_ID|\n",
    "    +--------------------+------+\n",
    "    |        Lee Kleinman|    35|\n",
    "    |  the  final  201...|    34|\n",
    "    |         Erik Wilson|    33|\n",
    "    |  the  final   20...|    32|\n",
    "    | Carolyn King Arnold|    31|\n",
    "    | Rickey D.  Callahan|    30|\n",
    "    |   the   final  2...|    29|\n",
    "    |    Monica R. Alonzo|    28|\n",
    "    |     Lee M. Kleinman|    27|\n",
    "    |   Jennifer S. Gates|    26|\n",
    "    +--------------------+------+\n",
    "    only showing top 10 rows\n",
    "```\n",
    "Notice the drastic difference in the 'ROW_ID' values between the two Data Frames. Understanding how lazy processing and partitioning behave are integral to mastering Spark. Make sure to always test your assumptions when creating a Spark workflow to avoid nasty suprises in production."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More ID tricks\n",
    "\n",
    "Once you define a Spark process, you'll likely want to use it many times. Depending on your needs, you may want to start your IDs at a certain value so there isn't overlap with previous runs of the Spark task. This behavior is similar to how IDs would behave in a relational database. You have been given the task to make sure that the IDs output from a monthly Spark task start at the highest value from the previous month.\n",
    "\n",
    "The spark session and two DataFrames, voter_df_march and voter_df_april, are available in your workspace. The pyspark.sql.functions library is available under the alias F.\n",
    "\n",
    "- Determine the highest ROW_ID in voter_df_march and save it in the variable previous_max_ID. The statement .rdd.max()[0] will get the maximum ID.\n",
    "- Add a ROW_ID column to voter_df_april starting at the value of previous_max_ID.\n",
    "- Show the ROW_ID's from both Data Frames and compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the highest ROW_ID and save it in previous_max_ID\n",
    "previous_max_ID = voter_df_march.select('ROW_ID').rdd.max()[0]\n",
    "\n",
    "# Add a ROW_ID column to voter_df_april starting at the desired value\n",
    "voter_df_april = voter_df_april.withColumn('ROW_ID', F.monotonically_increasing_id() + previous_max_ID)\n",
    "\n",
    "# Show the ROW_ID from both DataFrames and compare\n",
    "voter_df_march.select('ROW_ID').show()\n",
    "voter_df_april.select('ROW_ID').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<script.py> output:\n",
    "    +-------------+\n",
    "    |       ROW_ID|\n",
    "    +-------------+\n",
    "    |   8589934592|\n",
    "    |  25769803776|\n",
    "    |  34359738368|\n",
    "    |  42949672960|\n",
    "    |  51539607552|\n",
    "    | 103079215104|\n",
    "    | 111669149696|\n",
    "    | 231928233984|\n",
    "    | 240518168576|\n",
    "    | 360777252864|\n",
    "    | 395136991232|\n",
    "    | 601295421440|\n",
    "    | 635655159808|\n",
    "    | 670014898176|\n",
    "    | 807453851648|\n",
    "    | 850403524608|\n",
    "    | 944892805120|\n",
    "    | 962072674304|\n",
    "    |1005022347264|\n",
    "    |1047972020224|\n",
    "    +-------------+\n",
    "    only showing top 20 rows\n",
    "    \n",
    "    +-------------+\n",
    "    |       ROW_ID|\n",
    "    +-------------+\n",
    "    |1717986918400|\n",
    "    |1735166787584|\n",
    "    |1743756722176|\n",
    "    |1752346656768|\n",
    "    |1760936591360|\n",
    "    |1812476198912|\n",
    "    |1821066133504|\n",
    "    |1941325217792|\n",
    "    |1949915152384|\n",
    "    |2070174236672|\n",
    "    |2104533975040|\n",
    "    |2310692405248|\n",
    "    |2345052143616|\n",
    "    |2379411881984|\n",
    "    |2516850835456|\n",
    "    |2559800508416|\n",
    "    |2654289788928|\n",
    "    |2671469658112|\n",
    "    |2714419331072|\n",
    "    |2757369004032|\n",
    "    +-------------+\n",
    "    only showing top 20 rows\n",
    "```\n",
    "It's easy to forget that the output of a Spark method can often be modified before being assigned. This provides a lot of power and flexibility, especially when trying to migrate tasks from various technologies. Consider how you could use everything we've learned in this chapter to create a combination ID containing a name, a new ID, and perhaps a conditional value. When you are able to view your tasks as compositions of available functions, you can clean and modify your data in any way you see fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='perf'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving Performance\n",
    "\n",
    "<img src=\"images/spark3_032.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark3_033.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark3_034.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark3_035.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark3_036.png\" alt=\"\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caching a DataFrame\n",
    "\n",
    "You've been assigned a task that requires running several analysis operations on a DataFrame. You've learned that caching can improve performance when reusing DataFrames and would like to implement it.\n",
    "\n",
    "You'll be working with a new dataset consisting of airline departure information. It may have repetitive data and will need to be de-duplicated.\n",
    "\n",
    "The DataFrame departures_df is defined, but no actions have been performed.\n",
    "\n",
    "- Cache the unique rows in the departures_df DataFrame.\n",
    "- Perform a count query on departures_df, noting how long the operation takes.\n",
    "- Count the rows again, noting the variance in time of a cached DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Add caching to the unique rows in departures_df\n",
    "departures_df = departures_df.distinct().cache()\n",
    "\n",
    "# Count the unique rows in departures_df, noting how long the operation takes\n",
    "print(\"Counting %d rows took %f seconds\" % (departures_df.count(), time.time() - start_time))\n",
    "\n",
    "# Count the rows again, noting the variance in time of a cached DataFrame\n",
    "start_time = time.time()\n",
    "print(\"Counting %d rows again took %f seconds\" % (departures_df.count(), time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<script.py> output:\n",
    "    Counting 139358 rows took 1.848280 seconds\n",
    "    Counting 139358 rows again took 0.987845 seconds\n",
    "```\n",
    "You've successfully implemented caching on a DataFrame. Consider why the first run takes longer even though you've told it to `cache()` the DataFrame. Remember that even though you've applied the caching transformation, it doesn't take effect until an action is run. The action instantiates the caching after the `distinct()` function completes. The second time, there is no need to recalculate anything so it returns almost immediately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing a DataFrame from cache\n",
    "\n",
    "You've finished the analysis tasks with the departures_df DataFrame, but have some other processing to do. You'd like to remove the DataFrame from the cache to prevent any excess memory usage on your cluster.\n",
    "\n",
    "The DataFrame departures_df is defined and has already been cached for you.\n",
    "\n",
    "- Check the caching status on the departures_df DataFrame.\n",
    "- Remove the departures_df DataFrame from the cache.\n",
    "- Validate the caching status again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine if departures_df is in the cache\n",
    "print(\"Is departures_df cached?: %s\" % departures_df.is_cached)\n",
    "print(\"Removing departures_df from cache\")\n",
    "\n",
    "# Remove departures_df from the cache\n",
    "departures_df.unpersist()\n",
    "\n",
    "# Check the cache status again\n",
    "print(\"Is departures_df cached?: %s\" % departures_df.is_cached)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<script.py> output:\n",
    "    Is departures_df cached?: True\n",
    "    Removing departures_df from cache\n",
    "    Is departures_df cached?: False\n",
    "```\n",
    "You've removed the DataFrame from cache using `.unpersist()`. This is a fairly simple operation but can be very useful on long running or complex Spark jobs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improve import performance\n",
    "\n",
    "<img src=\"images/spark3_037.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark3_038.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark3_039.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark3_040.png\" alt=\"\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File import performance\n",
    "\n",
    "You've been given a large set of data to import into a Spark DataFrame. You'd like to test the difference in import speed by splitting up the file.\n",
    "\n",
    "You have two types of files available: departures_full.txt.gz and departures_xxx.txt.gz where xxx is 000 - 013. The same number of rows is split between each file.\n",
    "\n",
    "- Import the departures_full.txt.gz file and the departures_xxx.txt.gz files into separate DataFrames.\n",
    "- Run a count on each DataFrame and compare the run times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the full and split files into DataFrames\n",
    "full_df = spark.read.csv('departures_full.txt.gz')\n",
    "split_df = spark.read.csv('departures_0*.txt.gz')\n",
    "\n",
    "# Print the count and run time for each DataFrame\n",
    "start_time_a = time.time()\n",
    "print(\"Total rows in full DataFrame:\\t%d\" % full_df.count())\n",
    "print(\"Time to run: %f\" % (time.time() - start_time_a))\n",
    "\n",
    "start_time_b = time.time()\n",
    "print(\"Total rows in split DataFrame:\\t%d\" % split_df.count())\n",
    "print(\"Time to run: %f\" % (time.time() - start_time_b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<script.py> output:\n",
    "    Total rows in full DataFrame:\t139359\n",
    "    Time to run: 0.755548\n",
    "    Total rows in split DataFrame:\t139359\n",
    "    Time to run: 0.269601\n",
    "```\n",
    "The results should illustrate that using split files runs more quickly than using one large file for import. Note that in certain circumstances the results may be reversed. This is a side effect of running as a single node cluster. Depending on the tasks required and resources available, it may occasionally take longer than expected. If you perform multiple runs of the tasks, you should see the full file import as generally slower than the split file import."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster configurations\n",
    "\n",
    "<img src=\"images/spark3_041.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark3_042.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark3_043.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark3_044.png\" alt=\"\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Spark configurations\n",
    "\n",
    "You've recently configured a cluster via a cloud provider. Your only access is via the command shell or your python code. You'd like to verify some Spark settings to validate the configuration of the cluster.\n",
    "\n",
    "The spark object is available for use.\n",
    "\n",
    "- Check the name of the Spark application instance ('spark.app.name').\n",
    "- Determine the TCP port the driver runs on ('spark.driver.port').\n",
    "- Determine how many partitions are configured for joins.\n",
    "- Show the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of the Spark application instance\n",
    "app_name = spark.conf.get('spark.app.name')\n",
    "\n",
    "# Driver TCP port\n",
    "driver_tcp_port = spark.conf.get('spark.driver.port')\n",
    "\n",
    "# Number of join partitions\n",
    "num_partitions = spark.conf.get('spark.sql.shuffle.partitions')\n",
    "\n",
    "# Show the results\n",
    "print(\"Name: %s\" % app_name)\n",
    "print(\"Driver TCP port: %s\" % driver_tcp_port)\n",
    "print(\"Number of partitions: %s\" % num_partitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<script.py> output:\n",
    "    Name: pyspark-shell\n",
    "    Driver TCP port: 44415\n",
    "    Number of partitions: 200\n",
    "```\n",
    "Using the `spark.conf` object allows you to validate the settings of a cluster without having configured it initially. This can help you know what changes should be optimized for your needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing Spark configurations\n",
    "\n",
    "Now that you've reviewed some of the Spark configurations on your cluster, you want to modify some of the settings to tune Spark to your needs. You'll import some data to review that your changes have affected the cluster.\n",
    "\n",
    "The spark configuration is initially set to the default value of 200 partitions.\n",
    "\n",
    "The spark object is available for use. A file named departures.txt.gz is available for import. An initial DataFrame containing the distinct rows from departures.txt.gz is available as departures_df.\n",
    "\n",
    "- Store the number of partitions in departures_df in the variable before.\n",
    "- Change the spark.sql.shuffle.partitions configuration to 500 partitions.\n",
    "- Recreate the departures_df DataFrame reading the distinct rows from the departures file.\n",
    "- Print the number of partitions from before and after the configuration change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the number of partitions in variable\n",
    "before = departures_df.rdd.getNumPartitions()\n",
    "\n",
    "# Configure Spark to use 500 partitions\n",
    "spark.conf.set('spark.sql.shuffle.partitions', 500)\n",
    "\n",
    "# Recreate the DataFrame using the departures data file\n",
    "departures_df = spark.read.csv('departures.txt.gz').distinct()\n",
    "\n",
    "# Print the number of partitions for each instance\n",
    "print(\"Partition count before change: %d\" % before)\n",
    "print(\"Partition count after change: %d\" % departures_df.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<script.py> output:\n",
    "    Partition count before change: 200\n",
    "    Partition count after change: 500\n",
    "```\n",
    "It's important to remember that modifying the settings in Spark may change objects that already exist. Sometimes the changes only take effect after configuring a new DataFrame. Remember to test changes you make to Spark configurations to verify it does exactly what you think."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance improvements\n",
    "\n",
    "<img src=\"images/spark3_045.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark3_046.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark3_047.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark3_048.png\" alt=\"\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal joins\n",
    "\n",
    "You've been given two DataFrames to combine into a single useful DataFrame. Your first task is to combine the DataFrames normally and view the execution plan.\n",
    "\n",
    "The DataFrames flights_df and airports_df are available to you.\n",
    "\n",
    "- Create a new DataFrame normal_df by joining flights_df with airports_df.\n",
    "- Determine which type of join is used in the query plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the flights_df and aiports_df DataFrames\n",
    "normal_df = flights_df.join(airports_df, \\\n",
    "    flights_df[\"Destination Airport\"] == airports_df[\"IATA\"] )\n",
    "\n",
    "# Show the query plan\n",
    "normal_df.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<script.py> output:\n",
    "    == Physical Plan ==\n",
    "    *(5) SortMergeJoin [Destination Airport#192], [IATA#209], Inner\n",
    "    :- *(2) Sort [Destination Airport#192 ASC NULLS FIRST], false, 0\n",
    "    :  +- Exchange hashpartitioning(Destination Airport#192, 500)\n",
    "    :     +- *(1) Project [Date (MM/DD/YYYY)#190, Flight Number#191, Destination Airport#192, Actual elapsed time (Minutes)#193]\n",
    "    :        +- *(1) Filter isnotnull(Destination Airport#192)\n",
    "    :           +- *(1) FileScan csv [Date (MM/DD/YYYY)#190,Flight Number#191,Destination Airport#192,Actual elapsed time (Minutes)#193] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/tmp/tmprhigmoha/AA_DFW_2018_Departures_Short.csv.gz], PartitionFilters: [], PushedFilters: [IsNotNull(Destination Airport)], ReadSchema: struct<Date (MM/DD/YYYY):string,Flight Number:string,Destination Airport:string,Actual elapsed ti...\n",
    "    +- *(4) Sort [IATA#209 ASC NULLS FIRST], false, 0\n",
    "       +- Exchange hashpartitioning(IATA#209, 500)\n",
    "          +- *(3) Project [AIRPORTNAME#208, IATA#209]\n",
    "             +- *(3) Filter isnotnull(IATA#209)\n",
    "                +- *(3) FileScan csv [AIRPORTNAME#208,IATA#209] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/tmp/tmprhigmoha/airportnames.txt.gz], PartitionFilters: [], PushedFilters: [IsNotNull(IATA)], ReadSchema: struct<AIRPORTNAME:string,IATA:string>\n",
    "```\n",
    "You've implemented a basic join and examined the query plan. Learning to parse a query plan will help you understand what Spark is doing and when."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using broadcasting on Spark joins\n",
    "\n",
    "Remember that table joins in Spark are split between the cluster workers. If the data is not local, various shuffle operations are required and can have a negative impact on performance. Instead, we're going to use Spark's broadcast operations to give each node a copy of the specified data.\n",
    "\n",
    "A couple tips:\n",
    "\n",
    "- Broadcast the smaller DataFrame. The larger the DataFrame, the more time required to transfer to the worker nodes.\n",
    "- On small DataFrames, it may be better skip broadcasting and let Spark figure out any optimization on its own.\n",
    "- If you look at the query execution plan, a broadcastHashJoin indicates you've successfully configured broadcasting.\n",
    "\n",
    "The DataFrames flights_df and airports_df are available to you.\n",
    "\n",
    "- Import the broadcast() method from pyspark.sql.functions.\n",
    "- Create a new DataFrame broadcast_df by joining flights_df with airports_df, using the broadcasting.\n",
    "- Show the query plan and consider differences from the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the broadcast method from pyspark.sql.functions\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "# Join the flights_df and airports_df DataFrames using broadcasting\n",
    "broadcast_df = flights_df.join(broadcast(airports_df), \\\n",
    "    flights_df[\"Destination Airport\"] == airports_df[\"IATA\"] )\n",
    "\n",
    "# Show the query plan and compare against the original\n",
    "broadcast_df.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<script.py> output:\n",
    "    == Physical Plan ==\n",
    "    *(2) BroadcastHashJoin [Destination Airport#242], [IATA#259], Inner, BuildRight\n",
    "    :- *(2) Project [Date (MM/DD/YYYY)#240, Flight Number#241, Destination Airport#242, Actual elapsed time (Minutes)#243]\n",
    "    :  +- *(2) Filter isnotnull(Destination Airport#242)\n",
    "    :     +- *(2) FileScan csv [Date (MM/DD/YYYY)#240,Flight Number#241,Destination Airport#242,Actual elapsed time (Minutes)#243] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/tmp/tmprhigmoha/AA_DFW_2018_Departures_Short.csv.gz], PartitionFilters: [], PushedFilters: [IsNotNull(Destination Airport)], ReadSchema: struct<Date (MM/DD/YYYY):string,Flight Number:string,Destination Airport:string,Actual elapsed ti...\n",
    "    +- BroadcastExchange HashedRelationBroadcastMode(List(input[1, string, true]))\n",
    "       +- *(1) Project [AIRPORTNAME#258, IATA#259]\n",
    "          +- *(1) Filter isnotnull(IATA#259)\n",
    "             +- *(1) FileScan csv [AIRPORTNAME#258,IATA#259] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/tmp/tmprhigmoha/airportnames.txt.gz], PartitionFilters: [], PushedFilters: [IsNotNull(IATA)], ReadSchema: struct<AIRPORTNAME:string,IATA:string>\n",
    "```\n",
    "You've used Spark broadcasting to improve the performance of your data operations. You should see that the query plan uses the Broadcast operations instead of the default Spark versions. You'll likely use broadcasting often with production datasets - checking the query plan will help validate your configuration without actually running the tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing broadcast vs normal joins\n",
    "\n",
    "You've created two types of joins, normal and broadcasted. Now your manager would like to know what the performance improvement is by using Spark optimizations. If the results are promising, you'll be given more opportunity to tweak the Spark setup as needed.\n",
    "\n",
    "Your DataFrames normal_df and broadcast_df are available for your use.\n",
    "\n",
    "- Execute .count() on the normal DataFrame.\n",
    "- Execute .count() on the broadcasted DataFrame.\n",
    "- Print the count and duration of the DataFrames noting and differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "# Count the number of rows in the normal DataFrame\n",
    "normal_count = normal_df.count()\n",
    "normal_duration = time.time() - start_time\n",
    "\n",
    "start_time = time.time()\n",
    "# Count the number of rows in the broadcast DataFrame\n",
    "broadcast_count = broadcast_df.count()\n",
    "broadcast_duration = time.time() - start_time\n",
    "\n",
    "# Print the counts and the duration of the tests\n",
    "print(\"Normal count:\\t\\t%d\\tduration: %f\" % (normal_count, normal_duration))\n",
    "print(\"Broadcast count:\\t%d\\tduration: %f\" % (broadcast_count, broadcast_duration))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<script.py> output:\n",
    "    Normal count:\t\t119910\tduration: 2.495135\n",
    "    Broadcast count:\t119910\tduration: 0.381412\n",
    "```\n",
    "While the difference in time is miniscule for our example, the ratio between the durations is significant. Depending on the makeup of the data being joined, you can notably cut the run time for Spark operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='pipe'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to data pipelines\n",
    "\n",
    "<img src=\"images/spark3_049.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark3_050.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark3_051.png\" alt=\"\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick pipeline\n",
    "\n",
    "Before you parse some more complex data, your manager would like to see a simple pipeline example including the basic steps. For this example, you'll want to ingest a data file, filter a few rows, add an ID column to it, then write it out as JSON data.\n",
    "\n",
    "The spark context is defined, along with the pyspark.sql.functions library being aliased as F as is customary.\n",
    "\n",
    "- Import the file 2015-departures.csv.gz to a DataFrame. Note the header is already defined.\n",
    "- Filter the DataFrame to contain only flights with a duration over 0 minutes. Use the index of the column, not the column name (remember to use .printSchema() to see the column names / order).\n",
    "- Add an ID column.\n",
    "- Write the file out as a JSON document named output.json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data to a DataFrame\n",
    "departures_df = spark.read.csv('2015-departures.csv.gz', header=True)\n",
    "\n",
    "# Remove any duration of 0\n",
    "departures_df = departures_df.filter(departures_df[3] > 0)\n",
    "\n",
    "# Add an ID column\n",
    "departures_df = departures_df.withColumn('id', F.monotonically_increasing_id())\n",
    "\n",
    "# Write the file out to JSON format\n",
    "departures_df.write.json('output.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a very simple example, but this does represent a multi-step data pipeline in Spark. The same procedures are used even when the content requires much more processing to parse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data handling techniques\n",
    "\n",
    "<img src=\"images/spark3_052.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark3_053.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark3_054.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark3_055.png\" alt=\"\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing commented lines\n",
    "\n",
    "Your boss would like you to perform some complex parsing on a new dataset. The data represents annotation data for the ImageNet dataset, but focusing specifically on dog breeds and identifying them in images. Before any actual analysis can occur, you'll need to clear out several components of invalid / incorrect data. The general schema of the document is unknown so you'd like to import the rows into a single column, allowing for quick analysis.\n",
    "\n",
    "To start, you need to remove all commented rows in the dataset.\n",
    "\n",
    "The spark context, and the base CSV file (annotations.csv.gz) are available for you to work with. The col function is also available for use.\n",
    "\n",
    "- Import the annotations.csv.gz file to a DataFrame and perform a row count. Specify a separator character of |.\n",
    "- Query the data for the number of rows beginning with #.\n",
    "- Import the file again to a new DataFrame, but specify the comment character in the options to remove any commented rows.\n",
    "- Count the new DataFrame and verify the difference is as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the file to a DataFrame and perform a row count\n",
    "annotations_df = spark.read.csv('annotations.csv.gz', sep='|')\n",
    "full_count = annotations_df.count()\n",
    "\n",
    "# Count the number of rows beginning with '#'\n",
    "comment_count = annotations_df.where(col('_c0').startswith('#')).count()\n",
    "\n",
    "# Import the file to a new DataFrame, without commented rows\n",
    "no_comments_df = spark.read.csv('annotations.csv.gz', sep='|', comment='#')\n",
    "\n",
    "# Count the new DataFrame and verify the difference is as expected\n",
    "no_comments_count = no_comments_df.count()\n",
    "print(\"Full count: %d\\nComment count: %d\\nRemaining count: %d\" % (full_count, comment_count, no_comments_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handling commented rows is easy in Spark and allows you to quickly remove any row beginning with a defined character. Consider what would happen if you had multiple comments to filter out and how you might accomplish this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing invalid rows\n",
    "\n",
    "Now that you've successfully removed the commented rows, you have received some information about the general format of the data. There should be at minimum 5 tab separated columns in the DataFrame. Remember that your original DataFrame only has a single column, so you'll need to split the data on the tab (\\t) characters.\n",
    "\n",
    "The DataFrame annotations_df is already available, with the commented rows removed. The spark.sql.functions library is available under the alias F. The initial number of rows available in the DataFrame is stored in the variable initial_count.\n",
    "\n",
    "- Create a new variable tmp_fields using the annotations_df DataFrame column '_c0' splitting it on the tab character.\n",
    "- Create a new column in annotations_df named 'colcount' representing the number of fields defined in the previous step.\n",
    "- Filter out any rows from annotations_df containing fewer than 5 fields.\n",
    "- Count the number of rows in the DataFrame and compare to the initial_count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split _c0 on the tab character and store the list in a variable\n",
    "tmp_fields = F.split(annotations_df['_c0'], '\\t')\n",
    "\n",
    "# Create the colcount column on the DataFrame\n",
    "annotations_df = annotations_df.withColumn('colcount', F.size(tmp_fields))\n",
    "\n",
    "# Remove any rows containing fewer than 5 fields\n",
    "annotations_df_filtered = annotations_df.filter(~ (annotations_df[\"colcount\"] < 5))\n",
    "\n",
    "# Count the number of rows\n",
    "final_count = annotations_df_filtered.count()\n",
    "print(\"Initial count: %d\\nFinal count: %d\" % (initial_count, final_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting into columns\n",
    "\n",
    "You've cleaned up your data considerably by removing the invalid rows from the DataFrame. Now you want to perform some further transformations by generating specific meaningful columns based on the DataFrame content.\n",
    "\n",
    "You have the spark context and the latest version of the annotations_df DataFrame. pyspark.sql.functions is available under the alias F.\n",
    "\n",
    "- Split the content of the '_c0' column on the tab character and store in a variable called split_cols.\n",
    "- Add the following columns based on the first four entries in the variable above: folder, filename, width, height on a DataFrame named split_df.\n",
    "- Add the split_cols variable as a column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the content of _c0 on the tab character (aka, '\\t')\n",
    "split_cols = F.split(annotations_df[\"_c0\"], '\\t')\n",
    "\n",
    "# Add the columns folder, filename, width, and height\n",
    "split_df = annotations_df.withColumn('folder', split_cols.getItem(0))\n",
    "split_df = split_df.withColumn('filename', split_cols.getItem(1))\n",
    "split_df = split_df.withColumn('width', split_cols.getItem(2))\n",
    "split_df = split_df.withColumn('height', split_cols.getItem(3))\n",
    "\n",
    "# Add split_cols as a column\n",
    "split_df = split_df.withColumn('split_cols', split_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're getting close to the end of the course and things are getting more complex. You may be wondering why we're not using a schema instead to define the content layout. Spark's CSV parser can't handle advanced types (Arrays or Maps) so it wouldn't process correctly. In our example, we bypass using the types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further parsing\n",
    "\n",
    "You've molded this dataset into a significantly different format than it was before, but there are still a few things left to do. You need to prep the column data for use in later analysis and remove a few intermediary columns.\n",
    "\n",
    "The spark context is available and pyspark.sql.functions is aliased as F. The types from pyspark.sql.types are already imported. The split_df DataFrame is as you last left it. Remember, you can use .printSchema() on a DataFrame in the console area to view the column names and types.\n",
    "\n",
    "- Create a new function called retriever that takes two arguments, the split columns (cols) and the total number of columns (colcount). This function should return a list of the entries that have not been defined as columns yet (i.e., everything after item 4 in the list).\n",
    "- Define the function as a Spark UDF, returning an Array of strings.\n",
    "- Create the new column dog_list using the UDF and the available columns in the DataFrame.\n",
    "- Remove the columns _c0, colcount, and split_cols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retriever(cols, colcount):\n",
    "  # Return a list of dog data\n",
    "  return cols[4:colcount]\n",
    "\n",
    "# Define the method as a UDF\n",
    "udfRetriever = F.udf(retriever, ArrayType(StringType()))\n",
    "\n",
    "# Create a new column using your UDF\n",
    "split_df = split_df.withColumn('dog_list', udfRetriever(split_df.split_cols, split_df.colcount))\n",
    "\n",
    "# Remove the original column, split_cols, and the colcount\n",
    "split_df = split_df.drop('_c0').drop('split_cols').drop('colcount')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data validation\n",
    "\n",
    "<img src=\"images/spark3_056.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark3_057.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark3_058.png\" alt=\"\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate rows via join\n",
    "\n",
    "Another example of filtering data is using joins to remove invalid entries. You'll need to verify the folder names are as expected based on a given DataFrame named valid_folders_df. The DataFrame split_df is as you last left it with a group of split columns.\n",
    "\n",
    "The spark object is available, and pyspark.sql.functions is imported as F.\n",
    "\n",
    "- Rename the _c0 column to folder on the valid_folders_df DataFrame.\n",
    "- Count the number of rows in split_df.\n",
    "- Join the two DataFrames on the folder name, and call the resulting DataFrame joined_df. Make sure to broadcast the smaller DataFrame.\n",
    "- Check the number of rows remaining in the DataFrame and compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the column in valid_folders_df\n",
    "valid_folders_df = valid_folders_df.withColumnRenamed('_c0', 'folder')\n",
    "\n",
    "# Count the number of rows in split_df\n",
    "split_count = split_df.count()\n",
    "\n",
    "# Join the DataFrames\n",
    "joined_df = split_df.join(F.broadcast(valid_folders_df), \"folder\")\n",
    "\n",
    "# Compare the number of rows remaining\n",
    "joined_count = joined_df.count()\n",
    "print(\"Before: %d\\nAfter: %d\" % (split_count, joined_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nicely done - using joins in this fashion drastically simplifies a validation task if your data permits it. The validation data doesn't necessarily need to be loaded from a file - it could be calculated on the fly, or based on a previous dataset. Optimizing these tasks will improve your overall data cleaning process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining invalid rows\n",
    "\n",
    "You've successfully filtered out the rows using a join, but sometimes you'd like to examine the data that is invalid. This data can be stored for later processing or for troubleshooting your data sources.\n",
    "\n",
    "You want to find the difference between two DataFrames and store the invalid rows.\n",
    "\n",
    "The spark object is defined and pyspark.sql.functions are imported as F. The original DataFrame split_df and the joined DataFrame joined_df are available as they were in their previous states.\n",
    "\n",
    "- Determine the row counts for each DataFrame.\n",
    "- Create a DataFrame containing only the invalid rows.\n",
    "- Validate the count of the new DataFrame is as expected.\n",
    "- Determine the number of distinct folder columns removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the row counts for each DataFrame\n",
    "split_count = split_df.count()\n",
    "joined_count = joined_df.count()\n",
    "\n",
    "# Create a DataFrame containing the invalid rows\n",
    "invalid_df = split_df.join(F.broadcast(joined_df), 'folder', 'left_anti')\n",
    "\n",
    "# Validate the count of the new DataFrame is as expected\n",
    "invalid_count = invalid_df.count()\n",
    "print(\" split_df:\\t%d\\n joined_df:\\t%d\\n invalid_df: \\t%d\" % (split_count, joined_count, invalid_count))\n",
    "\n",
    "# Determine the number of distinct folder columns removed\n",
    "invalid_folder_count = invalid_df.select('folder').distinct().count()\n",
    "print(\"%d distinct invalid folders found\" % invalid_folder_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<script.py> output:\n",
    "     split_df:\t20580\n",
    "     joined_df:\t19956\n",
    "     invalid_df: \t624\n",
    "    1 distinct invalid folders found\n",
    "```\n",
    "Using different types of joins can produce useful results for the various stages of data cleaning. While there are often multiple ways to accomplish a task, using the various join methods will often finish more quickly and cleanly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final analysis and delivery\n",
    "\n",
    "<img src=\"images/spark3_059.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark3_060.png\" alt=\"\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dog parsing\n",
    "\n",
    "You've done a considerable amount of cleanup on the initial dataset, but now need to analyze the data a bit deeper. There are several questions that have now come up about the type of dogs seen in an image and some details regarding the images. You realize that to answer these questions, you need to process the data into a specific type. Before you can use it, you'll need to create a schema / type to represent the dog details.\n",
    "\n",
    "The joined_df DataFrame is as you last defined it, and the pyspark.sql.types have all been imported.\n",
    "\n",
    "- Select the column representing the dog details from the DataFrame and show the first 10 un-truncated rows.\n",
    "- Create a new schema as you've done before, using breed, start_x, start_y, end_x, and end_y as the names. Make sure to specify the proper data types for each field in the schema (any number value is an integer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the dog details and show 10 untruncated rows\n",
    "print(joined_df.select('dog_list').show(truncate=False))\n",
    "\n",
    "# Define a schema type for the details in the dog list\n",
    "DogType = StructType([\n",
    "    StructField(\"breed\", StringType(), False),\n",
    "    StructField(\"start_x\", IntegerType(), False),\n",
    "    StructField(\"start_y\", IntegerType(), False),\n",
    "    StructField(\"end_x\", IntegerType(), False),\n",
    "    StructField(\"end_y\", IntegerType(), False)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<script.py> output:\n",
    "    +--------------------------------------------------------+\n",
    "    |dog_list                                                |\n",
    "    +--------------------------------------------------------+\n",
    "    |[affenpinscher,0,9,173,298]                             |\n",
    "    |[Border_terrier,73,127,341,335]                         |\n",
    "    |[kuvasz,0,0,499,327]                                    |\n",
    "    |[Great_Pyrenees,124,225,403,374]                        |\n",
    "    |[schipperke,146,29,416,309]                             |\n",
    "    |[groenendael,168,0,469,374]                             |\n",
    "    |[Bedlington_terrier,10,12,462,332]                      |\n",
    "    |[Lhasa,39,1,499,373]                                    |\n",
    "    |[Kerry_blue_terrier,17,16,300,482]                      |\n",
    "    |[vizsla,112,93,276,236]                                 |\n",
    "    |[Eskimo_dog,43,20,472,461]                              |\n",
    "    |[cairn,71,2,319,302]                                    |\n",
    "    |[EntleBucher,307,94,515,448, EntleBucher,101,33,330,448]|\n",
    "    |[Japanese_spaniel,23,0,598,435]                         |\n",
    "    |[Great_Dane,51,36,355,332]                              |\n",
    "    |[Siberian_husky,7,2,235,498]                            |\n",
    "    |[Blenheim_spaniel,25,66,401,387]                        |\n",
    "    |[cairn,82,2,472,369]                                    |\n",
    "    |[Lhasa,141,40,423,185]                                  |\n",
    "    |[giant_schnauzer,227,130,339,367]                       |\n",
    "    +--------------------------------------------------------+\n",
    "    only showing top 20 rows\n",
    "    \n",
    "    None\n",
    "```\n",
    "Nicely done - you'll use this schema soon to determine some details about the dogs in the data. As you've just seen, schemas can be used for importing data, but they can also be used to simplify accessing information within pre-parsed data. If you're wondering why we didn't just define a full schema for the import, the Spark CSV parser is not capable of using complex schema types using lists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per image count\n",
    "\n",
    "Your next task in building a data pipeline for this dataset is to create a few analysis oriented columns. You've been asked to calculate the number of dogs found in each image based on your dog_list column created earlier. You have also created the DogType which will allow better parsing of the data within some of the data columns.\n",
    "\n",
    "The joined_df is available as you last defined it, and the DogType structtype is defined. pyspark.sql.functions is available under the F alias.\n",
    "\n",
    "- Create a Python function to split each entry in dog_list to its appropriate parts. Make sure to convert any strings into the appropriate types or the DogType will not parse correctly.\n",
    "- Create a UDF using the above function.\n",
    "- Use the UDF to create a new column called dogs. Drop the previous column in the same command.\n",
    "- Show the number of dogs in the new column for the first 10 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to return the number and type of dogs as a tuple\n",
    "def dogParse(doglist):\n",
    "  dogs = []\n",
    "  for dog in doglist:\n",
    "    (breed, start_x, start_y, end_x, end_y) = dog.split(',')\n",
    "    dogs.append((breed, int(start_x), int(start_y), int(end_x), int(end_y)))\n",
    "  return dogs\n",
    "\n",
    "# Create a UDF\n",
    "udfDogParse = F.udf(dogParse, ArrayType(DogType))\n",
    "\n",
    "# Use the UDF to list of dogs and drop the old column\n",
    "joined_df = joined_df.withColumn('dogs', udfDogParse('dog_list')).drop('dog_list')\n",
    "\n",
    "# Show the number of dogs in the first 10 rows\n",
    "joined_df.select(F.size('dogs')).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<script.py> output:\n",
    "    +----------+\n",
    "    |size(dogs)|\n",
    "    +----------+\n",
    "    |         1|\n",
    "    |         1|\n",
    "    |         1|\n",
    "    |         1|\n",
    "    |         1|\n",
    "    |         1|\n",
    "    |         1|\n",
    "    |         1|\n",
    "    |         1|\n",
    "    |         1|\n",
    "    +----------+\n",
    "    only showing top 10 rows\n",
    "```\n",
    "It can be tricky handling nested data, but using UDF and normal Python functions will often make better sense of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Percentage dog pixels\n",
    "\n",
    "The final task for parsing the dog annotation data is to determine the percentage of pixels in each image that represents a dog (or dogs). You'll need to use the various techniques you've learned in this course to help calculate this information and add it as columns for later analysis.\n",
    "\n",
    "To calculate the percentage of pixels, first calculate the total number of pixels representing each dog then sum them for the image. You can calculate the bounding box with the formula:\n",
    "\n",
    "(Xend - Xstart) * (Yend - Ystart)\n",
    "\n",
    "NOTE: You can ignore the possibility of overlapping bounding boxes in this instance.\n",
    "\n",
    "For the percentage, calculate the total number of \"dog\" pixels divided by the total size of the image, multiplied by 100.\n",
    "The joined_df DataFrame is as you last used it. pyspark.sql.functions is aliased to F.\n",
    "\n",
    "- Define a Python function to take a list of tuples (the dog objects) and calculate the total number of \"dog\" pixels per image.\n",
    "- Create a UDF of the function and use it to create a new column called 'dog_pixels' on the DataFrame.\n",
    "- Create another column, 'dog_percent', representing the percentage of 'dog_pixels' in the image. Make sure this is between 0-100%.\n",
    "- Show the first 10 rows with more than 60% 'dog_pixels' in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a UDF to determine the number of pixels per image\n",
    "def dogPixelCount(doglist):\n",
    "  totalpixels = 0\n",
    "  for dog in doglist:\n",
    "    totalpixels += (dog[3] - dog[1]) * (dog[4] - dog[2])\n",
    "  return totalpixels\n",
    "\n",
    "# Define a UDF for the pixel count\n",
    "udfDogPixelCount = F.udf(dogPixelCount, IntegerType())\n",
    "joined_df = joined_df.withColumn('dog_pixels', udfDogPixelCount('dogs'))\n",
    "\n",
    "# Create a column representing the percentage of pixels\n",
    "joined_df = joined_df.withColumn('dog_percent', (joined_df.dog_pixels / (joined_df.width * joined_df.height)) * 100)\n",
    "\n",
    "# Show the first 10 annotations with more than 60% dog\n",
    "joined_df.where('dog_percent > 60').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
