{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "From: https://github.com/ksatola\n",
    "Version: 1.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data Fundamentals with PySpark\n",
    "\n",
    "There's been a lot of buzz about Big Data over the past few years, and it's finally become mainstream for many companies. But what is this Big Data? This course covers the fundamentals of Big Data via PySpark. Spark is “lightning fast cluster computing\" framework for Big Data. It provides a general data processing platform engine and lets you run programs up to 100x faster in memory, or 10x faster on disk, than Hadoop. You’ll use PySpark, a Python package for spark programming and its powerful, higher-level libraries such as SparkSQL, MLlib (for machine learning), etc., to interact with works of William Shakespeare, analyze Fifa football 2018 data and perform clustering of genomic datasets. At the end of this course, you will gain an in-depth understanding of PySpark and it’s application to general Big Data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "- [Introduction](#intro)\n",
    "- [Programming in PySpark RDD's](#rdd)\n",
    "- [PySpark SQL & DataFrames](#sql)\n",
    "- [Overview of PySpark MLlib](#mlib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "path = \"data/dc32/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='intro'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Big Data?\n",
    "\n",
    "<img src=\"images/spark2_001.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark2_002.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark2_003.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark2_004.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark2_005.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark2_006.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark2_007.png\" alt=\"\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PySpark: Spark with Python\n",
    "\n",
    "<img src=\"images/spark2_008.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark2_009.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark2_010.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark2_011.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark2_012.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark2_013.png\" alt=\"\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=local appName=First App>\n"
     ]
    }
   ],
   "source": [
    "#from pyspark.sql import SparkSession\n",
    "#sc = SparkSession.builder.getOrCreate()\n",
    "#print(sc)\n",
    "\n",
    "# https://www.tutorialspoint.com/pyspark/pyspark_sparkcontext.htm\n",
    "\n",
    "# Initialize a SparkSession via PySpark\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local\", \"First App\")\n",
    "print(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding SparkContext\n",
    "\n",
    "A `SparkContext` represents the entry point to Spark functionality. It's like a key to your car. PySpark automatically creates a SparkContext for you in the PySpark shell (so you don't have to create it by yourself) and is exposed via a variable `sc`.\n",
    "\n",
    "In this simple exercise, you'll find out the attributes of the SparkContext in your PySpark shell which you'll be using for the rest of the course.\n",
    "\n",
    "- Print the version of SparkContext in the PySpark shell.\n",
    "- Print the Python version of SparkContext in the PySpark shell.\n",
    "- What is the master of SparkContext in the PySpark shell?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The version of Spark Context in the PySpark shell is 2.4.4\n",
      "The Python version of Spark Context in the PySpark shell is 3.7\n",
      "The master of Spark Context in the PySpark shell is local\n"
     ]
    }
   ],
   "source": [
    "# Print the version of SparkContext\n",
    "print(\"The version of Spark Context in the PySpark shell is\", sc.version)\n",
    "\n",
    "# These two are PySpark specific\n",
    "# Print the Python version of SparkContext\n",
    "print(\"The Python version of Spark Context in the PySpark shell is\", sc.pythonVer)\n",
    "\n",
    "# Print the master of SparkContext\n",
    "print(\"The master of Spark Context in the PySpark shell is\", sc.master)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Use of PySpark\n",
    "\n",
    "`Spark comes with an interactive python shell in which PySpark is already installed in it`. PySpark shell is useful for basic testing and debugging and it is quite powerful. The easiest way to demonstrate the power of PySpark’s shell is to start using it. In this example, you'll load a simple list containing numbers ranging from 1 to 100 in the PySpark shell.\n",
    "\n",
    "The most important thing to understand here is that `we are not creating any SparkContext object because PySpark automatically creates the SparkContext object named sc, by default in the PySpark shell`.\n",
    "\n",
    "- Create a python list named numb containing the numbers 1 to 100.\n",
    "- Load the list into Spark using Spark Context's `parallelize` method and assign it to a variable spark_data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a python list of numbers from 1 to 100 \n",
    "numb = range(1, 101)\n",
    "\n",
    "# Load the list into PySpark  \n",
    "spark_data = sc.parallelize(numb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data in PySpark shell\n",
    "\n",
    "`In PySpark, we express our computation through operations on distributed collections that are automatically parallelized across the cluster`. In the previous exercise, you have seen an example of loading a list as parallelized collections and in this exercise, you'll load the data from a local file in PySpark shell.\n",
    "\n",
    "Remember you already have a SparkContext `sc` and `file_path` variable (which is the path to the README.md file) already available in your workspace.\n",
    "\n",
    "- Load a local text file README.md in PySpark shell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data/dc32/README.md MapPartitionsRDD[2] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = path+'README.md'\n",
    "\n",
    "# Load a local file into PySpark shell\n",
    "lines = sc.textFile(file_path)\n",
    "lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review of functional programming in Python\n",
    "\n",
    "<img src=\"images/spark2_014.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark2_015.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark2_016.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark2_017.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark2_018.png\" alt=\"\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use of lambda() with map()\n",
    "\n",
    "The `map()` function in Python returns a list of the results after applying the given function to each item of a given iterable (list, tuple etc.). The general syntax of map() function is `map(fun, iter)`. We can also use lambda functions with map(). The general syntax of map() function with lambda() is `map(lambda <agument>:<expression>, iter)`.\n",
    "\n",
    "In this exercise, you'll be using `lambda function inside the map()` built-in function to square all numbers in the list.\n",
    "\n",
    "- Print my_list which is available in your environment.\n",
    "- Square each item in my_list using map() and lambda().\n",
    "- Print the result of map function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input list is [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "The squared numbers are [1, 4, 9, 16, 25, 36, 49, 64, 81, 100]\n"
     ]
    }
   ],
   "source": [
    "my_list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "\n",
    "# Print my_list in the console\n",
    "print(\"Input list is\", my_list)\n",
    "\n",
    "# Square all numbers in my_list\n",
    "squared_list_lambda = list(map(lambda x: x**2, my_list))\n",
    "\n",
    "# Print the result of the map function\n",
    "print(\"The squared numbers are\", squared_list_lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use of lambda() with filter()\n",
    "\n",
    "Another function that is used extensively in Python is the `filter()` function. The `filter()` function in Python takes in a function and a list as arguments. The general syntax of the filter() function is `filter(function, list_of_input)`. Similar to the map(), filter() can be used with lambda() function. The general syntax of the filter() function with lambda() is `filter(lambda <argument>:<expression>, list)`.\n",
    "\n",
    "In this exercise, you'll be using lambda() function inside the filter() built-in function to find all the numbers divisible by 10 in the list.\n",
    "\n",
    "- Print my_list2 which is available in your environment.\n",
    "- Filter the numbers divisible by 10 from my_list2 using filter() and lambda().\n",
    "- Print the numbers divisible by 10 from my_list2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input list is: [10, 21, 31, 40, 51, 60, 72, 80, 93, 101]\n",
      "Numbers divisible by 10 are: [10, 40, 60, 80]\n"
     ]
    }
   ],
   "source": [
    "my_list2 = [10, 21, 31, 40, 51, 60, 72, 80, 93, 101]\n",
    "\n",
    "# Print my_list2 in the console\n",
    "print(\"Input list is:\", my_list2)\n",
    "\n",
    "# Filter numbers divisible by 10\n",
    "filtered_list = list(filter(lambda x: (x%10 == 0), my_list2))\n",
    "\n",
    "# Print the numbers divisible by 10\n",
    "print(\"Numbers divisible by 10 are:\", filtered_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='rdd'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming in PySpark RDD's\n",
    "\n",
    "## Abstracting Data with RDDs\n",
    "\n",
    "<img src=\"images/spark2_019.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark2_020.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark2_021.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark2_022.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark2_023.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark2_024.png\" alt=\"\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDDs from Parallelized collections\n",
    "\n",
    "`Resilient Distributed Dataset (RDD)` is the basic abstraction in Spark. It is an immutable distributed collection of objects. Since RDD is a fundamental and backbone data type in Spark, it is important that you understand how to create it. In this exercise, you'll create your first RDD in PySpark from a collection of words.\n",
    "\n",
    "Remember you already have a SparkContext sc available in your workspace.\n",
    "\n",
    "- Create an RDD named RDD from a list of words.\n",
    "- Confirm the object created is RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The type of RDD is <class 'pyspark.rdd.RDD'>\n"
     ]
    }
   ],
   "source": [
    "# Create an RDD from a list of words\n",
    "RDD = sc.parallelize([\"Spark\", \"is\", \"a\", \"framework\", \"for\", \"Big Data processing\"])\n",
    "\n",
    "# Print out the type of the created object\n",
    "print(\"The type of RDD is\", type(RDD))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDDs from External Datasets\n",
    "\n",
    "PySpark can easily create RDDs from files that are stored in external storage devices such as `HDFS (Hadoop Distributed File System)`, `Amazon S3` buckets, etc. However, the most common method of creating RDD's is from files stored in your local file system. This method takes a file path and reads it as a collection of lines. In this exercise, you'll create an RDD from the file path (`file_path`) with the file name `README.md` which is already available in your workspace.\n",
    "\n",
    "Remember you already have a SparkContext sc available in your workspace.\n",
    "\n",
    "- Print the file_path in the PySpark shell.\n",
    "- Create an RDD named fileRDD from a file_path with the file name README.md.\n",
    "- Print the type of the fileRDD created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file_path is data/dc32/README.md\n",
      "The file type of fileRDD is <class 'pyspark.rdd.RDD'>\n"
     ]
    }
   ],
   "source": [
    "file_path = path+'README.md'\n",
    "\n",
    "# Print the file_path\n",
    "print(\"The file_path is\", file_path)\n",
    "\n",
    "# Create a fileRDD from file_path\n",
    "fileRDD = sc.textFile(file_path)\n",
    "\n",
    "# Check the type of fileRDD\n",
    "print(\"The file type of fileRDD is\", type(fileRDD))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partitions in your data\n",
    "\n",
    "SparkContext's `textFile()` method takes an optional second argument called `minPartitions` for specifying the minimum number of partitions. In this exercise, you'll create an RDD named `fileRDD_part` with 5 partitions and then compare that with fileRDD that you created in the previous exercise.\n",
    "\n",
    "Remember, you already have a SparkContext sc, file_path and fileRDD available in your workspace.\n",
    "\n",
    "- Find the number of partitions that support fileRDD RDD.\n",
    "- Create an RDD named fileRDD_part from the file path but create 5 partitions.\n",
    "- Confirm the number of partitions in the new fileRDD_part RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions in fileRDD is 1\n",
      "Number of partitions in fileRDD_part is 5\n"
     ]
    }
   ],
   "source": [
    "# Check the number of partitions in fileRDD\n",
    "print(\"Number of partitions in fileRDD is\", fileRDD.getNumPartitions())\n",
    "\n",
    "# Create a fileRDD_part from file_path with 5 partitions\n",
    "fileRDD_part = sc.textFile(file_path, minPartitions = 5)\n",
    "\n",
    "# Check the number of partitions in fileRDD_part\n",
    "print(\"Number of partitions in fileRDD_part is\", fileRDD_part.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that modifying the number of partitions may result in faster performance due to parallelization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic RDD Transformations and Actions\n",
    "\n",
    "<img src=\"images/spark2_025.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark2_026.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark2_027.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark2_028.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark2_029.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark2_030.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark2_031.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark2_032.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark2_033.png\" alt=\"\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map and Collect\n",
    "\n",
    "The main method by which you can manipulate data is PySpark is using map(). The `map()` transformation takes in a function and applies it to each element in the RDD. It can be used to do any number of things, from fetching the website associated with each URL in our collection to just squaring the numbers. In this simple exercise, you'll use `map()` transformation to cube each number of the numbRDD RDD that you created earlier. Next, you'll return all the elements to a variable and finally print the output.\n",
    "\n",
    "Remember, you already have a SparkContext sc, and numbRDD available in your workspace.\n",
    "\n",
    "- Create map() transformation that cubes all of the numbers in numbRDD.\n",
    "- Collect the results in a numbers_all variable.\n",
    "- Print the output from numbers_all variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "8\n",
      "27\n",
      "64\n",
      "125\n",
      "216\n",
      "343\n",
      "512\n",
      "729\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "my_list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "numbRDD = sc.parallelize(my_list)\n",
    "\n",
    "# Create map() transformation to cube numbers\n",
    "cubedRDD = numbRDD.map(lambda x: x**3)\n",
    "\n",
    "# Collect the results\n",
    "numbers_all = cubedRDD.collect()\n",
    "\n",
    "# Print the numbers from numbers_all\n",
    "for numb in numbers_all:\n",
    "    print(numb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`collect()` should only be used to retrieve results for small datasets. `It shouldn’t be used on large datasets`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter and Count\n",
    "\n",
    "The RDD transformation `filter()` returns a new RDD containing only the elements that satisfy a particular function. It is useful for filtering large datasets based on a keyword. For this exercise, you'll filter out lines containing keyword Spark from fileRDD RDD which consists of lines of text from the README.md file. Next, you'll count the total number of lines containing the keyword Spark and finally print the first 4 lines of the filtered RDD.\n",
    "\n",
    "Remember, you already have a SparkContext sc, file_path and fileRDD available in your workspace.\n",
    "\n",
    "- Create filter() transformation to select the lines containing the keyword Spark.\n",
    "- How many lines in fileRDD_filter contains the keyword Spark?\n",
    "- Print the first four lines of the resulting RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of lines with the keyword Spark is 5\n",
      "Examples for Learning Spark\n",
      "Examples for the Learning Spark book. These examples require a number of libraries and as such have long build files. We have also added a stand alone example with minimal dependencies and a small build file\n",
      "These examples have been updated to run against Spark 1.3 so they may\n",
      "* Spark 1.3\n"
     ]
    }
   ],
   "source": [
    "# Filter the fileRDD to select lines with Spark keyword\n",
    "fileRDD_filter = fileRDD.filter(lambda line: 'Spark' in line.split())\n",
    "\n",
    "# How many lines are there in fileRDD?\n",
    "print(\"The total number of lines with the keyword Spark is\", fileRDD_filter.count())\n",
    "\n",
    "# Print the first four lines of fileRDD\n",
    "for line in fileRDD_filter.take(4): \n",
    "  print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the `filter()` operation does not mutate the existing fileRDD. Instead, it returns a pointer to an entirely new RDD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pair RDDs in PySpark\n",
    "\n",
    "<img src=\"images/spark2_034.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark2_035.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark2_036.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark2_037.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark2_038.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark2_039.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark2_040.png\" alt=\"\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReduceBykey and Collect\n",
    "\n",
    "One of the most popular pair RDD transformations is `reduceByKey()` which operates on `key, value (k,v)` pairs and merges the values for each key. In this exercise, you'll first create a pair RDD from a list of tuples, then combine the values with the same key and finally print out the result.\n",
    "\n",
    "Remember, you already have a SparkContext sc available in your workspace.\n",
    "\n",
    "- Create a pair RDD named Rdd with tuples (1,2),(3,4),(3,6),(4,5).\n",
    "- Transform the Rdd with reduceByKey() into a pair RDD Rdd_Reduced by adding the values with the same key.\n",
    "- Collect the contents of pair RDD Rdd_Reduced and iterate to print the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key 1 has 2 Counts\n",
      "Key 3 has 10 Counts\n",
      "Key 4 has 5 Counts\n"
     ]
    }
   ],
   "source": [
    "# Create PairRDD Rdd with key value pairs\n",
    "Rdd = sc.parallelize([(1,2),(3,4),(3,6),(4,5)])\n",
    "\n",
    "# Apply reduceByKey() operation on Rdd\n",
    "Rdd_Reduced = Rdd.reduceByKey(lambda x, y: x+y)\n",
    "\n",
    "# Iterate over the result and print the output\n",
    "for num in Rdd_Reduced.collect(): \n",
    "  print(\"Key {} has {} Counts\".format(num[0], num[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`reduceByKey()` transformation merges the values for each key using an associative reduce function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SortByKey and Collect\n",
    "\n",
    "Many times it is useful to sort the pair RDD based on the key (for example word count which you'll see later). In this exercise, you'll sort the pair RDD Rdd_Reduced that you created in the previous exercise into descending order and print the final output.\n",
    "\n",
    "Remember, you already have a SparkContext sc and Rdd_Reduced available in your workspace.\n",
    "\n",
    "- Sort the Rdd_Reduced RDD using the key in descending order.\n",
    "- Collect the contents and iterate to print the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key 4 has 5 Counts\n",
      "Key 3 has 10 Counts\n",
      "Key 1 has 2 Counts\n"
     ]
    }
   ],
   "source": [
    "# Sort the reduced RDD with the key by descending order\n",
    "Rdd_Reduced_Sort = Rdd_Reduced.sortByKey(ascending=False)\n",
    "\n",
    "# Iterate over the result and print the output\n",
    "for num in Rdd_Reduced_Sort.collect():\n",
    "  print(\"Key {} has {} Counts\".format(num[0], num[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced RDD Actions\n",
    "\n",
    "<img src=\"images/spark2_041.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark2_042.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark2_043.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark2_044.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark2_045.png\" alt=\"\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountingBykeys\n",
    "\n",
    "For many datasets, it is important to count the number of keys in a key/value dataset. For example, counting the number of countries where the product was sold or to show the most popular baby names. In this simple exercise, you'll use the Rdd pair RDD that you created earlier and count the number of unique keys in that pair RDD.\n",
    "\n",
    "Remember, you already have a SparkContext sc and Rdd available in your workspace.\n",
    "\n",
    "- Use the countByKey() action on the Rdd to count the unique keys and assign the result to a variable total.\n",
    "- What is the type of total?\n",
    "- Iterate over the total and print the keys and their counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The type of total is <class 'collections.defaultdict'>\n",
      "key 1 has 1 counts\n",
      "key 3 has 2 counts\n",
      "key 4 has 1 counts\n"
     ]
    }
   ],
   "source": [
    "# Transform the rdd with countByKey()\n",
    "total = Rdd.countByKey()\n",
    "\n",
    "# What is the type of total?\n",
    "print(\"The type of total is\", type(total))\n",
    "\n",
    "# Iterate over the total and print the output\n",
    "for k, v in total.items(): \n",
    "  print(\"key\", k, \"has\", v, \"counts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember unlike `reduceByKey()` and `sortByKey()`, `countByKey()` is an action and not a transformation on the pair RDD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a base RDD and transform it\n",
    "\n",
    "The volume of unstructured data (log lines, images, binary files) in existence is growing dramatically, and PySpark is an excellent framework for analyzing this type of data through RDDs. In this 3 part exercise, you will write code that calculates the most common words from Complete Works of William Shakespeare.\n",
    "\n",
    "Here are the brief steps for writing the word counting program:\n",
    "\n",
    "- Create a base RDD from Complete_Shakespeare.txt file.\n",
    "- Use RDD transformation to create a long list of words from each element of the base RDD.\n",
    "- Remove stop words from your data.\n",
    "- Create pair RDD where each element is a pair tuple of ('w', 1)\n",
    "- Group the elements of the pair RDD by key (word) and add up their values.\n",
    "- Swap the keys (word) and values (counts) so that keys is count and value is the word.\n",
    "- Finally, sort the RDD by descending order and print the 10 most frequent words and their frequencies.\n",
    "- In this first exercise, you'll create a base RDD from Complete_Shakespeare.txt file and transform it to create a long list of words.\n",
    "\n",
    "Remember, you already have a SparkContext sc already available in your workspace. A file_path variable (which is the path to the Complete_Shakespeare.txt file) is also loaded for you.\n",
    "\n",
    "- Create an RDD called baseRDD that reads lines from file_path.\n",
    "- Transform the baseRDD into a long list of words and create a new splitRDD.\n",
    "- Count the total words in splitRDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words in splitRDD: 128576\n"
     ]
    }
   ],
   "source": [
    "file_path = path+'Complete_Shakespeare.txt'\n",
    "\n",
    "# Create a baseRDD from the file path\n",
    "baseRDD = sc.textFile(file_path)\n",
    "\n",
    "# Split the lines of baseRDD into words\n",
    "splitRDD = baseRDD.flatMap(lambda x: x.split())\n",
    "\n",
    "# Count the total number of words\n",
    "print(\"Total number of words in splitRDD:\", splitRDD.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove stop words and reduce the dataset\n",
    "\n",
    "After splitting the lines in the file into a long list of words using `flatMap()` transformation, in the next step, you'll remove stop words from your data. Stop words are common words that are often uninteresting. For example \"I\", \"the\", \"a\" etc., are stop words. You can remove many obvious stop words with a list of your own. But for this exercise, you will just remove the stop words from a curated list `stop_words` provided to you in your environment.\n",
    "\n",
    "After removing stop words, you'll next create a pair RDD where each element is a pair tuple `(k, v)` where `k` is the key and `v` is the value. In this example, pair RDD is composed of `(w, 1)` where w is for each word in the RDD and 1 is a number. Finally, you'll combine the values with the same key from the pair RDD using `reduceByKey()` operation\n",
    "\n",
    "Remember you already have a SparkContext sc and splitRDD available in your workspace.\n",
    "\n",
    "- Convert the words in splitRDD in lower case and then remove stop words from stop_words.\n",
    "- Create a pair RDD tuple containing the word and the number 1 from each word element in splitRDD.\n",
    "- Get the count of the number of occurrences of each word (word frequency) in the pair RDD using reduceByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', \n",
    "              'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', \n",
    "              'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', \n",
    "              'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', \n",
    "              'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', \n",
    "              'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', \n",
    "              'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', \n",
    "              'about', 'against', 'between', 'into', 'through', 'during', 'before', \n",
    "              'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', \n",
    "              'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', \n",
    "              'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', \n",
    "              'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', \n",
    "              'so', 'than', 'too', 'very', 'can', 'will', 'just', 'don', 'should', 'now']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the words in lower case and remove stop words from stop_words\n",
    "splitRDD_no_stop = splitRDD.filter(lambda x: x.lower() not in stop_words)\n",
    "\n",
    "# Create a tuple of the word and 1 \n",
    "splitRDD_no_stop_words = splitRDD_no_stop.map(lambda w: (w, 1))\n",
    "\n",
    "# Count of the number of occurences of each word\n",
    "resultRDD = splitRDD_no_stop_words.reduceByKey(lambda x, y: x + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print word frequencies\n",
    "\n",
    "After combining the values (counts) with the same key (word), you'll print the word frequencies using the `take(N)` action. You could have used the `collect()` action but as a best practice, it is not recommended as `collect()` returns all the elements from your RDD. You'll use `take(N)` instead, to return N elements from your RDD.\n",
    "\n",
    "What if we want to return the top 10 words? For this first, you'll need to swap the key (word) and values (counts) so that keys is count and value is the word. After you swap the key and value in the tuple, you'll sort the pair RDD based on the key (count) and print the top 10 words in descending order.\n",
    "\n",
    "You already have a SparkContext sc and resultRDD available in your workspace.\n",
    "\n",
    "- Print the first 10 words and their frequencies from the resultRDD.\n",
    "- Swap the keys and values in the resultRDD.\n",
    "- Sort the keys according to descending order.\n",
    "- Print the top 10 most frequent words and their frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Project', 9)\n",
      "('Gutenberg', 7)\n",
      "('EBook', 1)\n",
      "('Complete', 3)\n",
      "('Works', 3)\n",
      "('William', 11)\n",
      "('Shakespeare,', 1)\n",
      "('Shakespeare', 12)\n",
      "('eBook', 2)\n",
      "('use', 38)\n",
      "thou has 650 counts\n",
      "thy has 574 counts\n",
      "shall has 393 counts\n",
      "would has 311 counts\n",
      "good has 295 counts\n",
      "thee has 286 counts\n",
      "love has 273 counts\n",
      "Enter has 269 counts\n",
      "th' has 254 counts\n",
      "make has 225 counts\n"
     ]
    }
   ],
   "source": [
    "# Display the first 10 words and their frequencies\n",
    "for word in resultRDD.take(10):\n",
    "    print(word)\n",
    "\n",
    "# Swap the keys and values \n",
    "resultRDD_swap = resultRDD.map(lambda x: (x[1], x[0]))\n",
    "\n",
    "# Sort the keys in descending order\n",
    "resultRDD_swap_sort = resultRDD_swap.sortByKey(ascending=False)\n",
    "\n",
    "# Show the top 10 most frequent words and their frequencies\n",
    "for word in resultRDD_swap_sort.take(10):\n",
    "    print(\"{} has {} counts\". format(word[1], word[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have sucessfully created a word count program using RDD in PySpark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='sql'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PySpark SQL & DataFrames\n",
    "\n",
    "<img src=\"images/spark2_046.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark2_047.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark2_048.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark2_049.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark2_050.png\" alt=\"\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD to DataFrame\n",
    "\n",
    "Similar to RDDs, `DataFrames` are immutable and distributed data structures in Spark. Even though RDDs are a fundamental data structure in Spark, working with data in DataFrame is easier than RDD most of the time and so understanding of how to convert RDD to DataFrame is necessary.\n",
    "\n",
    "In this exercise, you'll first make an RDD using the `sample_list` which contains the list of tuples ('Mona',20), ('Jennifer',34),('John',20), ('Jim',26) with each tuple contains the name of the person and their age. Next, you'll create a DataFrame using the RDD and the `schema` (which is the list of 'Name' and 'Age') and finally confirm the output as PySpark DataFrame.\n",
    "\n",
    "Remember, you already have a SparkContext sc and SparkSession spark available in your workspace.\n",
    "\n",
    "- Create a sample_list from tuples - ('Mona',20), ('Jennifer',34), ('John',20), ('Jim',26).\n",
    "- Create an RDD from the sample_list.\n",
    "- Create a PySpark DataFrame using the above RDD and schema.\n",
    "- Confirm the output as PySpark DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('First App').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The type of names_df is <class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "# Create a list of tuples\n",
    "sample_list = [('Mona',20), ('Jennifer',34),('John',20), ('Jim',26)]\n",
    "\n",
    "# Create a RDD from the list\n",
    "rdd = sc.parallelize(sample_list)\n",
    "\n",
    "# Create a PySpark DataFrame\n",
    "names_df = spark.createDataFrame(rdd, schema=['Name', 'Age'])\n",
    "\n",
    "# Check the type of names_df\n",
    "print(\"The type of names_df is\", type(names_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading CSV into DataFrame\n",
    "\n",
    "In the previous exercise, you have seen a method of creating DataFrame but generally, `loading data from CSV file is the most common method of creating DataFrames`. In this exercise, you'll create a PySpark DataFrame from a people.csv file that is already provided to you as a `file_path` and confirm the created object is a PySpark DataFrame.\n",
    "\n",
    "Remember, you already have SparkSession spark and file_path variable (which is the path to the people.csv file) available in your workspace.\n",
    "\n",
    "- Create a DataFrame from file_path variable which is the path to the people.csv file.\n",
    "- Confirm the output as PySpark DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = path+'people.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The type of people_df is <class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "# Create an DataFrame from file_path\n",
    "people_df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Check the type of people_df\n",
    "print(\"The type of people_df is\", type(people_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operating on DataFrames in PySpark\n",
    "\n",
    "<img src=\"images/spark2_051.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark2_052.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark2_053.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark2_054.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark2_055.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark2_056.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark2_057.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark2_058.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark2_059.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark2_060.png\" alt=\"\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting data in PySpark DataFrame\n",
    "\n",
    "Inspecting data is very crucial before performing analysis such as plotting, modeling, training etc., In this simple exercise, you'll inspect the data in the `people_df` DataFrame that you have created in the previous exercise using basic DataFrame operators.\n",
    "\n",
    "Remember, you already have SparkSession spark and people_df DataFrame available in your workspace.\n",
    "\n",
    "- Print the first 10 observations in the people_df DataFrame.\n",
    "- Count the number of rows in the people_df DataFrame.\n",
    "- How many columns does people_df DataFrame have and what are their names?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----------------+------+-------------+\n",
      "|_c0|person_id|            name|   sex|date of birth|\n",
      "+---+---------+----------------+------+-------------+\n",
      "|  0|      100|  Penelope Lewis|female|   1990-08-31|\n",
      "|  1|      101|   David Anthony|  male|   1971-10-14|\n",
      "|  2|      102|       Ida Shipp|female|   1962-05-24|\n",
      "|  3|      103|    Joanna Moore|female|   2017-03-10|\n",
      "|  4|      104|  Lisandra Ortiz|female|   2020-08-05|\n",
      "|  5|      105|   David Simmons|  male|   1999-12-30|\n",
      "|  6|      106|   Edward Hudson|  male|   1983-05-09|\n",
      "|  7|      107|    Albert Jones|  male|   1990-09-13|\n",
      "|  8|      108|Leonard Cavender|  male|   1958-08-08|\n",
      "|  9|      109|  Everett Vadala|  male|   2005-05-24|\n",
      "+---+---------+----------------+------+-------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "There are 100000 rows in the people_df DataFrame.\n",
      "There are 5 columns in the people_df DataFrame and their names are ['_c0', 'person_id', 'name', 'sex', 'date of birth']\n"
     ]
    }
   ],
   "source": [
    "# Print the first 10 observations \n",
    "people_df.show(10)\n",
    "\n",
    "# Count the number of rows \n",
    "print(\"There are {} rows in the people_df DataFrame.\".format(people_df.count()))\n",
    "\n",
    "# Count the number of columns and their names\n",
    "print(\"There are {} columns in the people_df DataFrame and their names are {}\".format(len(people_df.columns), people_df.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PySpark DataFrame subsetting and cleaning\n",
    "\n",
    "After data inspection, it is often necessary to clean the data which mainly involves subsetting, renaming the columns, removing duplicated rows etc., PySpark DataFrame API provides several operators to do this. In this exercise, your job is to subset 'name', 'sex' and 'date of birth' columns from `people_df` DataFrame, remove any duplicate rows from that dataset and count the number of rows before and after duplicates removal step.\n",
    "\n",
    "Remember, you already have SparkSession spark and people_df DataFrames available in your workspace.\n",
    "\n",
    "- Select 'name', 'sex' and 'date of birth' columns from people_df and create people_df_sub DataFrame.\n",
    "- Print the first 10 observations in the people_df DataFrame.\n",
    "- Remove duplicate entries from people_df_sub DataFrame and create people_df_sub_nodup DataFrame.\n",
    "- How many rows are there before and after duplicates are removed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------+-------------+\n",
      "|            name|   sex|date of birth|\n",
      "+----------------+------+-------------+\n",
      "|  Penelope Lewis|female|   1990-08-31|\n",
      "|   David Anthony|  male|   1971-10-14|\n",
      "|       Ida Shipp|female|   1962-05-24|\n",
      "|    Joanna Moore|female|   2017-03-10|\n",
      "|  Lisandra Ortiz|female|   2020-08-05|\n",
      "|   David Simmons|  male|   1999-12-30|\n",
      "|   Edward Hudson|  male|   1983-05-09|\n",
      "|    Albert Jones|  male|   1990-09-13|\n",
      "|Leonard Cavender|  male|   1958-08-08|\n",
      "|  Everett Vadala|  male|   2005-05-24|\n",
      "+----------------+------+-------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "There were 100000 rows before removing duplicates, and 99998 rows after removing duplicates\n"
     ]
    }
   ],
   "source": [
    "# Select name, sex and date of birth columns\n",
    "people_df_sub = people_df.select('name', 'sex', 'date of birth')\n",
    "\n",
    "# Print the first 10 observations from people_df_sub\n",
    "people_df_sub.show(10)\n",
    "\n",
    "# Remove duplicate entries from people_df_sub\n",
    "people_df_sub_nodup = people_df_sub.dropDuplicates()\n",
    "\n",
    "# Count the number of rows\n",
    "print(\"There were {} rows before removing duplicates, and {} rows after removing duplicates\".format(people_df_sub.count(), people_df_sub_nodup.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering your DataFrame\n",
    "\n",
    "In the previous exercise, you have subset the data using `select()` operator which is mainly used to subset the DataFrame column-wise. What if you want to subset the DataFrame based on a condition (for example, select all rows where the sex is Female). In this exercise, you will `filter` the rows in the people_df DataFrame in which 'sex' is female and male and create two different datasets. Finally, you'll count the number of rows in each of those datasets.\n",
    "\n",
    "Remember, you already have SparkSession spark and people_df DataFrame available in your workspace.\n",
    "\n",
    "- Filter the people_df DataFrame to select all rows where sex is female into people_df_female DataFrame.\n",
    "- Filter the people_df DataFrame to select all rows where sex is male into people_df_male DataFrame.\n",
    "- Count the number of rows in people_df_female and people_df_male DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 49014 rows in the people_df_female DataFrame and 49066 rows in the people_df_male DataFrame\n"
     ]
    }
   ],
   "source": [
    "# Filter people_df to select females \n",
    "people_df_female = people_df.filter(people_df.sex == \"female\")\n",
    "\n",
    "# Filter people_df to select males\n",
    "people_df_male = people_df.filter(people_df.sex == \"male\")\n",
    "\n",
    "# Count the number of rows \n",
    "print(\"There are {} rows in the people_df_female DataFrame and {} rows in the people_df_male DataFrame\".format(people_df_female.count(), people_df_male.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interacting with DataFrames using PySpark SQL\n",
    "\n",
    "<img src=\"images/spark2_061.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark2_062.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark2_063.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark2_064.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark2_065.png\" alt=\"\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running SQL Queries Programmatically\n",
    "\n",
    "DataFrames can easily be manipulated using SQL queries in PySpark. The `sql()` function on a `SparkSession` enables applications to run SQL queries programmatically and returns the result as another DataFrame. In this exercise, you'll create a temporary table of the people_df DataFrame that you created previously, then construct a query to select the names of the people from the temporary table and assign the result to a new DataFrame.\n",
    "\n",
    "Spark SQL operations generally return DataFrames. This means you can freely mix DataFrames and SQL.\n",
    "\n",
    "Remember, you already have SparkSession spark and people_df DataFrame available in your workspace.\n",
    "\n",
    "- Create a temporary table people that's a pointer to the people_df DataFrame.\n",
    "- Construct a query to select the names of the people from the temporary table people.\n",
    "- Assign the result of Spark's query to a new DataFrame - people_df_names.\n",
    "- Print the top 10 names of the people from people_df_names DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|            name|\n",
      "+----------------+\n",
      "|  Penelope Lewis|\n",
      "|   David Anthony|\n",
      "|       Ida Shipp|\n",
      "|    Joanna Moore|\n",
      "|  Lisandra Ortiz|\n",
      "|   David Simmons|\n",
      "|   Edward Hudson|\n",
      "|    Albert Jones|\n",
      "|Leonard Cavender|\n",
      "|  Everett Vadala|\n",
      "+----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a temporary table \"people\"\n",
    "people_df.createOrReplaceTempView(\"people\")\n",
    "\n",
    "# Construct a query to select the names of the people from the temporary table \"people\"\n",
    "query = '''SELECT name FROM people'''\n",
    "\n",
    "# Assign the result of Spark's query to people_df_names\n",
    "people_df_names = spark.sql(query)\n",
    "\n",
    "# Print the top 10 names of the people\n",
    "people_df_names.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL queries for filtering Table\n",
    "\n",
    "In the previous exercise, you have run a simple SQL query on a DataFrame. There are more sophisticated queries you can construct to obtain the result that you want and use it for downstream analysis such as data visualization and Machine Learning. In this exercise, we will use the temporary table people that you created previously and filter out the rows where the \"sex\" is male and female and create two DataFrames.\n",
    "\n",
    "Remember, you already have SparkSession spark and people temporary table available in your workspace.\n",
    "\n",
    "- Filter the people table to select all rows where sex is female into people_female_df DataFrame.\n",
    "- Filter the people table to select all rows where sex is male into people_male_df DataFrame.\n",
    "- Count the number of rows in both people_female and people_male DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 49014 rows in the people_female_df and 49066 rows in the people_male_df DataFrames\n"
     ]
    }
   ],
   "source": [
    "# Filter the people table to select female sex \n",
    "people_female_df = spark.sql('SELECT * FROM people WHERE sex==\"female\"')\n",
    "\n",
    "# Filter the people table DataFrame to select male sex\n",
    "people_male_df = spark.sql('SELECT * FROM people WHERE sex==\"male\"')\n",
    "\n",
    "# Count the number of rows in both DataFrames\n",
    "print(\"There are {} rows in the people_female_df and {} rows in the people_male_df DataFrames\".format(people_female_df.count(), people_male_df.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization in PySpark using DataFrames\n",
    "\n",
    "<img src=\"images/spark2_066.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark2_067.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark2_068.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark2_069.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark2_070.png\" alt=\"\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PySpark DataFrame visualization\n",
    "\n",
    "Graphical representations or visualization of data is imperative for understanding as well as interpreting the data. In this simple data visualization exercise, you'll first print the column names of `names_df` DataFrame that you created earlier, then convert the names_df to Pandas DataFrame and finally plot the contents as horizontal bar plot with names of the people on the x-axis and their age on the y-axis.\n",
    "\n",
    "Remember, you already have SparkSession spark and names_df DataFrame available in your workspace.\n",
    "\n",
    "- Print the names of the columns in names_df DataFrame.\n",
    "- Convert names_df DataFrame to df_pandas Pandas DataFrame.\n",
    "- Use matplotlib's plot() method to create a horizontal bar plot with 'Name' on x-axis and 'Age' on y-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The column names of names_df are ['Name', 'Age']\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check the column names of names_df\n",
    "print(\"The column names of names_df are\", names_df.columns)\n",
    "\n",
    "# Convert to Pandas DataFrame  \n",
    "df_pandas = names_df.toPandas()\n",
    "\n",
    "# Create a horizontal bar plot\n",
    "df_pandas.plot(kind='barh', x='Name', y='Age', colormap='winter_r')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Create a DataFrame from CSV file\n",
    "\n",
    "Every 4 years, the soccer fans throughout the world celebrates a festival called “Fifa World Cup” and with that, everything seems to change in many countries. In this 3 part exercise, you'll be doing some `exploratory data analysis (EDA)` on the \"FIFA 2018 World Cup Player\" dataset using PySpark SQL which involve DataFrame operations, SQL queries and visualization.\n",
    "\n",
    "In the first part, you'll load FIFA 2018 World Cup Players dataset (Fifa2018_dataset.csv) which is in CSV format into a PySpark's dataFrame and inspect the data using basic DataFrame operations.\n",
    "\n",
    "Remember, you already have SparkSession spark and file_path variable (which is the path to the Fifa2018_dataset.csv file) available in your workspace.\n",
    "\n",
    "- Create a PySpark DataFrame from file_path which is the path to the Fifa2018_dataset.csv file.\n",
    "- Print the schema of the DataFrame.\n",
    "- Print the first 10 observations.\n",
    "- How many rows are in there in the DataFrame?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = path+'Fifa2018_dataset.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Photo: string (nullable = true)\n",
      " |-- Nationality: string (nullable = true)\n",
      " |-- Flag: string (nullable = true)\n",
      " |-- Overall: integer (nullable = true)\n",
      " |-- Potential: integer (nullable = true)\n",
      " |-- Club: string (nullable = true)\n",
      " |-- Club Logo: string (nullable = true)\n",
      " |-- Value: string (nullable = true)\n",
      " |-- Wage: string (nullable = true)\n",
      " |-- Special: integer (nullable = true)\n",
      " |-- Acceleration: string (nullable = true)\n",
      " |-- Aggression: string (nullable = true)\n",
      " |-- Agility: string (nullable = true)\n",
      " |-- Balance: string (nullable = true)\n",
      " |-- Ball control: string (nullable = true)\n",
      " |-- Composure: string (nullable = true)\n",
      " |-- Crossing: string (nullable = true)\n",
      " |-- Curve: string (nullable = true)\n",
      " |-- Dribbling: string (nullable = true)\n",
      " |-- Finishing: string (nullable = true)\n",
      " |-- Free kick accuracy: string (nullable = true)\n",
      " |-- GK diving: string (nullable = true)\n",
      " |-- GK handling: string (nullable = true)\n",
      " |-- GK kicking: string (nullable = true)\n",
      " |-- GK positioning: string (nullable = true)\n",
      " |-- GK reflexes: string (nullable = true)\n",
      " |-- Heading accuracy: string (nullable = true)\n",
      " |-- Interceptions: string (nullable = true)\n",
      " |-- Jumping: string (nullable = true)\n",
      " |-- Long passing: string (nullable = true)\n",
      " |-- Long shots: string (nullable = true)\n",
      " |-- Marking: string (nullable = true)\n",
      " |-- Penalties: string (nullable = true)\n",
      " |-- Positioning: string (nullable = true)\n",
      " |-- Reactions: string (nullable = true)\n",
      " |-- Short passing: string (nullable = true)\n",
      " |-- Shot power: string (nullable = true)\n",
      " |-- Sliding tackle: string (nullable = true)\n",
      " |-- Sprint speed: string (nullable = true)\n",
      " |-- Stamina: string (nullable = true)\n",
      " |-- Standing tackle: string (nullable = true)\n",
      " |-- Strength: string (nullable = true)\n",
      " |-- Vision: string (nullable = true)\n",
      " |-- Volleys: string (nullable = true)\n",
      " |-- CAM: double (nullable = true)\n",
      " |-- CB: double (nullable = true)\n",
      " |-- CDM: double (nullable = true)\n",
      " |-- CF: double (nullable = true)\n",
      " |-- CM: double (nullable = true)\n",
      " |-- ID: integer (nullable = true)\n",
      " |-- LAM: double (nullable = true)\n",
      " |-- LB: double (nullable = true)\n",
      " |-- LCB: double (nullable = true)\n",
      " |-- LCM: double (nullable = true)\n",
      " |-- LDM: double (nullable = true)\n",
      " |-- LF: double (nullable = true)\n",
      " |-- LM: double (nullable = true)\n",
      " |-- LS: double (nullable = true)\n",
      " |-- LW: double (nullable = true)\n",
      " |-- LWB: double (nullable = true)\n",
      " |-- Preferred Positions: string (nullable = true)\n",
      " |-- RAM: double (nullable = true)\n",
      " |-- RB: double (nullable = true)\n",
      " |-- RCB: double (nullable = true)\n",
      " |-- RCM: double (nullable = true)\n",
      " |-- RDM: double (nullable = true)\n",
      " |-- RF: double (nullable = true)\n",
      " |-- RM: double (nullable = true)\n",
      " |-- RS: double (nullable = true)\n",
      " |-- RW: double (nullable = true)\n",
      " |-- RWB: double (nullable = true)\n",
      " |-- ST: double (nullable = true)\n",
      "\n",
      "+---+-----------------+---+--------------------+-----------+--------------------+-------+---------+-------------------+--------------------+------+-----+-------+------------+----------+-------+-------+------------+---------+--------+-----+---------+---------+------------------+---------+-----------+----------+--------------+-----------+----------------+-------------+-------+------------+----------+-------+---------+-----------+---------+-------------+----------+--------------+------------+-------+---------------+--------+------+-------+----+----+----+----+----+------+----+----+----+----+----+----+----+----+----+----+-------------------+----+----+----+----+----+----+----+----+----+----+----+\n",
      "|_c0|             Name|Age|               Photo|Nationality|                Flag|Overall|Potential|               Club|           Club Logo| Value| Wage|Special|Acceleration|Aggression|Agility|Balance|Ball control|Composure|Crossing|Curve|Dribbling|Finishing|Free kick accuracy|GK diving|GK handling|GK kicking|GK positioning|GK reflexes|Heading accuracy|Interceptions|Jumping|Long passing|Long shots|Marking|Penalties|Positioning|Reactions|Short passing|Shot power|Sliding tackle|Sprint speed|Stamina|Standing tackle|Strength|Vision|Volleys| CAM|  CB| CDM|  CF|  CM|    ID| LAM|  LB| LCB| LCM| LDM|  LF|  LM|  LS|  LW| LWB|Preferred Positions| RAM|  RB| RCB| RCM| RDM|  RF|  RM|  RS|  RW| RWB|  ST|\n",
      "+---+-----------------+---+--------------------+-----------+--------------------+-------+---------+-------------------+--------------------+------+-----+-------+------------+----------+-------+-------+------------+---------+--------+-----+---------+---------+------------------+---------+-----------+----------+--------------+-----------+----------------+-------------+-------+------------+----------+-------+---------+-----------+---------+-------------+----------+--------------+------------+-------+---------------+--------+------+-------+----+----+----+----+----+------+----+----+----+----+----+----+----+----+----+----+-------------------+----+----+----+----+----+----+----+----+----+----+----+\n",
      "|  0|Cristiano Ronaldo| 32|https://cdn.sofif...|   Portugal|https://cdn.sofif...|     94|       94|     Real Madrid CF|https://cdn.sofif...|€95.5M|€565K|   2228|          89|        63|     89|     63|          93|       95|      85|   81|       91|       94|                76|        7|         11|        15|            14|         11|              88|           29|     95|          77|        92|     22|       85|         95|       96|           83|        94|            23|          91|     92|             31|      80|    85|     88|89.0|53.0|62.0|91.0|82.0| 20801|89.0|61.0|53.0|82.0|62.0|91.0|89.0|92.0|91.0|66.0|             ST LW |89.0|61.0|53.0|82.0|62.0|91.0|89.0|92.0|91.0|66.0|92.0|\n",
      "|  1|         L. Messi| 30|https://cdn.sofif...|  Argentina|https://cdn.sofif...|     93|       93|       FC Barcelona|https://cdn.sofif...| €105M|€565K|   2154|          92|        48|     90|     95|          95|       96|      77|   89|       97|       95|                90|        6|         11|        15|            14|          8|              71|           22|     68|          87|        88|     13|       74|         93|       95|           88|        85|            26|          87|     73|             28|      59|    90|     85|92.0|45.0|59.0|92.0|84.0|158023|92.0|57.0|45.0|84.0|59.0|92.0|90.0|88.0|91.0|62.0|                RW |92.0|57.0|45.0|84.0|59.0|92.0|90.0|88.0|91.0|62.0|88.0|\n",
      "|  2|           Neymar| 25|https://cdn.sofif...|     Brazil|https://cdn.sofif...|     92|       94|Paris Saint-Germain|https://cdn.sofif...| €123M|€280K|   2100|          94|        56|     96|     82|          95|       92|      75|   81|       96|       89|                84|        9|          9|        15|            15|         11|              62|           36|     61|          75|        77|     21|       81|         90|       88|           81|        80|            33|          90|     78|             24|      53|    80|     83|88.0|46.0|59.0|88.0|79.0|190871|88.0|59.0|46.0|79.0|59.0|88.0|87.0|84.0|89.0|64.0|                LW |88.0|59.0|46.0|79.0|59.0|88.0|87.0|84.0|89.0|64.0|84.0|\n",
      "|  3|        L. Suárez| 30|https://cdn.sofif...|    Uruguay|https://cdn.sofif...|     92|       92|       FC Barcelona|https://cdn.sofif...|  €97M|€510K|   2291|          88|        78|     86|     60|          91|       83|      77|   86|       86|       94|                84|       27|         25|        31|            33|         37|              77|           41|     69|          64|        86|     30|       85|         92|       93|           83|        87|            38|          77|     89|             45|      80|    84|     88|87.0|58.0|65.0|88.0|80.0|176580|87.0|64.0|58.0|80.0|65.0|88.0|85.0|88.0|87.0|68.0|                ST |87.0|64.0|58.0|80.0|65.0|88.0|85.0|88.0|87.0|68.0|88.0|\n",
      "|  4|         M. Neuer| 31|https://cdn.sofif...|    Germany|https://cdn.sofif...|     92|       92|   FC Bayern Munich|https://cdn.sofif...|  €61M|€230K|   1493|          58|        29|     52|     35|          48|       70|      15|   14|       30|       13|                11|       91|         90|        95|            91|         89|              25|           30|     78|          59|        16|     10|       47|         12|       85|           55|        25|            11|          61|     44|             10|      83|    70|     11|null|null|null|null|null|167495|null|null|null|null|null|null|null|null|null|null|                GK |null|null|null|null|null|null|null|null|null|null|null|\n",
      "|  5|   R. Lewandowski| 28|https://cdn.sofif...|     Poland|https://cdn.sofif...|     91|       91|   FC Bayern Munich|https://cdn.sofif...|  €92M|€355K|   2143|          79|        80|     78|     80|          89|       87|      62|   77|       85|       91|                84|       15|          6|        12|             8|         10|              85|           39|     84|          65|        83|     25|       81|         91|       91|           83|        88|            19|          83|     79|             42|      84|    78|     87|84.0|57.0|62.0|87.0|78.0|188545|84.0|58.0|57.0|78.0|62.0|87.0|82.0|88.0|84.0|61.0|                ST |84.0|58.0|57.0|78.0|62.0|87.0|82.0|88.0|84.0|61.0|88.0|\n",
      "|  6|           De Gea| 26|https://cdn.sofif...|      Spain|https://cdn.sofif...|     90|       92|  Manchester United|https://cdn.sofif...|€64.5M|€215K|   1458|          57|        38|     60|     43|          42|       64|      17|   21|       18|       13|                19|       90|         85|        87|            86|         90|              21|           30|     67|          51|        12|     13|       40|         12|       88|           50|        31|            13|          58|     40|             21|      64|    68|     13|null|null|null|null|null|193080|null|null|null|null|null|null|null|null|null|null|                GK |null|null|null|null|null|null|null|null|null|null|null|\n",
      "|  7|        E. Hazard| 26|https://cdn.sofif...|    Belgium|https://cdn.sofif...|     90|       91|            Chelsea|https://cdn.sofif...|€90.5M|€295K|   2096|          93|        54|     93|     91|          92|       87|      80|   82|       93|       83|                79|       11|         12|         6|             8|          8|              57|           41|     59|          81|        82|     25|       86|         85|       85|           86|        79|            22|          87|     79|             27|      65|    86|     79|88.0|47.0|61.0|87.0|81.0|183277|88.0|59.0|47.0|81.0|61.0|87.0|87.0|82.0|88.0|64.0|                LW |88.0|59.0|47.0|81.0|61.0|87.0|87.0|82.0|88.0|64.0|82.0|\n",
      "|  8|         T. Kroos| 27|https://cdn.sofif...|    Germany|https://cdn.sofif...|     90|       90|     Real Madrid CF|https://cdn.sofif...|  €79M|€340K|   2165|          60|        60|     71|     69|          89|       85|      85|   85|       79|       76|                84|       10|         11|        13|             7|         10|              54|           85|     32|          93|        90|     63|       73|         79|       86|           90|        87|            69|          52|     77|             82|      74|    88|     82|83.0|72.0|82.0|81.0|87.0|182521|83.0|76.0|72.0|87.0|82.0|81.0|81.0|77.0|80.0|78.0|            CDM CM |83.0|76.0|72.0|87.0|82.0|81.0|81.0|77.0|80.0|78.0|77.0|\n",
      "|  9|       G. Higuaín| 29|https://cdn.sofif...|  Argentina|https://cdn.sofif...|     90|       90|           Juventus|https://cdn.sofif...|  €77M|€275K|   1961|          78|        50|     75|     69|          85|       86|      68|   74|       84|       91|                62|        5|         12|         7|             5|         10|              86|           20|     79|          59|        82|     12|       70|         92|       88|           75|        88|            18|          80|     72|             22|      85|    70|     88|81.0|46.0|52.0|84.0|71.0|167664|81.0|51.0|46.0|71.0|52.0|84.0|79.0|87.0|82.0|55.0|                ST |81.0|51.0|46.0|71.0|52.0|84.0|79.0|87.0|82.0|55.0|87.0|\n",
      "+---+-----------------+---+--------------------+-----------+--------------------+-------+---------+-------------------+--------------------+------+-----+-------+------------+----------+-------+-------+------------+---------+--------+-----+---------+---------+------------------+---------+-----------+----------+--------------+-----------+----------------+-------------+-------+------------+----------+-------+---------+-----------+---------+-------------+----------+--------------+------------+-------+---------------+--------+------+-------+----+----+----+----+----+------+----+----+----+----+----+----+----+----+----+----+-------------------+----+----+----+----+----+----+----+----+----+----+----+\n",
      "only showing top 10 rows\n",
      "\n",
      "There are 17981 rows in the fifa_df DataFrame\n"
     ]
    }
   ],
   "source": [
    "# Load the Dataframe\n",
    "fifa_df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Check the schema of columns\n",
    "fifa_df.printSchema()\n",
    "\n",
    "# Show the first 10 observations\n",
    "fifa_df.show(10)\n",
    "\n",
    "# Print the total number of rows\n",
    "print(\"There are {} rows in the fifa_df DataFrame\".format(fifa_df.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: SQL Queries on DataFrame\n",
    "\n",
    "The `fifa_df` DataFrame that we created has additional information about datatypes and names of columns associated with it. This additional information allows PySpark SQL to run SQL queries on DataFrame. SQL queries are concise and easy to run compared to DataFrame operations. But in order to apply SQL queries on DataFrame first, you need to create a temporary view of DataFrame as a table and then apply SQL queries on the created table (Running SQL Queries Programmatically).\n",
    "\n",
    "In the second part, you'll create a temporary table of `fifa_df` DataFrame and run SQL queries to extract the 'Age' column of players from Germany.\n",
    "\n",
    "You already have a SparkContext spark and fifa_df available in your workspace.\n",
    "\n",
    "- Create temporary table fifa_df from fifa_df_table DataFrame.\n",
    "- Construct a \"query\" to extract the \"Age\" column from Germany players.\n",
    "- Apply the SQL \"query\" to the temporary view table and create a new DataFrame.\n",
    "- Computes basic statistics of the created DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|summary|              Age|\n",
      "+-------+-----------------+\n",
      "|  count|             1140|\n",
      "|   mean|24.20263157894737|\n",
      "| stddev|4.197096712293756|\n",
      "|    min|               16|\n",
      "|    max|               36|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a temporary view of fifa_df\n",
    "fifa_df.createOrReplaceTempView('fifa_df_table')\n",
    "\n",
    "# Construct the \"query\"\n",
    "query = '''SELECT Age FROM fifa_df_table WHERE `Nationality` == \"Germany\"'''\n",
    "\n",
    "# Apply the SQL \"query\"\n",
    "fifa_df_germany_age = spark.sql(query)\n",
    "\n",
    "# Generate basic statistics\n",
    "fifa_df_germany_age.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Data visualization\n",
    "\n",
    "Data visualization is important for `exploratory data analysis (EDA)`. PySpark DataFrame is a perfect for data visualization compared to RDDs because of it's inherent structure and schema.\n",
    "\n",
    "In this third part, you'll create a `histogram` of the ages of all the players from Germany from the DataFrame that you created in the previous exercise. For this, you'll first convert the PySpark DataFrame into Pandas DataFrame and use matplotlib's `plot()` function to create a density plot of ages of all players from Germany.\n",
    "\n",
    "Remember, you already have SparkSession spark, fifa_df_table temporary table and fifa_df_germany_age DataFrame available in your workspace.\n",
    "\n",
    "- Convert fifa_df_germany_age to fifa_df_germany_age_pandas Pandas DataFrame.\n",
    "- Generate a density plot of the 'Age' column from the fifa_df_germany_age_pandas Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert fifa_df to fifa_df_germany_age_pandas DataFrame\n",
    "fifa_df_germany_age_pandas = fifa_df_germany_age.toPandas()\n",
    "\n",
    "# Plot the 'Age' density of Germany Players\n",
    "fifa_df_germany_age_pandas.plot(kind='density')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='mlib'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of PySpark MLlib\n",
    "\n",
    "<img src=\"images/spark2_071.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark2_072.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark2_073.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark2_074.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark2_075.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "`pyspark.mllib` is the builtin library for `RDD-based API`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PySpark MLlib algorithms\n",
    "\n",
    "Before using any Machine learning algorithms in PySpark shell, you'll have to import the submodules of `pyspark.mllib` library and then choose the appropriate class that is needed for a specific machine learning task.\n",
    "\n",
    "In this simple exercise, you'll learn how to import the different submodules of pyspark.mllib along with the classes that are needed for performing Collaborative filtering, Classification and Clustering algorithms.\n",
    "\n",
    "- Import pyspark.mllib recommendation submodule and Alternating Least Squares class.\n",
    "- Import pyspark.mllib classification submodule and Logistic Regression with LBFGS class.\n",
    "- Import pyspark.mllib clustering submodule and kmeans class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the library for ALS\n",
    "from pyspark.mllib.recommendation import ALS\n",
    "\n",
    "# Import the library for Logistic Regression\n",
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "\n",
    "# Import the library for Kmeans\n",
    "from pyspark.mllib.clustering import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborative filtering\n",
    "\n",
    "<img src=\"images/spark2_076.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark2_077.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark2_078.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark2_079.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark2_080.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark2_081.png\" alt=\"\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Movie Lens dataset into RDDs\n",
    "\n",
    "`Collaborative filtering` is a technique for `recommender systems` wherein users' ratings and interactions with various products are used to recommend new ones. With the advent of Machine Learning and parallelized processing of data, Recommender systems have become widely popular in recent years, and are utilized in a variety of areas including movies, music, news, books, research articles, search queries, social tags. In this 3-part exercise, your goal is to develop a simple movie recommendation system using PySpark MLlib using a subset of MovieLens 100k dataset.\n",
    "\n",
    "In the first part, you'll first load the MovieLens data (`ratings.csv`) into RDD and from each line in the RDD which is formatted as `userId,movieId,rating,timestamp`, you'll need to map the MovieLens data to a Ratings object (`userID, productID, rating`) after removing timestamp column and finally you'll split the RDD into training and test RDDs.\n",
    "\n",
    "Remember, you have a SparkContext sc available in your workspace. Also file_path variable (which is the path to the ratings.csv file), and ALS class are already available in your workspace.\n",
    "\n",
    "- Load the ratings.csv dataset into an RDD.\n",
    "- Split the RDD using , as a delimiter.\n",
    "- For each line of the RDD, using Rating() class create a tuple of userID, productID, rating.\n",
    "- Randomly split the data into training data and test data (0.8 and 0.2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = path+'ratings.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data into RDD\n",
    "data = sc.textFile(file_path)\n",
    "\n",
    "# Split the RDD \n",
    "ratings = data.map(lambda l: l.split(','))\n",
    "\n",
    "# Transform the ratings RDD \n",
    "ratings_final = ratings.map(lambda line: Rating(int(line[0]), int(line[1]), float(line[2])))\n",
    "\n",
    "# Split the data into training and test\n",
    "training_data, test_data = ratings_final.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training and predictions\n",
    "\n",
    "After splitting the data into training and test data, in the second part of the exercise, you'll train the ALS algorithm using the training data. PySpark MLlib's `ALS` algorithm has the following mandatory parameters - `rank` (the number of latent factors in the model) and `iterations` (number of iterations to run). After training the ALS model, you can use the model to predict the ratings from the test data. For this, you will provide the user and item columns from the test dataset and finally print the first 2 rows of `predictAll()` output.\n",
    "\n",
    "Remember, you have SparkContext sc, training_data and test_data are already available in your workspace.\n",
    "\n",
    "- Train ALS algorithm with training data and configured parameters (rank = 10 and iterations = 10).\n",
    "- Drop the rating column in the test data.\n",
    "- Test the model by predicting the rating from the test data.\n",
    "- Print the first two rows of the predicted ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.recommendation import ALS, MatrixFactorizationModel, Rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Rating(user=624, product=69069, rating=2.821609216686585),\n",
       " Rating(user=77, product=5618, rating=4.23438205581369)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the ALS model on the training data\n",
    "model = ALS.train(training_data, rank=10, iterations=10)\n",
    "\n",
    "# Drop the ratings column \n",
    "testdata_no_rating = test_data.map(lambda p: (p[0], p[1]))\n",
    "\n",
    "# Predict the model  \n",
    "predictions = model.predictAll(testdata_no_rating)\n",
    "\n",
    "# Print the first rows of the RDD\n",
    "predictions.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation using MSE\n",
    "\n",
    "After generating the predicted ratings from the test data using ALS model, in this final part of the exercise, you'll prepare the data for calculating `Mean Square Error (MSE)` of the model. The MSE is the average value of (original rating – predicted rating)^2 for all users and indicates the absolute fit of the model to the data. To do this, first, you'll organize both the ratings and prediction RDDs to make a tuple of (`(user, product), rating)`), then join the ratings RDD with prediction RDD and finally apply a squared difference function along with `mean()` to get the MSE.\n",
    "\n",
    "Remember, you have a SparkContext sc available in your workspace. Also, ratings_final and predictions RDD are already available in your workspace.\n",
    "\n",
    "- Organize ratings RDD to make ((user, product), rating).\n",
    "- Organize predictions RDD to make ((user, product), rating).\n",
    "- Join the prediction RDD with the ratings RDD.\n",
    "- Evaluate the model using MSE between original rating and predicted rating and print it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error of the model for the test data = 1.34\n"
     ]
    }
   ],
   "source": [
    "# Prepare ratings data\n",
    "rates = ratings_final.map(lambda r: ((r[0], r[1]), r[2]))\n",
    "\n",
    "# Prepare predictions data\n",
    "preds = predictions.map(lambda r: ((r[0], r[1]), r[2]))\n",
    "\n",
    "# Join the ratings data with predictions data\n",
    "rates_and_preds = rates.join(preds)\n",
    "\n",
    "# Calculate and print MSE\n",
    "MSE = rates_and_preds.map(lambda r: (r[1][0] - r[1][1])**2).mean()\n",
    "print(\"Mean Squared Error of the model for the test data = {:.2f}\".format(MSE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "<img src=\"images/spark2_082.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark2_083.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark2_084.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark2_085.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark2_086.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark2_087.png\" alt=\"\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading spam and non-spam data\n",
    "\n",
    "`Logistic Regression` is a popular method to predict a categorical response. Probably one of the most common applications of the logistic regression is the message or email spam classification. In this 3-part exercise, you'll create an email spam classifier with logistic regression using `Spark MLlib`. Here are the brief steps for creating a spam classifier.\n",
    "\n",
    "- Create an RDD of strings representing email.\n",
    "- Run MLlib’s feature extraction algorithms to convert text into an RDD of vectors.\n",
    "- Call a classification algorithm on the RDD of vectors to return a model object to classify new points.\n",
    "- Evaluate the model on a test dataset using one of MLlib’s evaluation functions.\n",
    "- In the first part of the exercise, you'll load the 'spam' and 'ham' (non-spam) files into RDDs, split the emails into individual words and look at the first element in each of the RDD.\n",
    "\n",
    "Remember, you have a SparkContext sc available in your workspace. Also file_path_spam variable (which is the path to the 'spam' file) and file_path_ham (which is the path to the 'non-spam' file) is already available in your workspace.\n",
    "\n",
    "- Create two RDDS, one for 'spam' and one for 'non-spam (ham)'.\n",
    "- Split each email in 'spam' and 'non-spam' RDDs into words.\n",
    "- Print the first element in the split RDD of both 'spam' and 'non-spam'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_spam = path+'spam.txt'\n",
    "file_path_ham = path+'ham.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first element in spam_words is You\n",
      "The first element in non_spam_words is Rofl.\n"
     ]
    }
   ],
   "source": [
    "# Load the datasets into RDDs\n",
    "spam_rdd = sc.textFile(file_path_spam)\n",
    "non_spam_rdd = sc.textFile(file_path_ham)\n",
    "\n",
    "# Split the email messages into words\n",
    "spam_words = spam_rdd.flatMap(lambda email: email.split(' '))\n",
    "non_spam_words = non_spam_rdd.flatMap(lambda email: email.split(' '))\n",
    "\n",
    "# Print the first element in the split RDD\n",
    "print(\"The first element in spam_words is\", spam_words.first())\n",
    "print(\"The first element in non_spam_words is\", non_spam_words.first())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature hashing and LabelPoint\n",
    "\n",
    "After splitting the emails into words, our raw data set of 'spam' and 'non-spam' is currently composed of 1-line messages consisting of spam and non-spam messages. In order to classify these messages, we need to convert text into features.\n",
    "\n",
    "In the second part of the exercise, you'll first create a `HashingTF()` instance to map text to vectors of 200 features, then for each message in 'spam' and 'non-spam' files you'll split them into words, and each word is mapped to one feature. These are the features that will be used to decide whether a message is 'spam' or 'non-spam'. Next, you'll create labels for features. For a valid message, the label will be 0 (i.e. the message is not spam) and for a 'spam' message, the label will be 1 (i.e. the message is spam). Finally, you'll combine both the labeled datasets.\n",
    "\n",
    "Remember, you have a SparkContext sc available in your workspace. Also spam_words and non_spam_words variables are already available in your workspace.\n",
    "\n",
    "- Create a HashingTF() instance to map email text to vectors of 200 features.\n",
    "- Each message in 'spam' and 'non-spam' datasets are split into words, and each word is mapped to one feature.\n",
    "- Label the features: 1 for spam, 0 for non-spam.\n",
    "- Combine both the spam and non-spam samples into a single dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.feature import HashingTF, IDF\n",
    "from pyspark.mllib.regression import LabeledPoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a HashingTf instance with 200 features\n",
    "tf = HashingTF(numFeatures=200)\n",
    "\n",
    "# Map each word to one feature\n",
    "spam_features = tf.transform(spam_words)\n",
    "non_spam_features = tf.transform(non_spam_words)\n",
    "\n",
    "# Label the features: 1 for spam, 0 for non-spam\n",
    "spam_samples = spam_features.map(lambda features:LabeledPoint(1, features))\n",
    "non_spam_samples = non_spam_features.map(lambda features:LabeledPoint(0, features))\n",
    "\n",
    "# Combine the two datasets\n",
    "samples = spam_samples.union(non_spam_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression model training\n",
    "\n",
    "After creating labels and features for the data, we’re ready to build a model that can learn from it (training). But before you train the model, you'll split the combined dataset into training and testing dataset because it can assign a probability of being spam to each data point. We can then decide to classify messages as spam or not, depending on how high the probability.\n",
    "\n",
    "In this final part of the exercise, you'll split the data into training and test, run Logistic Regression on the training data, apply the same `HashingTF()` feature transformation to get vectors on a positive example (spam) and a negative one (non-spam) and finally check the accuracy of the model trained.\n",
    "\n",
    "Remember, you have a SparkContext sc available in your workspace, as well as the samples variable.\n",
    "\n",
    "- Split the combined data into training and test sets (80/20).\n",
    "- Train the Logistic Regression (LBFGS variant) model with the training dataset.\n",
    "- Create a prediction label from the trained model on the test dataset.\n",
    "- Combine the labels in the test dataset with the labels in the prediction dataset.\n",
    "- Calculate the accuracy of the trained model using original and predicted labels on the labels_and_preds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy : 0.82\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and testing\n",
    "train_samples,test_samples = samples.randomSplit([0.8, 0.2])\n",
    "\n",
    "# Train the model\n",
    "model = LogisticRegressionWithLBFGS.train(train_samples)\n",
    "\n",
    "# Create a prediction label from the test data\n",
    "predictions = model.predict(test_samples.map(lambda x: x.features))\n",
    "\n",
    "# Combine original labels with the predicted labels\n",
    "labels_and_preds = test_samples.map(lambda x: x.label).zip(predictions)\n",
    "\n",
    "# Check the accuracy of the model on the test data\n",
    "accuracy = labels_and_preds.filter(lambda x: x[0] == x[1]).count() / float(test_samples.count())\n",
    "print(\"Model accuracy : {:.2f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "\n",
    "<img src=\"images/spark2_088.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark2_089.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark2_090.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark2_091.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark2_092.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark2_093.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark2_094.png\" alt=\"\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and parsing the 5000 points data\n",
    "\n",
    "`Clustering` is the unsupervised learning task that involves grouping objects into clusters of high similarity. Unlike the supervised tasks, where data is labeled, clustering can be used to make sense of unlabeled data. PySpark MLlib includes the popular `K-means` algorithm for clustering. In this 3 part exercise, you'll find out how many clusters are there in a dataset containing 5000 rows and 2 columns. For this you'll first load the data into an RDD, parse the RDD based on the delimiter, run the KMeans model, evaluate the model and finally visualize the clusters.\n",
    "\n",
    "In the first part, you'll load the data into RDD, parse the RDD based on the delimiter and convert the string type of the data to an integer.\n",
    "\n",
    "Remember, you have a SparkContext sc available in your workspace. Also file_path variable (which is the path to the 5000_points.txt file) is already available in your workspace.\n",
    "\n",
    "- Load the 5000_points dataset into a RDD named clusterRDD.\n",
    "- Transform the clusterRDD by splitting the lines based on the tab (\"\\t\").\n",
    "- Transform the split RDD to create a list of integers for the two columns.\n",
    "- Confirm that there are 5000 rows in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = path+'5000_points.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 5000 rows in the rdd_split_int dataset\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset into a RDD\n",
    "clusterRDD = sc.textFile(file_path)\n",
    "\n",
    "# Split the RDD based on tab\n",
    "rdd_split = clusterRDD.map(lambda x: x.split(\"\\t\"))\n",
    "\n",
    "# Transform the split RDD by creating a list of integers\n",
    "rdd_split_int = rdd_split.map(lambda x: [int(x[0]), int(x[1])])\n",
    "\n",
    "# Count the number of rows in RDD \n",
    "print(\"There are {} rows in the rdd_split_int dataset\".format(rdd_split_int.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means training\n",
    "\n",
    "Now that the RDD is ready for training, in the second part of the exercise, you'll train the RDD with PySpark's MLlib's KMeans algorithm. The algorithm is somewhat naive--it clusters the data into `k` clusters, even if k is not the right number of clusters to use. Therefore, when using k-means clustering, the most important parameter is a target number of clusters to generate, k. In practice, you rarely know the “true” number of clusters in advance, so the best practice is to try several values of k until the average intracluster distance stops decreasing dramatically\n",
    "\n",
    "In this 2nd part, you'll test with k's ranging from 13 to 16 and `use the elbow method to chose the correct k`. The idea of the elbow method is to run k-means clustering on the dataset for a range of values of k, calculate `Within Set Sum of Squared Error` (WSSSE, this function is already provided to you) and select the best k based on the sudden drop in WSSSE. Finally, you'll retrain the model with the best k (15 in this case) and print the centroids (cluster centers).\n",
    "\n",
    "Remember, you already have a SparkContext sc and rdd_split_int RDD available in your workspace.\n",
    "\n",
    "- Train the KMeans model with clusters from 13 to 16 and print the WSSSE for each cluster.\n",
    "- Train the KMeans model again with the best cluster (k=15).\n",
    "- Get the Cluster Centers (centroids) of KMeans model trained with k=15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.rddsampler import RDDSampler, RDDRangeSampler, RDDStratifiedSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 412.0 failed 1 times, most recent failure: Lost task 0.0 in stage 412.0 (TID 509, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 393, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 839, in func\n    initial = next(iterator)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-65-3889b6c5b709>\", line 4, in <lambda>\nNameError: name 'error' is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 393, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 839, in func\n    initial = next(iterator)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-65-3889b6c5b709>\", line 4, in <lambda>\nNameError: name 'error' is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-3889b6c5b709>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mclst\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m13\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m17\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd_split_int\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mWSSSE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd_split_int\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mpoint\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The cluster {} has Within Set Sum of Squared Error {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWSSSE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mreduce\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    842\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 844\u001b[0;31m         \u001b[0mvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    845\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    814\u001b[0m         \"\"\"\n\u001b[1;32m    815\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 816\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    817\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 412.0 failed 1 times, most recent failure: Lost task 0.0 in stage 412.0 (TID 509, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 393, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 839, in func\n    initial = next(iterator)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-65-3889b6c5b709>\", line 4, in <lambda>\nNameError: name 'error' is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 393, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 839, in func\n    initial = next(iterator)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-65-3889b6c5b709>\", line 4, in <lambda>\nNameError: name 'error' is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "# Train the model with clusters from 13 to 16 and compute WSSSE \n",
    "for clst in range(13, 17):\n",
    "    model = KMeans.train(rdd_split_int, clst, seed=1)\n",
    "    WSSSE = rdd_split_int.map(lambda point: error(point)).reduce(lambda x, y: x + y)\n",
    "    print(\"The cluster {} has Within Set Sum of Squared Error {}\".format(clst, WSSSE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "```\n",
    "<script.py> output:\n",
    "    The cluster 13 has Within Set Sum of Squared Error 249164132.49410182\n",
    "    The cluster 14 has Within Set Sum of Squared Error 209371154.24941802\n",
    "    The cluster 15 has Within Set Sum of Squared Error 169394691.52639425\n",
    "    The cluster 16 has Within Set Sum of Squared Error 202384225.6640126\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model again with the best k \n",
    "model = KMeans.train(rdd_split_int, k=15, seed=1)\n",
    "\n",
    "# Get cluster centers\n",
    "cluster_centers = model.clusterCenters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing clusters\n",
    "\n",
    "After KMeans model training with an optimum K value (K = 15), in this final part of the exercise, you will visualize the clusters and their cluster centers (centroids) and see if they overlap with each other. For this, you'll first convert rdd_split_int RDD into spark DataFrame and then into Pandas DataFrame for plotting. Similarly, you'll convert cluster_centers into Pandas DataFrame. Once the DataFrames are created, you'll use matplotlib library to create scatter plots.\n",
    "\n",
    "Remember, you already have a SparkContext sc, rdd_split_int and cluster_centers variables available in your workspace.\n",
    "\n",
    "- Convert rdd_split_int RDD into a Spark DataFrame.\n",
    "- Convert Spark DataFrame into a Pandas DataFrame.\n",
    "- Create a Pandas DataFrame from cluster_centers list.\n",
    "- Create a scatter plot of the raw data and an overlaid scatter plot with centroids for k = 15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert rdd_split_int RDD into Spark DataFrame\n",
    "rdd_split_int_df = spark.createDataFrame(rdd_split_int, schema=[\"col1\", \"col2\"])\n",
    "\n",
    "# Convert Spark DataFrame into Pandas DataFrame\n",
    "rdd_split_int_df_pandas = rdd_split_int_df.toPandas()\n",
    "\n",
    "# Convert \"cluster_centers\" that you generated earlier into Pandas DataFrame\n",
    "cluster_centers_pandas = pd.DataFrame(cluster_centers, columns=[\"col1\", \"col2\"])\n",
    "\n",
    "# Create an overlaid scatter plot\n",
    "plt.scatter(rdd_split_int_df_pandas[\"col1\"], rdd_split_int_df_pandas[\"col2\"])\n",
    "plt.scatter(cluster_centers_pandas[\"col1\"], cluster_centers_pandas[\"col2\"], color=\"red\", marker=\"x\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
