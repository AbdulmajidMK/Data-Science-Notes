{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "From: https://github.com/ksatola\n",
    "Version: 1.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data Fundamentals with PySpark\n",
    "\n",
    "There's been a lot of buzz about Big Data over the past few years, and it's finally become mainstream for many companies. But what is this Big Data? This course covers the fundamentals of Big Data via PySpark. Spark is “lightning fast cluster computing\" framework for Big Data. It provides a general data processing platform engine and lets you run programs up to 100x faster in memory, or 10x faster on disk, than Hadoop. You’ll use PySpark, a Python package for spark programming and its powerful, higher-level libraries such as SparkSQL, MLlib (for machine learning), etc., to interact with works of William Shakespeare, analyze Fifa football 2018 data and perform clustering of genomic datasets. At the end of this course, you will gain an in-depth understanding of PySpark and it’s application to general Big Data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "- [Introduction](#intro)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "path = \"data/dc32/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='intro'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Big Data?\n",
    "\n",
    "<img src=\"images/spark2_001.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark2_002.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark2_003.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark2_004.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark2_005.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark2_006.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark2_007.png\" alt=\"\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PySpark: Spark with Python\n",
    "\n",
    "<img src=\"images/spark2_008.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark2_009.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark2_010.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark2_011.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark2_012.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark2_013.png\" alt=\"\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=local appName=First App>\n"
     ]
    }
   ],
   "source": [
    "#from pyspark.sql import SparkSession\n",
    "#sc = SparkSession.builder.getOrCreate()\n",
    "#print(sc)\n",
    "\n",
    "# https://www.tutorialspoint.com/pyspark/pyspark_sparkcontext.htm\n",
    "\n",
    "# Initialize a SparkSession via PySpark\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local\", \"First App\")\n",
    "print(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding SparkContext\n",
    "\n",
    "A `SparkContext` represents the entry point to Spark functionality. It's like a key to your car. PySpark automatically creates a SparkContext for you in the PySpark shell (so you don't have to create it by yourself) and is exposed via a variable `sc`.\n",
    "\n",
    "In this simple exercise, you'll find out the attributes of the SparkContext in your PySpark shell which you'll be using for the rest of the course.\n",
    "\n",
    "- Print the version of SparkContext in the PySpark shell.\n",
    "- Print the Python version of SparkContext in the PySpark shell.\n",
    "- What is the master of SparkContext in the PySpark shell?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The version of Spark Context in the PySpark shell is 2.4.4\n",
      "The Python version of Spark Context in the PySpark shell is 3.7\n",
      "The master of Spark Context in the PySpark shell is local\n"
     ]
    }
   ],
   "source": [
    "# Print the version of SparkContext\n",
    "print(\"The version of Spark Context in the PySpark shell is\", sc.version)\n",
    "\n",
    "# These two are PySpark specific\n",
    "# Print the Python version of SparkContext\n",
    "print(\"The Python version of Spark Context in the PySpark shell is\", sc.pythonVer)\n",
    "\n",
    "# Print the master of SparkContext\n",
    "print(\"The master of Spark Context in the PySpark shell is\", sc.master)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Use of PySpark\n",
    "\n",
    "`Spark comes with an interactive python shell in which PySpark is already installed in it`. PySpark shell is useful for basic testing and debugging and it is quite powerful. The easiest way to demonstrate the power of PySpark’s shell is to start using it. In this example, you'll load a simple list containing numbers ranging from 1 to 100 in the PySpark shell.\n",
    "\n",
    "The most important thing to understand here is that `we are not creating any SparkContext object because PySpark automatically creates the SparkContext object named sc, by default in the PySpark shell`.\n",
    "\n",
    "- Create a python list named numb containing the numbers 1 to 100.\n",
    "- Load the list into Spark using Spark Context's `parallelize` method and assign it to a variable spark_data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a python list of numbers from 1 to 100 \n",
    "numb = range(1, 101)\n",
    "\n",
    "# Load the list into PySpark  \n",
    "spark_data = sc.parallelize(numb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data in PySpark shell\n",
    "\n",
    "`In PySpark, we express our computation through operations on distributed collections that are automatically parallelized across the cluster`. In the previous exercise, you have seen an example of loading a list as parallelized collections and in this exercise, you'll load the data from a local file in PySpark shell.\n",
    "\n",
    "Remember you already have a SparkContext `sc` and `file_path` variable (which is the path to the README.md file) already available in your workspace.\n",
    "\n",
    "- Load a local text file README.md in PySpark shell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data/dc32/README.md MapPartitionsRDD[4] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = path+'README.md'\n",
    "\n",
    "# Load a local file into PySpark shell\n",
    "lines = sc.textFile(file_path)\n",
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<img src=\"images/spark2_014.png\" alt=\"\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "---\n",
    "<a id='intro'></a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
