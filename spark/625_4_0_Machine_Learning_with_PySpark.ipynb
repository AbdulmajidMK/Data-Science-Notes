{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "From: https://github.com/ksatola\n",
    "Version: 1.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning with PySpark\n",
    "\n",
    "Spark is a powerful, general purpose tool for working with Big Data. Spark transparently handles the distribution of compute tasks across a cluster. This means that operations are fast, but it also allows you to focus on the analysis rather than worry about technical details. In this course you'll learn how to get data into Spark and then delve into the three fundamental Spark Machine Learning algorithms: Linear Regression, Logistic Regression/Classifiers, and creating pipelines. Along the way you'll analyse a large dataset of flight delays and spam text messages. With this background you'll be ready to harness the power of Spark and apply it on your own Machine Learning projects!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "- [Introduction & Data Loading](#intro)\n",
    "- [Classification](#class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "path = \"data/dc35/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=local appName=First App>\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local\", \"First App\")\n",
    "print(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('First App').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.4\n",
      "sys.version_info(major=3, minor=7, micro=3, releaselevel='final', serial=0)\n"
     ]
    }
   ],
   "source": [
    "# Return spark version\n",
    "print(spark.version)\n",
    "\n",
    "# Return python version\n",
    "import sys\n",
    "print(sys.version_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='intro'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning & Spark\n",
    "\n",
    "<img src=\"images/spark5_001.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark5_002.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark5_003.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark5_004.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark5_005.png\" alt=\"\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting to Spark\n",
    "\n",
    "<img src=\"images/spark5_006.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark5_007.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark5_008.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark5_009.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark5_010.png\" alt=\"\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a SparkSession\n",
    "\n",
    "In this exercise, you'll spin up a local Spark cluster using all available cores. The cluster will be accessible via a `SparkSession` object.\n",
    "\n",
    "The SparkSession class has a `builder` attribute, which is an instance of the Builder class. The Builder class exposes three important methods that let you:\n",
    "\n",
    "- specify the location of the master node;\n",
    "- name the application (optional); and\n",
    "- retrieve an existing SparkSession or, if there is none, create a new one.\n",
    "\n",
    "The SparkSession class has a `version` attribute which gives the version of Spark.\n",
    "\n",
    "Find out more about SparkSession [here](https://spark.apache.org/docs/2.3.1/api/python/pyspark.sql.html#pyspark.sql.SparkSession).\n",
    "\n",
    "Once you are finished with the cluster, it's a good idea to shut it down, which will free up its resources, making them available for other processes.\n",
    "\n",
    "- Import the SparkSession class from pyspark.sql.\n",
    "- Create a SparkSession object connected to a local cluster. Use all available cores. Name the application 'test'.\n",
    "- Use the SparkSession object to retrieve the version of Spark running on the cluster.\n",
    "- Shut down the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.4\n"
     ]
    }
   ],
   "source": [
    "# Import the PySpark module\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create SparkSession object\n",
    "spark = SparkSession.builder \\\n",
    "                    .master('local[*]') \\\n",
    "                    .appName('test') \\\n",
    "                    .getOrCreate()\n",
    "\n",
    "# What version of Spark?\n",
    "print(spark.version)\n",
    "\n",
    "# Terminate the cluster\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "\n",
    "<img src=\"images/spark5_011.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark5_012.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark5_013.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark5_014.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark5_015.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark5_016.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark5_017.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark5_018.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/spark5_019.png\" alt=\"\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading flights data\n",
    "\n",
    "In this exercise you're going to load some airline flight data from a CSV file. To ensure that the exercise runs quickly these data have been trimmed down to only 50 000 records. You can get a larger dataset in the same format [here]().\n",
    "\n",
    "Notes on CSV format:\n",
    "\n",
    "- fields are separated by a comma (this is the default separator) and\n",
    "- missing data are denoted by the string 'NA'.\n",
    "\n",
    "Data dictionary:\n",
    "\n",
    "- mon — month (integer between 1 and 12)\n",
    "- dom — day of month (integer between 1 and 31)\n",
    "- dow — day of week (integer; 1 = Monday and 7 = Sunday)\n",
    "- org — origin airport (IATA code)\n",
    "- mile — distance (miles)\n",
    "- carrier — carrier (IATA code)\n",
    "- depart — departure time (decimal hour)\n",
    "- duration — expected duration (minutes)\n",
    "- delay — delay (minutes)\n",
    "\n",
    "pyspark has been imported for you and the session has been initialized.\n",
    "\n",
    "- Read data from a CSV file called 'flights.csv'. Assign data types to columns automatically. Deal with missing data.\n",
    "- How many records are in the data?\n",
    "- Take a look at the first five records.\n",
    "- What data types have been assigned to the columns? Do these look correct?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data contain 275000 records.\n",
      "+---+---+---+-------+------+---+----+------+--------+-----+\n",
      "|mon|dom|dow|carrier|flight|org|mile|depart|duration|delay|\n",
      "+---+---+---+-------+------+---+----+------+--------+-----+\n",
      "| 10| 10|  1|     OO|  5836|ORD| 157|  8.18|      51|   27|\n",
      "|  1|  4|  1|     OO|  5866|ORD| 466|  15.5|     102| null|\n",
      "| 11| 22|  1|     OO|  6016|ORD| 738|  7.17|     127|  -19|\n",
      "|  2| 14|  5|     B6|   199|JFK|2248| 21.17|     365|   60|\n",
      "|  5| 25|  3|     WN|  1675|SJC| 386| 12.92|      85|   22|\n",
      "+---+---+---+-------+------+---+----+------+--------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('mon', 'int'),\n",
       " ('dom', 'int'),\n",
       " ('dow', 'int'),\n",
       " ('carrier', 'string'),\n",
       " ('flight', 'int'),\n",
       " ('org', 'string'),\n",
       " ('mile', 'int'),\n",
       " ('depart', 'double'),\n",
       " ('duration', 'int'),\n",
       " ('delay', 'int')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read data from CSV file\n",
    "flights = spark.read.csv(path+'flights-larger.csv',\n",
    "                         sep=',',\n",
    "                         header=True,\n",
    "                         inferSchema=True,\n",
    "                         nullValue='NA')\n",
    "\n",
    "# Get number of records\n",
    "print(\"The data contain %d records.\" % flights.count())\n",
    "\n",
    "# View the first five records\n",
    "flights.show(5)\n",
    "\n",
    "# Check column data types\n",
    "flights.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading SMS spam data\n",
    "\n",
    "You've seen that it's possible to infer data types directly from the data. Sometimes it's convenient to have direct control over the column types. You do this by defining an explicit schema.\n",
    "\n",
    "The file sms.csv contains a selection of SMS messages which have been classified as either 'spam' or 'ham'. These data have been adapted from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/sms+spam+collection). There are a total of 5574 SMS, of which 747 have been labelled as spam.\n",
    "\n",
    "Notes on CSV format:\n",
    "\n",
    "- no header record and\n",
    "- fields are separated by a semicolon (this is not the default separator).\n",
    "\n",
    "Data dictionary:\n",
    "\n",
    "- id — record identifier\n",
    "- text — content of SMS message\n",
    "- label — spam or ham (integer; 0 = ham and 1 = spam)\n",
    "\n",
    "Instructions\n",
    "\n",
    "- Specify the data schema, giving columns names (\"id\", \"text\", and \"label\") and column types.\n",
    "- Read data from a delimited file called \"sms.csv\".\n",
    "- Print the schema for the resulting DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "# Specify column names and types\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType()),\n",
    "    StructField(\"text\", StringType()),\n",
    "    StructField(\"label\", IntegerType())\n",
    "])\n",
    "\n",
    "# Load data from a delimited file\n",
    "sms = spark.read.csv(path+'sms.csv', sep=';', header=False, schema=schema)\n",
    "\n",
    "# Print schema of DataFrame\n",
    "sms.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='class'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "## Data Preparation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<img src=\"images/spark5_020.png\" alt=\"\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "---\n",
    "<a id='intro'></a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
