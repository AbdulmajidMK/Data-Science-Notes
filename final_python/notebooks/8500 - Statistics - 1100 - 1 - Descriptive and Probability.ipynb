{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "From: https://github.com/ksatola\n",
    "Version: 0.0.1\n",
    "\n",
    "TODOs\n",
    "1. Distributions: \n",
    "    - https://mathworld.wolfram.com/PoissonDistribution.html\n",
    "    - https://mathworld.wolfram.com/BinomialDistribution.html\n",
    "    - https://mathworld.wolfram.com/UniformDistribution.html\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "## Statistics Basics 1 - Descriptive and Probability\n",
    "\n",
    "- [Descriptive Statistics](#toc00)\n",
    "- [The Center of Data](#toc01)\n",
    "- [Data Variability](#toc02)\n",
    "- [Distribution and Relative Position](#toc03)\n",
    "    - Empirical Rule \n",
    "    - Percentile Rank\n",
    "- [Probability](#toc04)\n",
    "    - Permutations\n",
    "    - Cobinations\n",
    "    - Random Experiment and Random Variable\n",
    "    - Continuous Probability Distribution\n",
    "    - Normal Distribution\n",
    "    - Z-transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='toc00'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptive Statistics\n",
    "The field of `statistics is the science of learning from data`. When statistical principles are correctly applied, statistical analyses tend to produce accurate results. What’s more, the analyses even account for real-world uncertainty in order to calculate the probability of being incorrect.\n",
    "\n",
    "Statisticians offer essential insight in determining which data, analyses, and conclusions are trustworthy. A statistician can be a study’s guide through a minefield of potential pitfalls, any of which could produce misleading conclusions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='toc01'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Center of Data - Mean, Median, Mode, Weighted Mean, Harmonic Mean\n",
    "\n",
    "### Example 1\n",
    "```\n",
    "Provide the mean and median for this data set: 4, 6, 8, 10, 17\n",
    "mean = 45/5 = 9\n",
    "median = 8\n",
    "```\n",
    "### Example 2\n",
    "```\n",
    "Provide the mode for the dataset: 3,5,7,10,3,3,9,2,5,10,9.\n",
    "3\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='toc02'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Variability - Min, Max, Range, Variance, Standard Deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='toc03'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution and Relative Position\n",
    "\n",
    "## z-score\n",
    "How many standard deviations our data point lies from the mean?\n",
    "\n",
    "<img src=\"images/z-score.png\" alt=\"\" style=\"width: 200px;\"/>\n",
    "\n",
    "Simply put, a `z-score` (also called a `standard score`) gives you an idea of how far from the mean a data point is. But more technically it’s a measure of how many standard deviations below or above the population mean a raw score is.\n",
    "\n",
    "A z-score can be placed on a normal distribution curve. Z-scores range from -3 standard deviations (which would fall to the far left of the normal distribution curve) up to +3 standard deviations (which would fall to the far right of the normal distribution curve). In order to use a z-score, you need to know the mean μ and also the population standard deviation σ.\n",
    "\n",
    "Z-scores are a way to compare results to a “normal” population. Results from tests or surveys have thousands of possible results and units; those results can often seem meaningless. For example, knowing that someone’s weight is 150 pounds might be good information, but if you want to compare it to the “average” person’s weight, looking at a vast table of data can be overwhelming (especially if some weights are recorded in kilograms). A z-score can tell you where that person’s weight is compared to the average population’s mean weight.\n",
    "\n",
    "see: https://www.statisticshowto.com/probability-and-statistics/z-score/\n",
    "\n",
    "z-score, standard score example:\n",
    "\n",
    "<img src=\"images/z-score-iq-example.png\" alt=\"\" style=\"width: 400px;\"/>\n",
    "\n",
    "\n",
    "<img src=\"images/z-score-empirical-rule.png\" alt=\"\" style=\"width: 400px;\"/>\n",
    "\n",
    "### Example 1\n",
    "```\n",
    "For a certain data set we have a mean of 100, a median of 95, and a standard deviation of 25. What is the z-score for the data point 138?\n",
    "Answer: z-score = (138-100)/25 = 1.52\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Empirical Rule ( 68-95-99.7) - Three Sigma Rule\n",
    "When you use a standard normal distribution (aka Gaussian Distribution):\n",
    "\n",
    "- About 68% of values fall within one standard deviation of the mean.\n",
    "- About 95% of the values fall within two standard deviations from the mean.\n",
    "- Almost all of the values—about 99.7%—fall within three standard deviations from the mean.\n",
    "\n",
    "These facts are the `68 95 99.7 rule`. It is sometimes called the `Empirical Rul`e because the rule originally came from observations (empirical means “based on observation”).\n",
    "\n",
    "The Normal/Gaussian distribution is the most common type of data distribution. All of the measurements are computed as distances from the mean and are reported in standard deviations.\n",
    "\n",
    "The Gaussian curve is a symmetric distribution, so the middle 68.2% can be divided in two. Zero to 1 standard deviations from the mean has 34.1% of the data. The opposite side is the same (0 to -1 standard deviations). Together, this area adds up to about 68% of the data.\n",
    "\n",
    "<img src=\"images/Empirical-rule-FINAL.jpg\" alt=\"\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Percentile Rank\n",
    "\n",
    "<img src=\"images/percentile_rank.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "Example of PR calculation for score of 85\n",
    "\n",
    "<img src=\"images/percentile_rank_85.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "<img src=\"images/PR_and_NCE.gif\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "\n",
    "\n",
    "See: https://en.wikipedia.org/wiki/Percentile_rank\n",
    "\n",
    "### Example 1\n",
    "```\n",
    "Six students earn the following test scores: 60, 70, 80, 90, 95, 100. The student that scored 95 is in the _____ percentile.\n",
    "Answer: (4 + 0.5) / 6 * 100 = 75\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='toc04'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability\n",
    "Always consider a type of probability you encounter and ask how it was calculated/infered to assess how reliable it might be\n",
    "- **Objective** probabilities (based on calculations)\n",
    "    - **Classical** (coin flip, roll a die) - we know all possible outcomes and they are equally likely to occur\n",
    "        - How: `number of wins / all possible outcomes = % of probability`\n",
    "    - **Empirical** (number of successful shots by a footbal player) \n",
    "        - possible outcomes are not equally likely to occur, each attempt is different and can be influenced by many factors. \n",
    "        - based on past data - we can only use historical data to infer, the more data we have, the more trust we can in the probability \n",
    "        - not perfect but can be done if we have some data in repeating situations\n",
    "        - gives a nice idea of what to expect\n",
    "        - How: `number of successful shots / all shots by the player = % of probability (ratio)`\n",
    "- **Subjective** (based on experience)\n",
    "    - Uses people's opinions and experience and perhaps some related data that influence the statement about probability\n",
    "    - A guess\n",
    "    \n",
    "### Example 1\n",
    "```\n",
    "When people attend a fundraiser, 40% of them donate money. If three people attend this month’s fundraiser -- Jane, Kate, and Liza -- what are the exact chances that just one of them will donate money?\n",
    "Answer:\n",
    "Probability of donating Amount = 40/100 = 0.4\n",
    "Probability of not donating Amount = 1 - 0.4 = 0.6\n",
    "Three possibilities\n",
    "- Jane Donates but Kate & liza do not donate\n",
    "- Kate donate but jane & liza do not donate\n",
    "- Liza donate but Jane & Kate do nt donate\n",
    "\n",
    "p = (0.4)(0.6)(0.6) + (0.4)(0.6)(0.6) + (0.4)(0.6)(0.6) = 0.432 = 43.2%\n",
    "```\n",
    "### Example 2\n",
    "```\n",
    "On an exam, 60 of 100 people got a passing score. 50 of 100 people studied for the exam. 45 of the people that studied got a passing score. Studying for the exam and passing the exam are _____.\n",
    "Answer: dependent events\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permutations: the order of things\n",
    "The number of ways in which objects can be arranged (order matters)\n",
    "- `n!` (n factorial)\n",
    "- for 5 objects: 5! = 5x4x3x2x1 = 120\n",
    "\n",
    "How many permutation we have when selecting x objects out of n?\n",
    "- `n! / (n-x)!`\n",
    "- if we have 8 players, how many permutations we may have on the podium (3 places)?\n",
    "- 8! / (8-3)! = 8x7x6 = 336\n",
    "\n",
    "### Example 1\n",
    "```\n",
    "There are 5 total candidates available for 2 different jobs. How many permutations are there for those two jobs from the pool of 5 candidates?\n",
    "Answer: 5! / (5-2)! = 5x4 = 20\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combinations: permutations without regard for order\n",
    "The number of ways in which objects can be chosen (order not important)\n",
    "- `n! / [(n-x)! * x!]` where n is total number of objects and x is number of objects chosen at one time\n",
    "- if we have 10 students in a class, how many combinations of 4 person team we could randomly choose?\n",
    "- 10! / [(10-4)! * 4!] = 10x9x8x7 / 4x3x2x1 = 5040 / 24 = 210 possible teams of four\n",
    "\n",
    "- what is the probability that 2 specific students (Tom and Kate) end up in the same team?\n",
    "- how many other students can fill the last free spots (2 first sports are already taken by our 2 students, 10 were in total, so we have 8 other students left)\n",
    "- 8! / [(8-2)! * 2!] = 28\n",
    "\n",
    "- so, we have 210 outcomes and 28 desired outcomes\n",
    "- the probability that 2 specific students end up in the same team is 28/210 = 14%\n",
    "\n",
    "Eight adults are carpooling to an event. At random 2 will be chosen as the drivers for the rest of the group. How many combinations of 2 drivers are there among this group of 8?\n",
    "- To solve you divide 8! by the product of 6!*2! This would be (8*7*6*5*4*3*2*1) / [(6*5*4*3*2*1)*(2*1)] This reduces to (8*7)/2 which equals 28.\n",
    "- 8! / [(8-2)! * 2!] = 28\n",
    "\n",
    "You have 10 employees chosen at random to be placed in a team of 4 people. You want Li and Raoul to be on the team. What will the formula 8!/[(8-2)! 2!] provide you?\n",
    "- The number of combinations for the eight employees other than Li and Raoul to be on the team.\n",
    "- When you calculate this, you will also know the probability of Li and Raoul being on the team.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Experiment and Random Variable\n",
    "`Random experiments` are opportunities to observe the outcome of a chance event. If we were rolling dice, the `random experiment` is observing and recording the outcome, which brings us to a random variable. A `random variable` is the numerical outcome of a random experiment. If we rolled a two and a three, our `random variable` would be five. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Variables\n",
    "As the result of the outcome is unknown (random), we call the result from an experiment a random variable\n",
    "- Discrete experimental results often characterised by whole numbers (no decimals are allowed (always whole numbers) and there is a limited number of possible outcomes)\n",
    "- Continuous - there is infinite number of possible outcomes (there are endless of possibilities in terms of outcomes)\n",
    "\n",
    "The distinction between the two is important because you will calculate probabilities differently in each type of situation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discrete Probability Distribution\n",
    "\n",
    "Probability distribution of drinks orders during a party:\n",
    "\n",
    "<img src=\"images/probability_distribution_discrete.png\" alt=\"\" style=\"width: 400px;\"/>\n",
    "\n",
    "<img src=\"images/probability_distribution_discrete2.png\" alt=\"\" style=\"width: 400px;\"/>\n",
    "\n",
    "Relative frequency:\n",
    "\n",
    "<img src=\"images/probability_distribution_discrete_relative_frequency.png\" alt=\"\" style=\"width: 500px;\"/>\n",
    "\n",
    "Mean of discrete probability distribution:\n",
    "\n",
    "<img src=\"images/probability_distribution_discrete_relative_frequency_mean.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "An average consumer ordered 1.46 drinks during the party.\n",
    "\n",
    "Standard deviation of discrete probability distribution:\n",
    "\n",
    "<img src=\"images/probability_distribution_discrete_relative_frequency_std.png\" alt=\"\" style=\"width: 650px;\"/>\n",
    "\n",
    "3.02 (sum of squared weights) - 2.13 (mean squared) = 0.89 (variance) -> 0.94 (variance squared root -> standard deviation / sigma)\n",
    "\n",
    "#### Expected Value\n",
    "Total of the weighted payoffs associated with the decision. `Expected monetary value (EMV)` is a variation of the mean for a discrete probability distribution that includes subtracting the cost of the investment.\n",
    "```\n",
    "You hold a lottery ticket. There is a 10% chance you win $500, a 40% chance you win $25, and a 50% you win $0. What is the expected monetary value of your lottery ticket?\n",
    "Answer: For each outcome multiply the possible winnings times the percentage, then add up all three products. (0.1*500)+(0.4*25)+(0.5*0) = $60\n",
    "```\n",
    "\n",
    "#### Binomial random variable\n",
    "An experiment that has only two possible outcomes. With binomial random variables, you can use n, the number trial, and the chance of success represented as p to predict a result.\n",
    "\n",
    "Binomial vs. Normal distribution\n",
    "\n",
    "<img src=\"images/CompareBinomialAndNormalDistribution.png\" alt=\"\" style=\"width: 400px;\"/>\n",
    "\n",
    "### Example 1\n",
    "```\n",
    "10 customers ordered pizza. Five ordered 1 pizza, two ordered 2 pizzas, two ordered 3 pizzas, and one ordered 4 pizzas. What's the mean number of pizzas ordered for this discrete distribution?\n",
    "\n",
    "    pizzas ordered    frequences    relative freq.    weights\n",
    "        1                5                5/10=0.5        1*0.5=0.5\n",
    "        2                2                2/10=0.2        2*0.2=0.4\n",
    "        3                2                    0.2         3*0.2=0.6\n",
    "        4                1                    0.1         4x0.1=0.4\n",
    "        \n",
    "                                                        Mean (sum of weights) = 1.9\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous Probability Distribution\n",
    "\n",
    "<img src=\"images/probability_distribution_continuous.png\" alt=\"\" style=\"width: 400px;\"/>\n",
    "\n",
    "Probability density curves can be used to show the distribution of outcomes. The area under the curve represents the probability of outcomes. The probability of A is X, and the probability of B is Y. In reality, the probability in a single point is equal to 0, so we usually check the probability over a ranges of random variables. The all area under the curve is equal to 1 (or 100%).\n",
    "\n",
    "<img src=\"images/probability_distribution_continuous_density_distribution_curve.png\" alt=\"\" style=\"width: 400px;\"/>\n",
    "\n",
    "<img src=\"images/probability_distribution_continuous_density_distribution_curve2.png\" alt=\"\" style=\"width: 400px;\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal distribution & Central Limit Theorem\n",
    "\n",
    "The `\"fuzzy\" central limit theorem` says that data which are influenced by many small and unrelated random effects are approximately normally distributed.\n",
    "\n",
    "<img src=\"images/probability_distribution_continuous_normal.png\" alt=\"\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Z-transformations\n",
    "\n",
    "### Example 1\n",
    "What percentage of men weight more than 211 pounds? The weight is normally distributed with mean of 150 pounds and std of 25.\n",
    "\n",
    "First, calculate the `z-score`\n",
    "\n",
    "<img src=\"images/z-score-example.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "It would appear here far to the right\n",
    "\n",
    "<img src=\"images/z-score-example1b.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "Then, find the percentage of men who would weight more than 211 pounds by using `standard normal distribution / z-score table` and find the probability value for z-score of 2.44\n",
    "\n",
    "<img src=\"images/z-score-example1c.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "According to our mean and std 99.27% of all men weight 211 pounds or less, which means that the percentage of men that weight more than 211 pounds is 1-0.9927 = 0,0073 or 0,73%.\n",
    "\n",
    "### Example 2\n",
    "What is the probability a man weight between 140 and 170 pounds?\n",
    "\n",
    "First, we need two z-scores: one for 140, another for 170. Next, check the standard normal distribution table (chart value) values for the scores. \n",
    "\n",
    "<img src=\"images/z-score-example2b.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "How to find a value for a negative z-score? Because the bell curve is symetrical, we can substract number found for the positive z-score from 1.0.\n",
    "\n",
    "<img src=\"images/z-score-example2d.png\" alt=\"\" style=\"width: 400px;\"/>\n",
    "\n",
    "<img src=\"images/z-score-example2e.png\" alt=\"\" style=\"width: 400px;\"/>\n",
    "\n",
    "So, the result is:\n",
    "\n",
    "<img src=\"images/z-score-example2f.png\" alt=\"\" style=\"width: 400px;\"/>\n",
    "\n",
    "\n",
    "### Example 3\n",
    "Comparing distributions\n",
    "\n",
    "<img src=\"images/example_ztransform.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "See: http://www.statistics4u.info/fundstat_eng/ee_ztransform.html\n",
    "\n",
    "### Example 4\n",
    "```\n",
    "A student scores 1510 on a standardized test, for a z-score of 2.17. The z-score table shows that the z-score for 2.17 is 0.9850. Therefore, the probability someone scored >1510 on this test is 0.015 or 1.5%.\n",
    "Answer: 0.9850 indicates that this student was equal to or greater than 98.50% of other student scores. Thus 1.5% are likely to score higher.\n",
    "```\n",
    "\n",
    "### Example 5\n",
    "```\n",
    "The average height for men is 5'9\". Michael's height is 6'7\". If the data is normally distributed and you calculated the Z-score, how can you find the percentage of men who are 6'7\" or taller?\n",
    "Answer: The Standard Distribution Table gives you the percentage of men shorter than 6'7\", thus giving you the probability of being taller. The table associates Z-scores to percentages at or below the Z-score, which leaves the percentage of men taller than the mean.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferential Statistics\n",
    "Based on samples of data infer about whole population."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling\n",
    "The challenge is getting the right answers, especially when the world, even your small slice of it is very big. \n",
    "\n",
    "Measuring everything is just way \n",
    "- too expensive, \n",
    "- too time consuming and \n",
    "- in some cases, it's just impossible. \n",
    "\n",
    "Political operatives can't poll every voter. Cell phone companies can't measure the quality level of every single item the produce. A farmer can't measure the actual size of every tomato grown. Scientists, they can't track the health of every single person in the country. Instead of measuring everything, they just measure a small group or subset of the total population. That small subset of measurements is a `sample`. And under the right circumstances, this `sample can act as a representative of the entire population`. The best samples are chosen at random.\n",
    "\n",
    "### Random Sample\n",
    "The most dependable type of data comes from what we call a `simple random sample`. This means that \n",
    "- the sample is chosen such that each individual in the population has the same probability of being chosen at any stage during the sampling process. \n",
    "- And each subset of k individuals has the same probability of be chosen for the sample as any other subset of k individuals.\n",
    "\n",
    "The simple random sample can be rather elusive. Eliminating bias and maintaining data independence is quite challenging. As a result, `alternatives to the simple random sample` are sometimes utilized (but the simple random sample is still the only way to get dependable statistical outcomes). These alternative methods are simpler to organize, easier to carry out, and often, they seem both logical and sound:\n",
    "- **Systematic sample** - Choose one unit and then every k unit thereafter. So if we're measuring customer satisfaction at a store, perhaps you might ask the first person to come out of the store for their opinions, then you might ask every tenth customer after them, for their opinion.\n",
    "- **Sources of bias** - The sampling time and sampling location as well as the presence of a sampler, may introduce bias or inhibit independence.\n",
    "- **Opportunity sample** - the sampler simply takes the first n number of units that come along.\n",
    "- **Stratified sample** - Is one where the total population is broken up into homogeneous groups. Let's say, we're trying to figure out the average amount of sugar in a single cookie, regardless of the type of cookie. We could break up the population into so many different cookie types. Chocolate chip, peanut butter, oatmeal, sugar, ginger, snickerdoodle, oatmeal raisin. From there, we might take a sample of 30 cookies from each category. Perhaps chocolate chip cookies make up 50% of all cookies and ginger cookies make up only 3% of all the cookies. Our very fair-looking system might actually be biased against the most popular cookies.\n",
    "- **Cluster sample** - is similar to stratified samples in that we are breaking things up into groups. What's the difference? In stratified groups, all the members of each group were the same. In clusters, the groups are likely to have a mix of characteristics. They're heterogeneous. Suppose we are testing a new product. We might ask for samples of people in 20 major cities, what they think about the new product. While the people in a single sample might all be from the same city, each sample might contain men and women, people of different races, politics and socio-economic backgrounds.\n",
    "\n",
    "The `simple random sample` will always be the gold standard, but these `alternative sampling methods` should not be completely dismissed.\n",
    "\n",
    "### Sample Size\n",
    "A `sample` is a group of units drawn from a population, and the `sample size` is the number of units drawn and measured for that particular sample. The total population itself may be very large or, perhaps, immeasurable, so a `sample` is just looking at a slice of the population in the hopes of providing us a representative picture of the entire population. The larger the sample size, the more accurate our measurement or, at least, the more confidence we have that our sample is actually providing us a glimpse of the whole population. \n",
    "\n",
    "<img src=\"images/sample_size.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "And this is where sample size becomes important. Why? Well, let's take a look at the formula for standard deviation, where `n` is our sample size. `P` and the quantity `one minus p`, they won't change, but sample size will. So, `the bigger the sample size, the smaller our standard deviation`. \n",
    "\n",
    "<img src=\"images/sample_size4.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "For this sample, if n is equal to five, then one standard deviation would be 13.4% from 90% in either direction, which means that, with a sample size of five, we would expect 68% of all of our samples to have between 76.6% and 100% good forks, since we can't have more than 100% good forks in any sample. \n",
    "\n",
    "<img src=\"images/sample_size2.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "If n equals 25, one standard deviation would be 6%. At n equals 100, one standard deviation would be 3%. And at n equals 400, one standard deviation would be 1.5%, which means that, with a sample size of 400, we would expect at least 68% of all of our samples to have between 88.5% and 91.5% good forks. As you can see, a larger sample size can really make us feel so much more comfortable with our results. It gives us more confidence when we apply the sample results to our larger population.\n",
    "\n",
    "<img src=\"images/sample_size3.png\" alt=\"\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The central limit theorem\n",
    "The central limit theorem. Let's start simple. A distribution of discrete numbers. We start on the left, where we have five values of five. We move right along our distribution. Two units of 10. Four units of 15. Six units of 20. And on the right of our distribution, we have three values of 25. 20 different readings in our entire population. If we average out the values of our 20 different readings, we get an average of 15.0. \n",
    "\n",
    "<img src=\"images/central_limit_theorem.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "Now suppose we didn't want to tally up all 20 values, but we still wanted to find the average of the data set. Could we use samples to direct us to the population mean? Let's try it. Let's take samples of four units every day. Here's our first sample. Sample one, 10, 15, 20, 25. Our sample mean for this sample is 17.5. Sample two, five, 15, 20, and 20. Our sample mean for this sample is 15. Sample three, five, five, 15, and 20. Our sample mean here is 11.25. We have three samples, thus we have three sample means, 17.5, 15, and 11.25. If we average those, we get the mean of our means, 14.58. Not too far off from our actual population mean of 15.0. \n",
    "\n",
    "<img src=\"images/central_limit_theorem2.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "`The central limit theorem tells us the more samples we take, the closer the means of our sample means will get to the population mean`. Actually, it's even more interesting, because as we start to take many more samples, dozens of samples, hundreds of samples, even thousands of samples, the sample means, if plotted as a histogram, our sampling distribution of our sample means would start to look like a normal distribution. \n",
    "\n",
    "<img src=\"images/central_limit_theorem3.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "`Not only does the central limit theorem work with our example with a tiny population and discrete values, it works with massive populations and continuous values`. So no matter if you're interested in learning about the average test scores of a small school, the average weight of watermelons grown in North America, or the political preferences of voters in the United States, the central limit theorem is there to provide you the guidance to understand the overall population with the assistance of some simple random samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Error (for proportions)\n",
    "We've already begun to see the impact of sample size. In general, `the larger the sample size, the more confidence we have in our results`. Now let's shift our attention to the standard error. In short, `the standard error` is the standard deviation of our proportion distribution. \n",
    "\n",
    "Through an example, let's take a look at how we calculate the standard error and also what that calculated number would mean to us. In the cell phone industry, companies struggle to keep their clients happy. Suppose a reputable, national poll finds that 60% of adults are satisfied with their cell phone provider. Let's take that as our population proportion `p`=0.60. We'd like to see if cell phone service in our city reflects what is being seen nationally. In an attempt to measure this, we'll take simple random samples of 100 cell phone users in our city. Now, we know that 100 people can't possibly reflect the satisfaction levels of everyone in our city, therefor we can assume that each sample will carry with it some level of `standard error`. \n",
    "\n",
    "So, how big is this standard error? Well the answer to that question really depends on you guessed it: sample size. `The standard error is ultimately related to the standard deviation`, our formula for standard deviation is seen here, `p` is the proportion population, in this case 0.60 and `n` is sample size. In this case, 100.\n",
    "\n",
    "<img src=\"images/standard_error_proportions.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "So we can see that for n=100, our standard deviation is approximately 0.05 or 5%. And remember, if we assume a normal distribution, 68% of all the samples taken should fall within one standard deviation of the population proportion. So in this situation, for simple random samples with 100 ratings, we would expect 68% of the samples to provide `sample proportions` or `p-hats` between 55% and 65%. 55% is our lower limit. 65% is our upper limit and 60% our population proportion that would be at the center. \n",
    "\n",
    "<img src=\"images/standard_error_proportions2.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "So if tomorrow we gathered a simple random sample of 100 cell phone customers and 57% of those customers were satisfied, we can say that our city was likely on par with the national proportion of 60% because we were within the 5% standard error. \n",
    "\n",
    "Then again, if we could afford to take simple random samples with sample sizes of 1000 cell phone customers. Notice what happens here, since n is now 1000. Our standard deviation drops to about 0.015 or 1.5%. So if n=1000, we would expect 68% of those larger samples to have p-hats between 58.5% and 61.5%. \n",
    "\n",
    "<img src=\"images/standard_error_proportions3.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "So let's recap, `the standard error in situations where we are looking at proportions is the standard deviation`. This is the formula for the standard deviation of a sample proportion, `p-hat`. The bigger our sample size, the smaller our standard deviation. This standard deviation is our standard error. `The standard error allows us to set up a range around the population proportion that extends the equivalent of one standard deviation in both the positive and negative direction`. The formula for our upper limit is p plus the standard deviation and for the lower the limit, p minus our standard deviation. Once the range is established, if we assume the probability distribution is nearly normal, then we would expect that 68% of the simple random samples gathered in the upcoming weeks, that they would fall within one standard deviation or the standard error. \n",
    "\n",
    "What happens when 68% of our samples are not falling within our calculated upper and lower limits? \n",
    "- Perhaps, it signals that something in our city is different form the overall nation. Customers, companies, or a combination of the two might create a unique environment in our city. \n",
    "- Perhaps there is a flaw in the reported national average of 60%, maybe their data gathering techniques were flawed. \n",
    "- Perhaps, the market has changed since that number was first reported or perhaps our sampling method was biased. \n",
    "\n",
    "Don't forget, while standard errors are there to help us judge and analyze future samples, samples that fall beyond the standard error, they should be analyzed not necessarily judged as failures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling distribution of the mean\n",
    "Suppose we know that the average player in a men's college basketball league weighs 180 pounds. Let's also say that the median player weighs about 190 pounds, so that means quite a few of the smaller players in the league are bringing down that average. This league has over 4,000 players. \n",
    "\n",
    "Would we have to weigh everyone of those 4,000 plus players to know the average weight of a player in the league? \n",
    "\n",
    "Well, if you remember, `the Central Limit Theorem tells us that by taking some simple random samples, we can get a very good approximation of the true population average`. \n",
    "\n",
    "If we take five random samples, with a sample size of only four, we might find that those five tiny samples will have sample means that average to perhaps 182 pounds. Now, if we take five random samples, but increase the sample size to 25, we would likely see the mean of the sample means closer to 180.5 pounds. \n",
    "\n",
    "<img src=\"images/sampling_distribution_of_the_mean.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "Only five simple random samples with sample sizes as low as four can get us very close to our true population mean of 180 pounds. Now, this basketball league has the capability of tracking all 4,000 plus players, so we knew that the average weight of the players in this league was actually 180 pounds, but hopefully the example has helped convince you that even when we have a massive population, the Central Limit Theorem tells us we can trust our simple random samples to point us in the direction of the true population mean. \n",
    "\n",
    "Let's say instead of 4,000 well-tracked male college basketball players, we wanted to know the average weight of 18 to 24 year old men in United States colleges. There are millions of young males in college, and these men are not tracked nearly as well as the basketball players, but that shouldn't be a concern. The Central Limit Theorem tells us that if we are diligent in collecting simple random samples, we should trust our sample means in this scenario, with a population of over three million students, to be as accurate as the example of five samples collected from the pool of 4,000 college basketball players. And it works the other way too. If you have a school of only 50 students, a very small population, you can approximate the population mean for those 50 students by simply taking a few simple random samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Error (for means)\n",
    "Through the use of the central limit theorem, we've seen how taking just a few random samples can guide us in the direction of the population mean. Of course when we use only a few samples to try and figure out a population mean, we understand that `the average of our sample means comes with a standard error`. \n",
    "\n",
    "So how do we figure out the standard error for our simple random samples? Let's say we're trying to figure out how long it takes to get our coffee drinks from our local cafe between 7 a.m. and 8 a.m. on a Monday morning. We take samples on four different Mondays. Our sample size for each of these samples is five. Here is our data set for those days, times are in minutes. \n",
    "\n",
    "<img src=\"images/standard_error_means.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "So, for sample A you can see that the time it took our five customers to get coffee ranged from 0.6 minutes to 2.4 minutes. The average for sample A was 1.58 minutes. If we take the average of the sample means, we will find that the average time to get a coffee drink was about 1.52 minutes. And the standard deviation of those four sample means is 0.25 minutes. \n",
    "\n",
    "`The standard deviation of our sample means this is our estimated standard error`. What's interesting is that `there's a relationship between the standard error of our population means and the standard deviation of the population`. Take a look at this formula, Sigma Xbar is equal to Sigma over the square root of our sample size n. Sigma Xbar is our standard error. So it is the standard deviation of our four sample means. On the other side of the equation, we have another Signma, this Sigma is the standard deviation for the entire population.\n",
    "\n",
    "<img src=\"images/standard_error_means2.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "So by plugging in our calculated standard error from our cafe example which was 0.25 minutes, and then plugging in 5 for n, our sample size. We can then solve for Sigma, our populations standard deviation. We can see that based on these four samples with a sample size of five, we can estimate the population's standard deviation is 0.56 minutes. Again, we can see how sample size can have a huge impact on working with samples to find out information about the entire population. In essence, what the formula tells us is that `if we use larger sample sizes, our standard error gets smaller`. This is also important as we collect samples in the future, why, well, when we collect a sample of drink service times from our cafe tomorrow and the rest of the week, we will know that 68% of our samples should have sample means that fall within 0.25 minutes of 1.52 minutes, our average. Upper limit would then be 1.77 minutes and our lower limit, 1.23 minutes. The standard error formula is very simple, but still very informative, by understanding the simple relationship between sample size, the standard deviation of our population and the stand deviation of our sample means, we can better understand our population as well as the samples we take going forward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1\n",
    "80% of customers pay with a debit or credit card. 25 customers are chosen at random each day. 68% of the samples would have p-hats between:\n",
    "```\n",
    "Sigma p-hat = sqrt((0.80*0.20)/25) = sqrt(0.0064). Next take the square root of 0.0064, this provides a standard dev. of 0.08 or 8% (standard error). The p-hats would be between both 80%-8% and 80%+8%.\n",
    "```\n",
    "### Example 2\n",
    "Three of the choices are true of the Central Limit Theorem. Which is not?\n",
    "- the larger the sample size, the taller and more narrow our distribution\n",
    "- the central limit theorem works for both small and large population sizes\n",
    "- the mean of our random sample means directs us to the population mean\n",
    "- (wrong) the greater the size of our samples, the greater the standard deviation\n",
    "\n",
    "### Example 3\n",
    "A school has 70% female students. 25 random students win a prize each day. We would expect 68% of the daily pools of winners to have between _____ females.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confidence Intervals\n",
    "At this point, you should hopefully feel comfortable with the concepts of sample size and the central limit theorem. `The central limit theorem tells us that if we take enough simple random samples, we can get an excellent approximation of our population means`. In other words, rather than measure everything in the population, we can take some random samples. Those random samples will provide us with the measurements that will be nearly normal in our distribution, and will direct us to the population mean. \n",
    "\n",
    "And you also, hopefully, remember that `the larger the sample size of those random samples, the smaller the standard deviation of our distributions`, so the more certain we are about our resulting population mean. Lots of samples make us feel confident about our population numbers.\n",
    "\n",
    "In this section, in which we will cover `confidence intervals`, we're going to go in the opposite direction. `What happens when we have only one sample?` If we have only one sample, how confident are we that this single sample mean is near our actual population mean? \n",
    "\n",
    "In this section, you'll often see results that look like this. We are 95% confident that the average adult in the United States drinks between two and three liters of beverages per day. As you can see, one random sample will allow us to calculate our range and attach to it a level of confidence. Think about how incredibly powerful this is, the efficient use of resources, the overall savings, and the ability to be 95% confident, or perhaps even more confident than that. \n",
    "\n",
    "But before we move on, let's take a moment to discuss what a 95% confidence level means. It means that if, instead of taking a poll only once, we took a similar poll 20 times. 19 times, the results of the poll, in other words, the resulting ranges of those 19 polls, they would capture the population mean. But of course, one of the 20 times, the reported range would not include our population mean. Remember that the next time a pre-election poll predicts the wrong candidate to win. So, let's get started with that exact example. Let's see how they create confidence intervals for those pre-election polls.\n",
    "\n",
    "### What exactly is a confidence interval?\n",
    "Let's create a 95% confidence interval for an election poll where the voters have two choices: Candidate A and Candidate B. As you may have guessed, we'll be working with proportions. Before we start creating a 95% confidence interval for this scenario, let's recap a few things. \n",
    "\n",
    "- First, if we took a lot of voter samples, the distribution would be approximately normal. \n",
    "- Second, the larger the voter sample size, the smaller the variation, and thus, the smaller the standard deviation of the resulting distribution. \n",
    "- Third, a 95% interval is one where we are 95% certain that our interval which will be centered at the sample's proportion will contain the actual population proportion. \n",
    "\n",
    "Now what we're going to do is take a single sample. This single random sample might include 50 eligible voters, but the resulting sample proportion of eligible voters that favor Candidate A is a single number we can call `p-hat`. This single sample proportion is a single dot on this distribution. \n",
    "\n",
    "<img src=\"images/confidence_intervals.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "Now the question is, is it a dot that is close to the population's true proportion or is it a dot that is very far away from the true proportion. I also want you to remember we don't actually know the true population proportion. So while our single sample is here, we don't know if the true population proportion is here or here or here. So, what are we actually doing? Well, let's imagine that this is the true population proportion distribution. We're going to take a sample and build an interval around that sample. And we're going to hope that the true population proportion falls within that interval. Now, what we really want to create is an interval, an interval of a certain length we will call `y`. This interval will have a lower limit and an upper limit. And this interval will be centered on our sample proportion, `p-hat`. This interval would need to be big enough where if we took 20 samples, 19 of the 20 intervals would contain the true population proportion. In other words, 95% of my samples would have an interval within range of `p`. \n",
    "\n",
    "So let's recap. When we gather a sample with a certain sample proportion `p-hat` and when we create an interval around that p-hat, we are 95% certain that the real population proportion is somewhere between the lower and upper limit of that interval.\n",
    "\n",
    "### 95% confidence intervals for population proportions\n",
    "So there's an upcoming election for mayor of a large city between two candidates. Let's simply call the candidates Candidate A and Candidate B. You work for Candidate A. Candidate A's campaign team wants to know where Candidate A stands so they ask you to conduct a poll. You gather a random sample of 100 voters and ask if they will be voting for Candidate A or Candidate B. 55% of the 100 voters polled said they planned on voting for Candidate A. Anything over 50% in the real election would result in a win for your candidate. So far, based on the results of the poll, things look promising for your candidate but remember, this was just one sample with a sample size of 100. \n",
    "\n",
    "<img src=\"images/confidence_intervals2.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "Now, look, I understand that my small pre-election poll likely didn't provide the actual percentage of votes Candidate A will get on the actual day of the election but maybe we're close. So let's create a 95% confidence interval. In other words, let's use our sample result to create an interval that very likely includes the actual percentage of votes Candidate A will get on election day. \n",
    "\n",
    "Let's take a look at a normal distribution curve. If we want a 95% confidence level, that would mean we'd want to capture 95% of the area under the curve between two points equidistant from the sample proportion which means 2.5% of the area under the curve on the right side of the curve would not be included and 2.5% of the area under the curve on the left side of the curve would not be included. So how do we find these two points which establish our interval? We'll have to take a look at `z-scores`. \n",
    "\n",
    "<img src=\"images/confidence_intervals3.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "`Z-scores tell us how many standard deviations away from the mean we would need to be to capture a certain percentage of the total distribution`. So we pull up a z-score table. \n",
    "\n",
    "<img src=\"images/confidence_intervals4.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "We find 0.975. This means 97.5% of the data points are to the left of this point and thus, 2.5% of the data points would fall to the right of this point. As you can see, our z-score is 1.96. This means if we go 1.96 standard deviations in the positive direction and 1.96 standard deviations in the negative direction, 95% of the area under our distribution will fall between these two limits. \n",
    "\n",
    "<img src=\"images/confidence_intervals5.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "So let's see where we are so far. `Our sample proportion is 0.55`. `This will be the center of our interval`. Therefore, the upper limit will be 0.55 plus 1.96 times our standard deviation and our lower limit will be 0.55 minus 1.96 times our standard deviation. So of course now, we need to find our standard deviation. \n",
    "\n",
    "In the absence of the population proportion and the population's standard deviation, we can use the formula for the standard error. It's basically the formula for the standard deviation but we use the sample proportion as our p hat. Since we use the sample proportion, p hat, instead of the population proportion, p, we can't call it the standard error. When you use the sample proportion, it's called `the sampling error`. So we put p hat into our formula and of course, in our particular example, we polled 100 voters so n is equal to 100. As you can see, our standard deviation or sampling error in this case is 0.05. \n",
    "\n",
    "<img src=\"images/confidence_intervals6.png\" alt=\"\" style=\"width: 300px;\"/>\n",
    "\n",
    "So when we plug that standard deviation into our formula, we get an interval that has a lower limit of 0.452 and an upper limit of 0.648. \n",
    "\n",
    "<img src=\"images/confidence_intervals7.png\" alt=\"\" style=\"width: 300px;\"/>\n",
    "\n",
    "Uh oh, our interval goes all the way down to 0.452 which means that our margin of error tells us that losing is still possible. Then again, a large proportion of our interval is over 50%. Still, your candidate is probably a bit nervous but how about if we took a bigger sample? \n",
    "\n",
    "How about if the campaign is willing to fund a poll of one thousand voters? The numbers on this poll are a bit lower for your candidate. This poll tells us 54% of the voters are for Candidate A. Let's calculate our sampling error with our new sample proportion and sample size of one thousand. \n",
    "\n",
    "<img src=\"images/confidence_intervals8.png\" alt=\"\" style=\"width: 300px;\"/>\n",
    "\n",
    "Our sampling error is now 0.16. So let's calculate our interval limits. \n",
    "\n",
    "<img src=\"images/confidence_intervals9.png\" alt=\"\" style=\"width: 300px;\"/>\n",
    "\n",
    "Our new interval stretches from 0.509 to 0.571. If the election were to have a result identical to anything within our 95% confidence interval, Candidate A would win. Remember, according to this sample, `there is now a 95% chance that on election day, Candidate A will receive between 50.9% and 57.1% of the vote`. There's only a 5% chance that the election day results will fall outside of that interval. And don't forget, it's possible that those 5% might include results that are even better than 57.1% of the vote for Candidate A. No matter how you slice it, that should make Candidate A's team feel pretty good, right? Yeah, there's always that one person on the team that asks, can we get a 96% interval or what about 98%? So for those people, next, we'll create confidence intervals that are greater than 95%. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do you want to be more than 95% confident?\n",
    "For some people, 95% just isn't good enough. So what happens if someone demands a 98% confidence interval? Well, let's remember a 95% confidence interval stretches in equal distances in opposite directions from our sample proportion. How far? Enough to include 95% of the probability distribution, which means that a 98% confidence interval would have to stretch a little bit farther so our interval would include 98% of the probability distribution. Notice my numbers didn't really get any better. It's more like saying, \"I'm 75% sure \"my lost car keys are in my living room, \"but I'm 99% sure my lost car keys \"are somewhere in this house. I simply increase the likely location of my keys and that increased the likelihood that this area contained my keys. \n",
    "\n",
    "So, when someone demands that we provide a 98% confidence interval instead of a confidence interval of 95%, it's important that they understand what the difference is between the two intervals. With that in mind, let's go ahead and figure out how to calculate the limits of this expanded interval. Remember, to find the limits of our 95% confidence interval, we used these formulas. \n",
    "\n",
    "<img src=\"images/confidence_intervals10.png\" alt=\"\" style=\"width: 300px;\"/>\n",
    "\n",
    "Our sample proportion, p-hat plus or minus our sampling error, which is really just our sample's standard deviation, times 1.96. Why 1.96? Because that was the appropriate z-score for 95%. So really, the only thing we will change to adjust our interval is the z-score of 1.96. But how do we find the right z-score for, let's say, 98%? Don't get fooled, you do not want the z-score for 0.98. You actually need the z-score for 0.99. Why? Well, let's take a look at our distribution. We want to set limits where 98% of the data is under the curve between the limits, so 2% of the distribution falls outside of the limits. 1% of the distribution on the right end of the curve and 1% on the left end of the curve. So we need to find the z-score for 0.99. Here's a z-score table. \n",
    "\n",
    "<img src=\"images/confidence_intervals11.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "Within the table, we are looking for 0.9900 or the closest number that is greater than that. In this case, the appropriate z-score is 2.33, and since the interval will stretch in equal distances in opposite directions, we will use 2.33 for both our upper and lower limit. \n",
    "\n",
    "<img src=\"images/confidence_intervals12.png\" alt=\"\" style=\"width: 300px;\"/>\n",
    "\n",
    "So, if we poll 1,000 people and 540 of those people favor Candidate A instead of Candidate B, then we know we have a p-hat of 0.54 and we know that n is equal to 1,000.\n",
    "\n",
    "<img src=\"images/confidence_intervals13.png\" alt=\"\" style=\"width: 300px;\"/>\n",
    "\n",
    "Using this formula, we can calculate our sampling error. \n",
    "\n",
    "<img src=\"images/confidence_intervals14.png\" alt=\"\" style=\"width: 300px;\"/>\n",
    "\n",
    "Our sampling error, therefore, is 0.017. \n",
    "\n",
    "We now have what we need to calculate our 98% confidence interval.\n",
    "\n",
    "<img src=\"images/confidence_intervals15.png\" alt=\"\" style=\"width: 300px;\"/>\n",
    "\n",
    "Look at that. If an election victory requires at least 50% of the vote, victory is still within our margin of error. I know, I know there's always the person that wants more. How about 99%? Using the same logic as with 98%, we realize we need to find the z-score for 0.995, which is 2.58. Let's calculate our 99% interval. \n",
    "\n",
    "<img src=\"images/confidence_intervals16.png\" alt=\"\" style=\"width: 300px;\"/>\n",
    "\n",
    "Here we see that the chance for a narrow election loss is within our margin of error. Nonetheless, it looks like, based on our simple random sample, we can feel fairly confident that an election win is likely. Still, even with this strong statistical evidence, the candidate can lose. If the candidate lost after we reported that a win seemed rather likely, how might that loss be explained? Let's try and figure that out next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explaining unexpected outcomes\n",
    "Suppose before an election, a polling organization reports a 95% confidence interval for candidate A. This confidence interval stretches from 0.51 to 0.54. In other words, the poll believes that Candidate A will get between 51% and 54% of the vote. Then election day comes around and Candidate A looses. Candidate A would probably be furious. Before the election, they were very confident of a win. And now they realize that they actually lost. \n",
    "\n",
    "How could this have happened? Well this is where it helps to be a well rounded statistician. `Beyond having a knowledge of the numbers and formulas, you need to understand the real environment that surrounds the poll`. In this case, it would be helpful if we understood how political polls are done and also the nature of the actual election. What might go wrong during the actual poll? \n",
    "- Lying, \n",
    "- Correspondents might want to throw off the polls, they may just lie. \n",
    "- Or perhaps, they're embarrassed to tell a pollster about their true opinions, and thus they would rather give them an answer that would please the pollster. \n",
    "- Maybe they didn't lie. Perhaps the respondent just changed their mind between the time of the poll and the actual date of the election. It's possible some people were unsure who they wanted to vote for on the day of the poll. But they chose a candidate for the poll just to please the pollster. \n",
    "- Perhaps, there were issue in gathering a random sample. The location of the poll. The people chosen. How they were chosen. The incentives used to entice more participants. \n",
    "- It takes a very experienced organization to gather a truly random sample. Sometimes, in an effort to influence voters that are still uncertain about who they will vote for, some politically biased polling organizations might actually seek biased polling results that they can use in the media to show that their candidate is popular among likely voters. These organizations may have had poor sample selection, poorly worded questions, or other questionable if not deceitful practices. \n",
    "\n",
    "As you can see, the polling process itself is filled with challenges. So let's move on to election day. What might go wrong on Election Day. \n",
    "- Bad weather. \n",
    "- A health epidemic. \n",
    "- Unsafe travel conditions. \n",
    "- Car trouble. \n",
    "- Work or family commitments. \n",
    "\n",
    "This is just a short list of reasons people might not be able to get to the voting booth on Election day. Perhaps between the time of the pre election poll, and election day, something changed. \n",
    "- An event occurred that changed the way voters made their decisions. \n",
    "- Perhaps a scandal involving one of the candidates was uncovered. \n",
    "\n",
    "Sometimes, voters just choose to stay home. Why? \n",
    "- They don't really care that much. \n",
    "- Maybe they just forgot. \n",
    "- Perhaps they heard the lines were long. \n",
    "- Maybe they had been watching the news, and analysts made it seem as though the outcomes were certain. \n",
    "- Perhaps the voter thought the election is a done deal. \n",
    "- My vote isn't going to make a difference at this point. \n",
    "\n",
    "Hopefully you aren't growing distrustful of statistics. If you investigate confidence intervals that have been reported over the last few decades, you'll see that very often, they are very accurate. Nonetheless, when the confidence interval misses the mark, it's important to know where the poll might have gone wrong. Actually, it's best to know these things before the study is even performed. `If you're looking to use confidence intervals to make important decisions, be sure you investigate how the study was done and which assumptions and simplifications were included in the development of the study. As I said, a good statistician has to know more than numbers and formulas. They need to really understand the environment that they're looking to measure`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 95% confidence intervals for population means\n",
    "Political campaigns rely heavily on developing confidence intervals for voter preference. In many cases, these types of confidence intervals are for proportions, but how about if we wanted to develop confidence intervals for these types of situations? \n",
    "- What's the average salary of a cardiologist? \n",
    "- What's the weight of the average grapefruit grown in the state of Texas? \n",
    "- How long does it take the average female, 20 to 29 years old, to run a mile? \n",
    "\n",
    "In these cases, we're not looking at proportions. Instead, `we're looking for means`. So, how do we create a confidence interval for a population mean? Well, it's not really that much different than the reasoning we use to develop confidence intervals for proportions. Here are the `formulas used to develop a 95% confidence interval for proportions`. \n",
    "\n",
    "<img src=\"images/confidence_intervals17.png\" alt=\"\" style=\"width: 300px;\"/>\n",
    "\n",
    "The sample proportion plus or minus 1.96, times the sample's sampling error. Remember, `1.96` is the appropriate `Z-Score for a 95% interval`. So, if we wanted a different confidence interval, we would just find the appropriate Z-Score. Now let's move from proportions to means. Here are the `formulas for developing a 95% confidence interval for means`. \n",
    "\n",
    "<img src=\"images/confidence_intervals18.png\" alt=\"\" style=\"width: 300px;\"/>\n",
    "\n",
    "Pretty much the same thing, except we substitute in the sample mean where we had the population proportion. \n",
    "\n",
    "So, suppose we wanted to create a 95% confidence interval for the average time it takes females, in their twenties, to run a mile. We could take a simple random sample of women in their twenties. We would then time their miles. Suppose these are the one mile run times in minutes for a simple random sample of nine women in their twenties. \n",
    "\n",
    "<img src=\"images/confidence_intervals19.png\" alt=\"\" style=\"width: 100px;\"/>\n",
    "\n",
    "So, all we need to do is find the sample mean. In this case, it's 8.96 minutes. The standard deviation for these nine times is 1.36. We then use this standard deviation, 1.36, to compute our sampling error. \n",
    "\n",
    "<img src=\"images/confidence_intervals20.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "If we plug these numbers into our formulas for the 95% confidence interval, we will get an interval from 8.08 minutes to 9.84 minutes. \n",
    "\n",
    "<img src=\"images/confidence_intervals21.png\" alt=\"\" style=\"width: 300px;\"/>\n",
    "\n",
    "According to this simple random sample, `there is a 95% chance that the population mean for the one mile run time for females in their twenties is somewhere between 8.08 minutes and 9.84 minutes`. Just as with our proportion intervals, by adjusting our sample size, which influences our sampling error, we can impact the size of the interval. And, of course, when we ask for an interval with a different confidence level, we can just adjust our Z-Score, which will also influence our confidence interval.\n",
    "\n",
    "You are now ready to develop confidence intervals for situations that use proportions as well as situations that require means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1\n",
    "Which of the following is NOT an explanation of why poll results may differ from an election outcome?\n",
    "- (correct) the group polled was a random sample\n",
    "- the respondents were still unsure at time of poll\n",
    "- a major event occurred between the poll date and the election date\n",
    "- the polling organization was biased\n",
    "\n",
    "### Example 2\n",
    "```\n",
    "When looking for a 95% confidence interval we need the z-score for ___.\n",
    "0.975\n",
    "In order for to have 0.95 within the interval, 0.05 would be outside the interval, 0.025 on each end of the interval. Thus a z-score of 0.975 would account for all but the 0.025.\n",
    "```\n",
    "\n",
    "### Example 3\n",
    "```\n",
    "A 95% confidence interval means that if a similar random sample were taken 20 times, we would expect the population mean to fall within the resulting range ____ times.\n",
    "19\n",
    "20 samples * 0.95 = 19 samples within the range.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis Testing\n",
    "\n",
    "Have you ever come upon situations, outcomes, or events that just seem odd? In a city made up of 51% women, where jury pools are said to be chosen at random, a certain jury pool of 50 people contains only eight women. A national restaurant chain provides a game piece for every drink a customer buys. There are 10 prizes worth over $100,000. Two of those prizes are won by relatives of restaurant employees. Three employees from a particular chemical factory with 400 employees are diagnosed with brain cancer in a two-year period. When you hear things like this, they make you think. It doesn't seem right. Is that even possible? And if it is possible, how likely is it that it could have happened at random? Sometimes these questions and the related answers could impact our careers and companies. They may help us make decisions. They might influence our superiors to act. Perhaps you work at a healthcare company. Your company has developed a drug to treat the common cold. It's reported that the average adult with the common cold will experience cold symptoms for about 8.5 days. When testing this new medicine on a random sample of 250 people with the common cold, it's found that these patients recovered about 1.2 days sooner that those that did not take this drug. Is this significant? Could this sample just be the result of chance, or did this drug have an impact? Should the drug be tested further? Does this mean this new drug should be approved for use? This is where hypothesis testing comes in. \n",
    "\n",
    "**Hypothesis testing** is an extremely popular method for exploring outcomes. In general, statisticians will: \n",
    "- make an assumption about a population. \n",
    "- collect a random sample from the population. \n",
    "- measure the sample. \n",
    "- see whether or not the sample measurement supports their assumption. \n",
    "\n",
    "It can be complex, but when done properly, hypothesis testing can be extremely powerful. Hypothesis testing, it really requires you to use all of your statistical muscles, but once you have hypothesis testing at your disposal, you'll be able to provide valuable input in almost any career. Science, medicine, business, education, public policy, and even sports and entertainment. No matter your field, make sure you understand the most basic elements of hypothesis testing.\n",
    "\n",
    "### How to test a hypothesis in four steps\n",
    "\n",
    "<img src=\"images/hypothesis_testing_01.png\" alt=\"\" style=\"width: 300px;\"/>\n",
    "\n",
    "The adult residents of a large town with an adult population of 35,000 are half male and half female. Each week, 50 adults are chosen at random to participate in jury duty. Women have complained that they are getting called to jury duty more often than the men. Jury administrators contend the system is random and fair. A committee is setup to investigate. They use the next jury pool as a sample. They find that in that pool of 50 potential jurors, 14 are men and 36 are women. The jury administration contends this happened by chance. A lobbying group disagrees. What we have here is an excellent opportunity to utilize hypothesis testing. \n",
    "\n",
    "**Hypothesis testing** is really a process. In our case, a four step process. \n",
    "\n",
    "<img src=\"images/hypothesis_testing_02.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "In our first step, we need to setup our hypotheses. There will typically be two hypotheses. H sub-zero or H not, this is our `null hypothesis`. We might refer to this as what we consider to be the status quo. In our case, this basically accepts that these jury numbers did happen by chance. In this hypothesis test, the null hypothesis states that everything's okay. And thus, the odds of a women being picked for the jury duty was at least 50%. So, our null hypothesis is p is less than or equal to 0.50. In other words, women had a 50% chance, or perhaps even less than a 50% chance, of being chosen for jury duty. \n",
    "\n",
    "Let's move to our `alternative hypothesis`, H sub-a, This would be the opposite of the null hypothesis. This one would say that women did not have a 50% chance of being chosen for jury duty. In fact, the chance of a women being chosen for the jury are greater than 50%. So here, our alternative hypothesis is p is greater than 0.50. \n",
    "\n",
    "<img src=\"images/hypothesis_testing_03.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "With our two hypotheses now stated, we'll then also want to state as `significance level`. Essentially, this sets a threshold for our test. In other words, suppose through our test we find that 36 or more women might end up on a 50 person jury pool by chance 30% of the time or 20% of the time. Or, what if it's only 10% of the time? Would you believe that this actually happened by chance or would you say, if it is below some `significance level alpha`, you would think that something was wrong. \n",
    "\n",
    "So, let's set our significance level at 5%. If 36 or more women ending up on a jury has less than a 5% chance of occurring at random, then, we will reject our null hypothesis. \n",
    "\n",
    "<img src=\"images/hypothesis_testing_04.png\" alt=\"\" style=\"width: 600px;\"/> \n",
    "\n",
    "In our second step, we look to find a `statistic` that will assess the validity of our null hypothesis. How could we see if this outcome, 36 women and 14 men, or an outcome that is even more extreme, could happen at random under these circumstances. Here we will use binomial probabilities. Our `p` here is equal to 0.50. That's the probability of a women being chosen for the jury panel. And, we will have 50 trials since that's how many seats there are on the jury panel. Finally, the number of successful trials will be 36. We have our null hypothesis. We also have our `test statistic`. Now, we find the `p value for that test statistic`. \n",
    "\n",
    "The `p value` is the probability that this outcome, 36 women and 14 men, or an outcome even more extreme, could occur by chance. So, we're looking for the probability that the number of women chosen for the jury panel would be 36 or more. What do we find? Whether you did the calculation the hard way, with a binomial table, with a spreadsheet, or perhaps with an online binomial calculator, you would find the probability of this particular outcome would be 0.0013 or 0.13%. \n",
    "\n",
    "<img src=\"images/hypothesis_testing_05.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "Those are some pretty long odds. So, our final step, time to compare our p value to our fixed significance level. \n",
    "\n",
    "<img src=\"images/hypothesis_testing_06.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "What we found was that assuming that the odds that a man and a women were equally likely to be chosen for a jury there was only a 0.13% chance that at random 36 or more women would be chosen for a panel of 50 potential jurors. Our fixed significance level, alpha, was 0.05 or 5%. Clearly, the p value fell short of our significance level and thus, we must reject the null hypothesis. This means, we believe that something is making it much more likely for a women to be chosen versus a man. \n",
    "\n",
    "Now, it doesn't prove that the cause is evil or intentional, nor does it prove that the cause is unintentional and innocent. It simply means, we reject the null hypothesis. Let's briefly talk about that too. Our only outcomes possible for this test would have been to reject the hypothesis, which is what we just saw. The alternative would have been do not reject the null hypothesis. Notice, that does not mean we said accept the hypothesis. We were looking to contradict the hypothesis. It's sort of like saying, a person on trial is guilty or not guilty. Guilty means the evidence is there to convict. Not guilty means there was a lack of evidence. Not guilty does not necessarily mean the jury believed the person was innocent, they just lacked the evidence to prove guilt. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-tailed vs. two-tailed tests\n",
    "Let's consider three different statements. \n",
    "- First, a recent national study found that the average American between the ages of 18 and 24 checks their phone 74 times per day. A mobile service provider questions these results. \n",
    "- Second, the average amount of time it takes an adult to recover from the common cold is 8.5 days. A new medicine was tested on a sample of adults suffering from the common cold. The average recovery time for the people in this group was 7.3 days. The company that developed this medicine thinks the drug should be considered for federal approval. \n",
    "- Finally, consider the national average for the college entrance exam, 1000 points. The Regent Test Prep Academy claims that their students consistently beat that national average. \n",
    "\n",
    "These are all situations where `hypothesis testing` would be useful. But each of these situations would require a different type of hypothesis test. Let's look at each situation individually. \n",
    "\n",
    "#### Scenario 1\n",
    "In our first situation, we had a claim that said people between the ages of 18 and 24 checked their phones 74 times per day. Some folks doubted that claim though. Notice, this group did not say the number was too high, nor did they contend the number was too low. They just expressed doubt in the stated average of 74 times per day. In this case, out hypotheses look like this. \n",
    "\n",
    "<img src=\"images/hypothesis_testing_07.png\" alt=\"\" style=\"width: 300px;\"/>\n",
    "\n",
    "Our null hypothesis, H sub zero, or H naught. Mu is equal to 74.0. Our alternative hypothesis H sub A, Mu is not equal to 74.0 if we look at our normal distribution, what we have is this, 74.0 is the mean of our null hypothesis. Let's say that we thought that anything more than 1.7 standard deviations from the mean in either direction, would mean we could reject our null hypothesis.\n",
    "\n",
    "<img src=\"images/hypothesis_testing_08.png\" alt=\"\" style=\"width: 300px;\"/>\n",
    "\n",
    "On the the other hand, anything that was less than 1.7 standard deviations, from the null hypothesis mean, would tell us that we could not reject the null hypothesis. \n",
    "\n",
    "<img src=\"images/hypothesis_testing_09.png\" alt=\"\" style=\"width: 300px;\"/>\n",
    "\n",
    "As you can see, we have two rejections areas here, one rejection area in the positive direction, greater than the mean. The other in the negative direction, less than the mean. This is considered a `two tailed test` because the null hypothesis is tested in both directions. \n",
    "\n",
    "#### Scenario 2\n",
    "On the other hand, in our example where the average person recovers from the cold in 8.5 days, our test group recovers in 7.3 days. This is a `one tail test`. Why? Well in this case, our hypotheses look like this. \n",
    "\n",
    "<img src=\"images/hypothesis_testing_10.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "Our null hypothesis H sub zero, Mu is greater than or equal to 8.5 days. Our alternative hypothesis H sub a, Mu is less than 8.5 days. So our null hypothesis is saying that patients do not recover faster with the drug and perhaps they may even take longer to recover. Both of these situations would indicate the medicine was not helpful. The alternative hypothesis is indicating that the medicine does in fact have an impact. \n",
    "\n",
    "<img src=\"images/hypothesis_testing_11.png\" alt=\"\" style=\"width: 300px;\"/>\n",
    "\n",
    "On our normal distribution graph, we have 8.5 as our null hypothesis mean. We can set two areas, 1.7 standard deviations from the mean. The difference here is that the drug can only be considered helpful if the patients actually get better faster. Which means that this area to the left, this small single tail, represents the area where we would reject the null hypothesis. \n",
    "\n",
    "<img src=\"images/hypothesis_testing_12.png\" alt=\"\" style=\"width: 300px;\"/>\n",
    "\n",
    "The large area the right would indicate that we could not reject the null hypothesis. That would be bad news for the drug company. \n",
    "\n",
    "#### Scenario 3\n",
    "So lets take a look at our test prep school. This example is very similar to the cold medicine example. The difference here is that we are looking for an increase in the test scores. Here are our hypotheses for this situation. \n",
    "\n",
    "<img src=\"images/hypothesis_testing_13.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "Our null hypothesis, H sub zero, H naught. Mu is less than or equal to 1000 points. Our alternative hypothesis H sub A, Mu is greater than 1000 points. Our null hypothesis is saying students of the Regent School do not see increased test scores. Instead they see average scores or perhaps even below average scores. The alternative hypothesis says that the Regent students do score over the national average. \n",
    "\n",
    "<img src=\"images/hypothesis_testing_14.png\" alt=\"\" style=\"width: 300px;\"/>\n",
    "\n",
    "What do we see on our normal distribution? Again, we see one tail. If we land in this area on the right, we would reject the null hypothesis. \n",
    "\n",
    "<img src=\"images/hypothesis_testing_15.png\" alt=\"\" style=\"width: 300px;\"/>\n",
    "\n",
    "If we land anywhere else in the large area to the left, we would not reject the null hypothesis. \n",
    "\n",
    "As you start to look for opportunities to utilize hypothesis testing, be sure you consider whether your hypothesis test is a one tailed test, or a two tailed test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Significance test for proportions\n",
    "A candidate's campaign finds that in a random sample of 500 eligible voters, 54% of those polled said they planned on voting for this candidate. The candidate needs over 50% of the vote to win the election. This candidate would like to test the hypothesis that he will win this election. Let's go through our four step process. \n",
    "\n",
    "<img src=\"images/hypothesis_testing_16.png\" alt=\"\" style=\"width: 300px;\"/>\n",
    "\n",
    "Step one, we're going to develop the hypotheses and state the significance level. \n",
    "\n",
    "<img src=\"images/hypothesis_testing_17.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "H sub zero, our null hypothesis, will be p is less than or equal to 0.50. This hypothesis states that the candidate would get 50% or less of the votes, and thus not have enough of the votes to win the election. H sub a, our alternative hypothesis, the candidate wins. This would be the opposite of the null hypothesis. This one would say that this candidate would get a majority of the vote and thus win the election. Our alternative hypothesis is p is greater than 0.5. \n",
    "\n",
    "<img src=\"images/hypothesis_testing_18.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "Our significance level for this test will be 5%. If this has less than a 5% chance of occurring, then we reject our null hypothesis. We're looking at a one-tailed test, where the rejection region is on the right-hand side of our distribution. If our proportion does not fall in the rejection region, we'll not have enough evidence to reject the null hypothesis. \n",
    "\n",
    "<img src=\"images/hypothesis_testing_19.png\" alt=\"\" style=\"width: 300px;\"/>\n",
    "\n",
    "So let's go to step two, we're going to identify the test statistic. In this situation, our test statistic is a `z-score`. We will call it `z sub p`. This `z-score` will establish the point on our distribution which divides the do not reject area from the rejection area. `p hat` is the sample proportion. `p sub zero` is the proportion from our null hypothesis. `n` is our sample size. \n",
    "\n",
    "<img src=\"images/hypothesis_testing_20.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "So in our case, p hat is equal to 0.54. p sub zero is equal to 0.50. And n our sample size is 500. And if we use these numbers, we get a z-score of 1.79. \n",
    "\n",
    "So now we move on to step three, our `p-value`. \n",
    "\n",
    "<img src=\"images/hypothesis_testing_21.png\" alt=\"\" style=\"width: 300px;\"/>\n",
    "\n",
    "So our test statistic z sub p is 1.79. If we look at this number on our z-score chart, you'll find that 1.79 leads you to 0.9633. So our p-value is 1 minus 0.9633 which gives us a p-value of 0.0367. \n",
    "\n",
    "Step four, so now we're going to compare our p-value to our fixed significance level. \n",
    "\n",
    "<img src=\"images/hypothesis_testing_22.png\" alt=\"\" style=\"width: 300px;\"/>\n",
    "\n",
    "Our fixed significance level, alpha, was 5% or 0.05. Our p-value was 0.0367. That's smaller than our 0.05 significance level, thus `we can reject the null hypothesis`. \n",
    "\n",
    "Graphically, we can look at this a few ways. Here's our distribution. \n",
    "\n",
    "<img src=\"images/hypothesis_testing_27.png\" alt=\"\" style=\"width: 300px;\"/>\n",
    "\n",
    "We established that the left side of the distribution is the do not reject the null hypothesis area. The right part of the distribution is the reject the null hypothesis area. Our alpha was 0.05, which means that 95% of the distribution was on the left side of the distribution, and 5% was to the right. \n",
    "\n",
    "We can also look at this by comparing z-scores. The z-score for 0.05 on a one-tailed test is 1.65. That would be 1.65 standard deviations from the null hypothesis population proportion, which was 0.50. \n",
    "\n",
    "<img src=\"images/hypothesis_testing_23.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "Our calculated z sub p though, was 1.79. 1.79 standard deviations from the population proportion. \n",
    "\n",
    "<img src=\"images/hypothesis_testing_24.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "Again, we land in this region on the right. So we reject the null hypothesis. The politician can breathe easy. Unless they demand `a hypothesis test with a 2% significance level`. So here is where the 2% significance level would be on our distribution. \n",
    "\n",
    "<img src=\"images/hypothesis_testing_25.png\" alt=\"\" style=\"width: 300px;\"/>\n",
    "\n",
    "Here is where our p-value of 0.0367 would put us. \n",
    "\n",
    "If we wanted to use our z-scores, the z-score for 0.02 on a one-tailed test is 2.06. That would be 2.06 standard deviations from the null hypothesis population proportion which was 0.50. Our calculated z sub p though was 1.79. \n",
    "\n",
    "<img src=\"images/hypothesis_testing_26.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "No matter the method, we are now in the do not reject the null hypothesis area. In this case, the hypothesis test tells us `we cannot reject the hypothesis` that the candidate will get 50% or less of the vote. As you can see, this hypothesis test hinged on significance level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Significance test for means (acceptance sampling)\n",
    "K-Nosh is a national gourmet dog food company. They sell thousands of bags of dog food each day. They sell dog food in eight, 20, and 40-pound bags. And the 20-pound bag is by far the most popular size. K-Nosh's high-end customers demand outstanding products and excellent service. Customers don't want a bag with less than 20 pounds. So while the bag is labeled as 20 pounds, K-Nosh sets the desired weight of each bag at 20.15 pounds to ensure customers get at least 20 pounds in each bag. \n",
    "\n",
    "<img src=\"images/hypothesis_testing_28.png\" alt=\"\" style=\"width: 300px;\"/>\n",
    "\n",
    "Each day, K-Nosh employees pull a random sample of 100 bags out of the thousands they ship. Based on the 100-bag sample, they will either send out the shipment or they will reject the shipment for that day. Today's sample had an average weight of 20.10 pounds, and the population standard deviation is 0.26 pounds. So let's start our four-step process. \n",
    "\n",
    "Step one, develop the hypotheses and state the significance level. So, let's develop our hypotheses. \n",
    "\n",
    "<img src=\"images/hypothesis_testing_29.png\" alt=\"\" style=\"width: 300px;\"/>\n",
    "\n",
    "Our null hypothesis, H sub zero or H-naught, mu is greater than or equal to 20.15 pounds. This hypothesis states that the bags of dog food are equal to or greater than 20.15 pounds. It's what we would consider the standard state. Our alternative hypothesis, H sub a, this one says the bags of dog food weigh less than 20.15 pounds. This would be the opposite of the null hypothesis. Our alternative hypothesis is mu is less than 20.15 pounds. As usual, we will see whether or not we will reject the null hypothesis. If we reject the null hypothesis, that would mean K-Nosh would not make any shipments of 20-pound bags on that date. \n",
    "\n",
    "<img src=\"images/hypothesis_testing_30.png\" alt=\"\" style=\"width: 300px;\"/>\n",
    "\n",
    "Our significance level for this test will be five percent. If this has less than a five percent chance of occurring, then we reject our null hypothesis. We're looking at a one-tail test where the rejection region is on the left-hand side of our distribution. If our sample falls in the rejection region, we will reject the entire shipment. \n",
    "\n",
    "So now we move on to step two, identify the test statistic. \n",
    "\n",
    "<img src=\"images/hypothesis_testing_31.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "In this situation, our test statistic is a z-score. This `z-score` will establish the point on our distribution which divides the \"do not reject\" area from the rejection area. `x-bar` is the sample mean. `Mu` is the mean from our null hypothesis. `n` is our sample size. And `sigma` is the population standard deviation. In our case, x-bar was equal to 20.10 pounds. Mu was 20.15 pounds. n, our sample size, is 100. And sigma, the population standard deviation, is 0.26 pounds. So if we use these numbers, we get a z-score of -1.92. \n",
    "\n",
    "<img src=\"images/hypothesis_testing_32.png\" alt=\"\" style=\"width: 300px;\"/>\n",
    "\n",
    "Step three, our p-value. Our test statistic z is -1.92. If we look this up on our z-score chart, you will find that -1.92 leads you 0.0274 or 2.74%. That is our p-value, 0.0274. \n",
    "\n",
    "<img src=\"images/hypothesis_testing_33.png\" alt=\"\" style=\"width: 300px;\"/>\n",
    "\n",
    "Step four, we're going to compare our p-value to our fixed significance level. Our fixed significance level alpha was five percent or 0.05. Our p-value was 0.0274. That's smaller than our 0.05 significance level. Thus, we have to reject the null hypothesis. \n",
    "\n",
    "<img src=\"images/hypothesis_testing_35.png\" alt=\"\" style=\"width: 300px;\"/>\n",
    "\n",
    "Our alpha was 0.05 which means that 95% of the distribution was on the right side of the alpha and five percent was to the left. Remember, we want to be close to our goal of 20.15 pounds. If we're too far to the left of 20.15 pounds, the bags are likely too light. If we compare z-scores, the z-score for 0.05 on a one-tail test is -1.65. That would be 1.65 standard deviations from the null hypothesis mean, 20.15. Our calculated z though was -1.92, 1.92 standard deviations from the mean. No matter how you look at it, `we must reject the null hypothesis`. The bags are too light. And so, we must reject the entire shipment. I'm guessing some of you might see this as harsh. But believe it or not, this quality control technique which is called acceptance sampling was very popular in the past and is still used in some industries today."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type I and type II errors\n",
    "In our hypothesis tests, we've always set up a null hypothesis and an alternative hypothesis. The null hypothesis typically assumes that the status quo prevails. The null hypothesis might state that the system works, it might tell us that nothing has changed in our system. Our alternative hypothesis assumes the opposite. The alternative hypothesis might tell us that the system is broken. It might tell us that things have changed. \n",
    "\n",
    "Let's use a special type of cancer screening test as an example. This fictional screening would provide a reading based on your blood. The average reading is 100. People that get a reading over 125 get a positive result. This would indicate they have cancer. If we were going to equate this to a hypothesis test, we would say the cancer screening had two hypotheses. \n",
    "\n",
    "<img src=\"images/hypothesis_testing_36.png\" alt=\"\" style=\"width: 300px;\"/>\n",
    "\n",
    "The null hypothesis would be that everything is okay. The person being tested does not have cancer. The alternative hypothesis would state that the person being tested does, in fact, have cancer. Let's say that the incidence of cancer is normally distributed. \n",
    "\n",
    "<img src=\"images/hypothesis_testing_37.png\" alt=\"\" style=\"width: 300px;\"/>\n",
    "\n",
    "So, if we were going to look at this on a normal distribution, we might say that 100 is the mean. Anything to the right of 125 would be considered a positive result for cancer. So, left of 125, we do not reject the null hypothesis, but to the right, we would reject the null hypothesis. \n",
    "\n",
    "Up until now, we've assumed that if you are beyond 125, the patient has cancer, but remember, even if 125 represented an alpha of 0.02 or 2%, it would mean that it is extremely unlikely that someone with a reading over 125 is cancer-free. It's unlikely, but with an alpha of 2%, it's not impossible. Just as political polls sometimes predict the wrong candidate to win, cancer screening tests also make mistakes. But there are two types of mistakes or errors. \n",
    "\n",
    "Let's look at this small grid. \n",
    "\n",
    "<img src=\"images/hypothesis_testing_38.png\" alt=\"\" style=\"width: 300px;\"/>\n",
    "\n",
    "At the top, we see the true state of the system. The patient does not have cancer, which agrees with our null hypothesis. And the patient has cancer, this would agree with our alternative hypothesis. Along the side, we have the two possible outcomes of the test. The test comes back positive, which means that according to the test, they have cancer. This is the equivalent of rejecting our null hypothesis. \n",
    "\n",
    "How about the second outcome for our screening test? The test comes back negative, which means that according to the test, they do not have cancer. This is the equivalent of not rejecting our null hypothesis. \n",
    "\n",
    "Now, let's look at the possible results. If we get a negative test and the patient does not have cancer, the hypothesis test worked. If we get a positive test and the patient actually has cancer, the hypothesis test worked. But how about these other two quadrants? It's possible a person might get a positive test but not actually have cancer. This is what we would call a `Type One Error`. Typically, we refer to this as a `false positive`. This is the same as a person getting a reading over 125, but not actually having cancer. If we start to see lots of Type One Errors, Perhaps our screening test is not sensitive enough. You might start to question if there are better ways of testing the null hypothesis. \n",
    "\n",
    "The opposite is also possible. A person might get a negative test, even though they do have cancer. This is what is called a `Type Two Error`. This might also be referred to as a `false negative`. This would be the same as a person getting a reading under 125, even though they have cancer. If we start to see lots of Type Two Errors, that may mean our screening test is too sensitive. Again, we may need to question how we are testing our null hypothesis. \n",
    "\n",
    "Hypothesis tests, even when they are done the right way, can be flawed. So, it's important to understand that a hypothesis test might make a mistake. And by knowing the different types of errors, Type One and Type Two, it can help you in developing and interpreting our hypothesis tests and the subsequent results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1\n",
    "```\n",
    "What is the probability that AT LEAST 36 of 50 randomly chosen people will be women, where the probability of choosing a woman is p=0.5? (Assuming the choosing of each person is independent of the others)\n",
    "This is a binomial (discrete) distribution problem.\n",
    "\n",
    "1. How many DIFFERENT combinations of choosing X women out of the 50 people are there? \n",
    "This would be C(50, X) = 50! / (X!*(50-X)!)\n",
    "\n",
    "2. What is the probability of getting ONE of these choices? \n",
    "This would be p^X*(1-p)^(50-X), which would be in our case 0.5^X*0.5^(50-X)=0.5^50\n",
    "\n",
    "So what is the probability of getting ANY such choice? \n",
    "This would be C(50, X)*0.5^50\n",
    "\n",
    "What is the probability of choosing 36 women and 14 men? C(50, 36)*0.5^50\n",
    "What is the probability of choosing 37 women and 13 men? C(50, 37)*0.5^50\n",
    ".....\n",
    "What is the probability of choosing 49 women and one man? C(50, 49)*0.5^50\n",
    "What is the probability of choosing 50 women and zero men? C(50, 50)*0.5^50=1 (because 0!=1) \n",
    "\n",
    "The sum of these, is the probability of choosing AT LEAST 36 women (choosing 36 OR MORE women) which is ~0.13%\n",
    "```\n",
    "\n",
    "### Example 2\n",
    "A smoke detector fails to beep even though there was a fire. This is an example of a:\n",
    "- true negative\n",
    "- (correct - see the errors table above) type II error\n",
    "- false positive\n",
    "- type I error\n",
    "A type II error is when we have a fire but the alarm does not sound.\n",
    "\n",
    "### Example 3\n",
    "If our significance level is 5% and our p-value is calculated as 0.016 we should _____.\n",
    "reject the null hypothesis\n",
    "If the p-value is below our stated significance level you must reject the null hypothesis.\n",
    "\n",
    "### Example 4\n",
    "What is the first step in a hypothesis test?\n",
    "The first step is to develop the hypothesis which will be tested and identify the significance level.\n",
    "\n",
    "### Example 5\n",
    "```\n",
    "You measure a value of 1.73 for a random variable with a mean of 2.20 and a standard deviation of 0.22. \n",
    "What is the z-value of the measurement?\n",
    "z = (2.20-1.73)/0.22\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Small Sample Sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T-statistic vs. z-statistic\n",
    "Up until now, we've used the `z-score` to help us identify how many `standard deviations` a data point might lie from the population mean. It's also been very helpful in developing `confidence intervals`. Now remember, the `z-score` requires that our data be normally distributed. It also requires that we know the `standard deviation` of the population. The central limit theorem tells us that given enough iterations, the mean of our sample will be normally distributed. But often, the population `standard deviation` is unknown. So how can we create confidence intervals when the population standard deviation is unknown? \n",
    "\n",
    "<img src=\"images/t-statistic_01.png\" alt=\"\" style=\"width: 300px;\"/>\n",
    "\n",
    "Believe it or not, you can use the standard deviation of a single sample. But if you have only one sample with a sample size under 30, a relatively small sample size, you can probably guess that your confidence interval will suffer, and this is why the `z-score` is not valid in this situation. \n",
    "\n",
    "If you're creating a confidence interval when the population variance is unknown, you must instead use something called the `t-distribution`. Before we discuss the differences between the z- and t-distributions, let's first discuss how they are similar. \n",
    "\n",
    "<img src=\"images/t-statistic_02.png\" alt=\"\" style=\"width: 300px;\"/>\n",
    "\n",
    "Both are symmetrical, bell-shaped distributions. Both require data with a normal distribution. And in both cases, the area under the curve is equal to 1.0. So how is the t-test different from our z-test? Well, the z-test is mostly used to compare the mean of a sample to its larger population. The sample comes from the population, so the means of the sample and population are intertwined. On the other hand, the `t-test` compares two completely independent samples. They don't have to come from the same population. \n",
    "\n",
    "So because of these differences, and also because of the small sample size, the `t-distribution` isn't one curve but rather a series of curves. Each curve is representative of the distribution for different sample sizes. The smaller the sample size, the flatter the curve. The larger the sample size, the closer it gets to the `z-distribution`, which we use for the standard normal curve. \n",
    "\n",
    "<img src=\"images/t-statistic_03.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "Since all of the t-distribution curves are flatter than the z-distribution curve, `the critical scores for t-distributions are higher than those for z-distributions`. You might remember that the appropriate z-score for a 95% confidence interval was 1.96. That's 1.96 standard deviations. How does that compare to t-scores? Well, it depends on the sample size. \n",
    "\n",
    "<img src=\"images/t-statistic_04.png\" alt=\"\" style=\"width: 300px;\"/>\n",
    "\n",
    "For a sample size of three, the t-score is 4.303. For a sample size of 10, the t-score is 2.262. For a sample size of 20, the t-score is 2.093. And by the time our sample size is equal to a hundred, our t-score goes to 1.98. As you can see, `the larger our sample size, the closer the t-score is to the z-score of 1.96`. So where do we get t-scores for all of the different possible sample sizes? Let's take a look at that next.\n",
    "\n",
    "### T-score tables and degrees of freedom\n",
    "Since `T-distributions rely on the standard deviation of a sample`, instead of the standard deviation of the population, there is a greater level of uncertainty when creating confidence intervals. As a result, the `z-scores we gather from a z-distribution chart are not sufficient`. Instead, we need to utilize `t-distribution charts`. There's not one single t-distribution chart, but rather multiple charts. Remember, the curve associated with a t-distribution is dependent on the sample size. The smaller the sample size, the flatter the distribution curve, and the greater the uncertainty. \n",
    "\n",
    "<img src=\"images/t-statistic_05.png\" alt=\"\" style=\"width: 300px;\"/>\n",
    "\n",
    "The larger the sample size, the closer the curve gets to the normal distribution. \n",
    "\n",
    "<img src=\"images/t-statistic_06.png\" alt=\"\" style=\"width: 300px;\"/>\n",
    "\n",
    "That is why for each sample size, we need a different t-score distribution table. Just in case you forgot, here's a snapshot of just one part of a z-score table. \n",
    "\n",
    "<img src=\"images/t-statistic_07.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "So imagine having a different table for each unique sample size. Yep, that would be a lot of really big tables. Most of the time, however, we're looking for the most common confidence intervals. 90%, 95%, 99%. That's why you're more likely to see t-distribution tables that look like this.\n",
    "\n",
    "<img src=\"images/t-statistic_08.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "As you can see, along the top is the confidence level, which is also given as the equivalent of a one- or two-tailed test. Along the left side, you have a column labeled as `df`. This stands for `degrees of freedom`. What are degrees of freedom? The easy answer is that `degrees of freedom` is just our sample size minus one, also referred to as `n-1`. `N` is the amount of data points in our sample. \n",
    "\n",
    "<img src=\"images/t-statistic_09.png\" alt=\"\" style=\"width: 300px;\"/>\n",
    "\n",
    "So, for a sample size of five, we have four degrees of freedom. For a sample size of 10, we have nine degrees of freedom. There's a more complex answer, too, but let's leave that for another day. \n",
    "\n",
    "Let's go back to our t-distribution table. `Once we have our sample size, we have our degrees of freedom`. Let's suppose we want a 95% confidence interval, and that our sample size was four, which means we have three degrees of freedom. We isolate the column for 95%, we find the row for three degrees of freedom, and the intersection of that row and column bring us to our critical `t-score`, 3.182. \n",
    "\n",
    "<img src=\"images/t-statistic_10.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "How about if our sample size is 10? Now we look at the row for nine degrees of freedom. Our critical t-score is now 2.262. As you can see, `as our sample size gets larger, our critical t-score gets smaller, because the larger sample size is associated with the curve that is closer to our normal z-distribution`. So, now that you can find t-scores, let's use this to create some confidence intervals.\n",
    "\n",
    "<img src=\"images/t-statistic_11.png\" alt=\"\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating confidence intervals using t-scores\n",
    "Let's develop a confidence interval using t scores. Remember, that this confidence interval gives us a range of values for estimated population parameter. \n",
    "\n",
    "Imagine that a national testing organization has made some major changes to the standardized exam that most aspiring college students take. The exam scores range between 50 points and 200 points. The old exam typically had an average score of 130 points. They like to see how the average score for the updated exam compares to the old version of the exam. Further, this testing organization wants to create a **98% confidence interval** for the updated exam's mean score. \n",
    "\n",
    "In order to do so, they gave the updated test to a random sample of 10 aspiring college students. The scores for these 10 students are as follows. \n",
    "\n",
    "<img src=\"images/t-statistic_12.png\" alt=\"\" style=\"width: 300px;\"/>\n",
    "\n",
    "Our sample mean is 126, while we don't know the standard deviation of the exam scores for the entire population of aspiring college students, we can calculate the standard deviation for this sample, 29.51. We also know that since our sample size N was 10, our degrees of freedom is N minus one. In this case, that would be nine. \n",
    "\n",
    "We'd like to read a 98% confidence interval. So, when we go to our T distribution table, we find that the critical t score for a 98% confidence interval with nine degrees of freedom is 2.821. \n",
    "\n",
    "<img src=\"images/t-statistic_13.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "So, to calculate our confidence interval, we use these formulas. \n",
    "\n",
    "<img src=\"images/t-statistic_14.png\" alt=\"\" style=\"width: 300px;\"/>\n",
    "\n",
    "We have our sample mean, 126. We found our t score, 2.821. So now, we need to find our standard error. For this, we'll use this formula. \n",
    "\n",
    "<img src=\"images/t-statistic_15.png\" alt=\"\" style=\"width: 300px;\"/>\n",
    "\n",
    "Remember, our standard deviation of our sample was 29.51. Our sample size N is 10. Therefore, we can calculate our standard error, 9.33. \n",
    "\n",
    "Now, we have everything we need to calculate our 98% confidence interval. \n",
    "\n",
    "<img src=\"images/t-statistic_16.png\" alt=\"\" style=\"width: 300px;\"/>\n",
    "\n",
    "Our upper and lower control limits will be 126 plus and minus 2.821 times 9.33 which means our 98% confidence interval for the mean of the exam scores for the updated standardized exam stretches from about 99.7 to 152.3. Again, that means the we're 98% certain that the population mean lies between those two values. \n",
    "\n",
    "<img src=\"images/t-statistic_17.png\" alt=\"\" style=\"width: 300px;\"/>\n",
    "\n",
    "That's a rather big spend for an exam where scores can only be as low as 50 and where they can only be as high as 200. \n",
    "\n",
    "Suppose, we were content with a **95% confidence interval**, we can go back to our t distribution table. We find that the critical t score for a 95% confidence interval with nine degrees of freedom is 2.262. We plug in this new value into our confidence interval limit formulas and we get our upper and lower limits are 126 plus or minus our critical t score, 2.262 times 9.33. Our 95% confidence interval would be from about 105 to 147. \n",
    "\n",
    "<img src=\"images/t-statistic_18.png\" alt=\"\" style=\"width: 300px;\"/>\n",
    "\n",
    "Still pretty big but nonetheless, a bit smaller. You're probably thinking, that is still a huge confidence interval. Well, I think `the big lesson here is that without the availability of the population standard deviation, a much larger sample size is needed to provide us with a more meaningful confidence interval`. So, perhaps, this testing organization should go back and administer the updated exam to a much larger random sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1\n",
    "```\n",
    "What is the standard error for a sample size of 10 and a standard deviation of 3.64?\n",
    "The standard error is the standard deviation divided by the square root of the sample size; it reflects uncertainty in the estimate of the mean.\n",
    "SE = 3.64 / sqrt(10) = 1.15\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Two Populations (Proportions)\n",
    "\n",
    "### Explanation of two populations\n",
    "Confidence intervals and hypothesis testing. You've now been introduced to both of these important statistical foundations. `Confidence intervals` allow us to take a single sample and create an interval, which we're fairly confident contains the population proportion. `Hypothesis testing` allows us to see if this one sample was likely the result of chance, or if an external force may have impacted the sample data. \n",
    "\n",
    "We're now moving on to `comparing two populations`. We'll look to answer questions like: \n",
    "- Does taking aspirin reduce the chance of a heart attack? \n",
    "- Are young male drivers more likely to get into car accidents than young female drivers? \n",
    "- Are people in Los Angeles more likely to be victims of violent crime than people in New York City? \n",
    "- Are male high school teachers more likely to have higher salaries than female high school teachers? \n",
    "\n",
    "<img src=\"images/two_populations_prop_01.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "Notice, we keep using the wording **\"more likely\"**. Even with our comparisons, we can't be sure, but we can create confidence intervals. But, `what makes all of these questions similar is that each situation can be analyzed by comparing two independent random samples`. One from each population: an `experimental population` and a `control population`. \n",
    "\n",
    "<img src=\"images/two_populations_prop_02.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "- Those that take aspirin versus those that take a placebo. The placebo is the control group. \n",
    "- A sample of young male drivers versus a sample of young female drivers. In this case, either gender can be used as the control. \n",
    "- A sample of citizens of Los Angeles versus a sample of New Yorkers. In this case, either city could be used as a control. \n",
    "- And of course, a sample of male high school teacher salaries versus a sample of female high school teacher salaries. Again, either gender can be used as the control. \n",
    "\n",
    "In this first section, we will look at the comparison of two proportions of two independent populations. We'll use our knowledge of basic proportions. We'll work to create a confidence interval for the difference between these two population proportions, and finally, we will use hypothesis testing to compare the difference between the proportions for each independent sample. Yeah, I know, that sounds like a whole lot of work, but rather than just talk about it, let's walk through a problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up a comparison\n",
    "So, let's compare two independent populations in an effort to figure out if a new drug is effective in reducing the chance of a heart attack. In real life, this testing would take many years. There are several different phases for drug testing but let's ignore that for now. Imagine the a drug company gathers a large number of subjects. The subjects are randomly placed into two groups. One group of subjects is given this new drug. The other group of subjects is given a placebo, a pill with no medicinal value.\n",
    "\n",
    "<img src=\"images/two_populations_prop_03.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "Suppose, these were the results of this long-term study. \n",
    "\n",
    "<img src=\"images/two_populations_prop_04.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "For a new drug group, we had a sample size of 2,219. 26 of those had a heart attack. So, our p-hat here is 26 divided by 2,219. For our placebo group, we had a sample size of 2,035. 46 of those people had a heart attack. So, in this case, our p-hat is 46 divided by 2,035. The sample group that took the drug had heart attacks at a rate of 0.0117 or 1.17%. The sample group that took the placebo had heart attacks at a rate of 0.0226 or 2.26%. \n",
    "\n",
    "`Remember, these are just samples. So, while the central limit theorem tell us these rather large samples are likely to be representative of the population, they are still only samples`. What we found was that the difference in the proportions of the samples was p-hat one minus p-hat two. \n",
    "\n",
    "<img src=\"images/two_populations_prop_05.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "In this case, that's 0.0226 minus 0.0117 which gives us 0.0109. A 1.09% difference between the two sample proportions. What we'd like to know is, what is the true difference between the rate of heart attacks for the population when they take the new drug versus when they take the placebo. Since the drug company does not likely have the means to do this type of test, they would have to create a confidence interval. The confidence interval formula looks just like the formulas we've already been using for confidence intervals. \n",
    "\n",
    "<img src=\"images/two_populations_prop_06.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "Let's start filling in our values. Suppose we want a 95% confidence interval, we go to a Z distribution chart and find a critical Z score of 1.96. \n",
    "\n",
    "<img src=\"images/two_populations_prop_07.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "That number probably looks familiar by now. We also know the observe difference from our samples. We calculated this to be 0.0109. So now, all we need is our standard error. \n",
    "\n",
    "<img src=\"images/two_populations_prop_08.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "We have everything we need, p-hat one is the sample proportion for our placebo group, 0.0226, n1 is the sample size for this group, 2,035, p-hat two is the sample proportion for our new drug group, 0.0117, and n2 is the sample size for this group, 2,219. When we plug in all of our numbers, we find that our standard error is 0.0040. \n",
    "\n",
    "<img src=\"images/two_populations_prop_09.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "And now, we can calculate the limits of our confidence interval. Our upper limit and lower limit are just 0.0109 plus or minus 1.96 or critical value times 0.004. Therefore, our upper limit is 0.0188. Our lower limit is 0.0030. \n",
    "\n",
    "<img src=\"images/two_populations_prop_10.png\" alt=\"\" style=\"width: 300px;\"/>\n",
    "\n",
    "What do these numbers mean? `It means that we are 95% confident that the new drug reduces the population's chance of having a heart attack by somewhere between 0.3% and 1.88%`. In other words, we are 95% confident this new drug is more effective than the placebo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis testing\n",
    "Before we prepare our hypothesis test, let's briefly recap our example. A company is trying to figure out if a new drug is effective in reducing the chance of a heart attack. The company gathers a large number of subjects. The subjects are randomly placed into two groups. One group of subjects is given this new drug. The other group of subjects is given a placebo. The people in the study are not to be told if they are getting the new drug or the placebo. The results of the study were as follows. \n",
    "\n",
    "<img src=\"images/two_populations_prop_04.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "For the new drug, we had a sample size 2,219. 26 of those people had a heart attack, so our p-hat was 26 divided by 2,219. For our placebo group, we had a sample size of 2,035. In this case, 46 people had a heart attack, so our p-hat here was 46 divided by 2,035. The results and the resulting 95% confidence interval both provide evidence that the new drug did help reduce the rate of heart attacks.\n",
    "\n",
    "<img src=\"images/two_populations_prop_11.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "The question we have is, what's the probability that our results happened by chance? In other words, we had 4,254 people in the study. 72 of those people suffered a heart attack. `What's the probability that even without the drug or placebo these same people would have suffered heart attacks?` Perhaps these 4,254 people were randomly put into two groups and one group just happened to get a lot more of the people that would end up getting heart attacks. \n",
    "\n",
    "<img src=\"images/two_populations_prop_12.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "This is extremely important to think about, especially for `random sampling` and `random assignment`. So let's go ahead and perform a hypothesis test. \n",
    "\n",
    "<img src=\"images/two_populations_prop_13.png\" alt=\"\" style=\"width: 300px;\"/>\n",
    "\n",
    "A hypothesis test evaluates two mutually exclusive statements about a population to determine which statement is best supported by the sample data. \n",
    "\n",
    "<img src=\"images/two_populations_prop_14.png\" alt=\"\" style=\"width: 300px;\"/>\n",
    "\n",
    "So for step one, let's develop the hypotheses and state the significance level. Our null hypothesis, H-naught, population proportion of the placebo group minus the population proportion of our new drug group is equal to zero. In other words, the new drug had no effect. Both proportions are identical. Our alternative hypothesis, H sub one, here, our population proportion for the placebo group minus the population proportion for our new drug group is not equal to zero. This means that the proportion of heart attacks for people that took the new drug was different than that of those that took the placebo. This would indicate the new drug had some effect. Let's set our `significance level at 5%`. If there is less than a 5% chance the results of our study could have happened by chance, then we will reject our null hypothesis. \n",
    "\n",
    "<img src=\"images/two_populations_prop_14.png\" alt=\"\" style=\"width: 300px;\"/>\n",
    "\n",
    "So let's go to step two. Let's identify the test statistic. As in previous hypothesis tests the test statistic will be a Z-statistic. In this case, we are using the `Z-statistic for a hypothesis test for the difference between two proportions`. \n",
    "\n",
    "<img src=\"images/two_populations_prop_15.png\" alt=\"\" style=\"width: 300px;\"/>\n",
    "\n",
    "<img src=\"images/two_populations_prop_16.png\" alt=\"\" style=\"width: 300px;\"/>\n",
    "\n",
    "Yes, a very ugly formula, so let's start plugging in the numbers. The numerator of this formula is looking for the difference between our sample proportions and the true population proportions. Remember, we're testing the null hypothesis. The null hypothesis assumes that the difference between the two populations should be zero. So we can eliminate the second half of our numerator. \n",
    "\n",
    "<img src=\"images/two_populations_prop_17.png\" alt=\"\" style=\"width: 300px;\"/>\n",
    "\n",
    "The first part of the numerator is just the difference between our two samples. Proportion of heart attacks for the placebo was 0.0226. The proportion of heart attacks for the new drug group was 0.0117. Therefore, our numerator is 0.0109. \n",
    "\n",
    "<img src=\"images/two_populations_prop_18.png\" alt=\"\" style=\"width: 300px;\"/>\n",
    "\n",
    "How 'about that scary denominator with the square root? This is actually our standard error. Let's fill in our two sample sizes first, 2,219 for the new drug group, 2,035 for the placebo group. That leaves us with p-hat for the placebo and new drug, which again, are 0.0117 and 0.0226 respectively. When we calculate our denominator, which is the standard error, we get 0.004. Our numerator is 0.0109. Therefore, our Z-statistic is 2.725. \n",
    "\n",
    "<img src=\"images/two_populations_prop_19.png\" alt=\"\" style=\"width: 100px;\"/>\n",
    "\n",
    "So let's see what this looks like. We have our normal distribution. If the null hypothesis were true, the difference between the two populations would be zero. In our samples, we found that the new drug group had a lower proportion of people that suffered heart attacks. The difference between the two groups was 0.0109. \n",
    "\n",
    "<img src=\"images/two_populations_prop_20.png\" alt=\"\" style=\"width: 300px;\"/>\n",
    "\n",
    "This is a two-tailed test with a 5% significance level. The Z-score for this would be 1.96. This means that if our actual outcome were more than 1.96 standard deviations from the expected outcome, then we must reject our null hypothesis. Our result was 2.725 standard deviations from the expected outcome. In fact, by looking at a Z distribution chart, 2.725 corresponds with an outcome that is 0.3% likely. \n",
    "\n",
    "<img src=\"images/two_populations_prop_21.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "We have to reject our null hypothesis. In other words, we can feel fairly confident that the positive results exhibited by the group that took the new drug did not occur by chance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1\n",
    "The two hypotheses to be tested _____.\n",
    "- must be logically equivalent\n",
    "- (correct) cannot both be true\n",
    "- must have differing probabilities of truth\n",
    "- must both be true\n",
    "\n",
    "The hypotheses must be mutually exclusive.\n",
    "\n",
    "### Example 2\n",
    "How should populations differ when we wish to investigate an effect?\n",
    "- They should differ by at least five known variables.\n",
    "- They should differ in size.\n",
    "- (correct) They should differ by one or more known variables.\n",
    "- They should differ by country of origin.\n",
    "\n",
    "We should control at least one variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Two Populations (Means)\n",
    "\n",
    "### Basics of comparing two population means\n",
    "Consider these situations: \n",
    "- A large national corporation has a hundred senior executives. About 40 of those senior executives are women. The other 60 are men. It is found that the average salary for male senior executives is about $15,000 per year greater than the salaries of the female senior executives. Why are male senior executives at this company paid higher salaries? Does the gender of the senior executives play a role? \n",
    "\n",
    "<img src=\"images/two_populations_mean_01.png\" alt=\"\" style=\"width: 400px;\"/>\n",
    "\n",
    "- A hundred obese males in their 20s are randomly assigned to two groups for a period of three months. One group of males exercise two hours a day but are allowed to eat whatever they please. The other group of males are not required to exercise, but they must adhere to a very strict diet. The males on the strict diet lose an average of four pounds more during the three month period versus the individuals that are required to exercise two hours per day. Is diet a better mechanism for influencing weight loss among young obese males versus daily exercise? \n",
    "\n",
    "<img src=\"images/two_populations_mean_02.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "- A group of 1,000 high school students are randomly assigned to two groups. One group is required to take a daily multivitamin pill. The other group is given a placebo, a pill with no medicinal value. The group that took the daily multivitamin scored an average of 3% higher on their math exams during their academic year. Does taking this daily vitamin improve a student's ability to perform well on math exams? \n",
    "\n",
    "<img src=\"images/two_populations_mean_03.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "In each scenario discussed, we had a group of people. They were assigned to two groups, either by their sex or by the stimuli to which they were exposed. In each case, the results of one group differed from the other group. One group had a higher mean salary. One group experienced a higher mean weight loss. And another group had higher mean exam scores on their math tests. \n",
    "\n",
    "The question is, did these measurable changes occur because of the stated differences in their groups or by chance? In other words, maybe the gender of the senior executives did not play a role in the salaries. Perhaps this group of female executives didn't hit their sales numbers that would warrant higher salaries. Maybe the weight loss program was not the differentiator in weight loss. It's possible that one group of obese men just happened to be assigned more of the men that were prone to lose weight under any program. And how would a vitamin help someone do better on a math exam? Perhaps the students that were better at math were randomly selected to be in the group that got the daily multivitamin. \n",
    "\n",
    "In this section, we'll look at different ways `to figure out whether population means for two populations can be attributed to actual differences or to chance`. Using data, charts, and randomizations, we'll look at different ways to figure out whether stimuli or chance influenced our statistical outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization (re-randomizing)\n",
    "A certain school has 200 students. The students are randomly assigned to two different groups of 100 students. Each of the 100 students in the first group is given a math textbook to learn a certain math concept. We'll call this Group A. The second group of 100 students is asked to watch an online video to learn the same math concept. We'll call this Group B. Each group is given 30 minutes to learn the math concept. After the 30 minutes are up each of the 200 students takes an exam with 20 questions. \n",
    "\n",
    "<img src=\"images/two_populations_mean_04.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "The students that learn from the online video, Group B, they had a median test score that was five questions higher than the students that learned from the textbook, Group A. Students that watched the video had a median score of 17 out of 20. Students that learned the concept from the textbook had a median score of 12 out of 20. \n",
    "\n",
    "Is the video a more effective teaching tool or did this outcome just happen by chance? In other words, were most of the students that were better at math just happened to be assigned to the group that had access to the online video? \n",
    "\n",
    "One way to visualize the likelihood that this might've happened by chance is by taking all 200 test scores and then randomly assigning them to two different groups of 100. But if we randomize the 200 test scores into two groups we might find that one group of test scores has a median of 15 and the other a median of 14. What happens if we randomize the 200 test scores again? Maybe we'll get one group with a median score of 13 and the other group with a median score of 15. And what happens if we randomize these math test scores a total of a hundred times? \n",
    "\n",
    "<img src=\"images/two_populations_mean_05.png\" alt=\"\" style=\"width: 400px;\"/>\n",
    "\n",
    "For each randomization we record the difference between Group A, of 100 test scores, and Group B, and their hundred test scores. This allows us to visualize the difference between the averages of two randomized groups of these test scores. Suppose that this was the resulting distribution, where the X axis measures the difference between the medians of the two random groups. There are 100 dots on our distribution. Each dot represents the difference between the two group medians for a different randomization of the scores. \n",
    "\n",
    "<img src=\"images/two_populations_mean_06.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "The result of the experiment found that the group that used the online video had a median score five questions higher than the group that used the textbook. From the distribution chart for our 100 test score randomizations we can see that only on three occasions did the median test score for the online video group, Group B, exceed the median test score for Group A by five questions or more. If our significance level were 5% we can see that our experiment results were significant. According to this distribution chart this outcome is less than 5% likely to have occurred by chance. In fact, it's 3% likely to have occurred by chance. Granted, this was only 100 rerandomizations of the data. And this was a rather limited statistical exercise. But this simple example allowed us to, both, understand and visualize how statisticians test whether an outcome was potentially meaningful or if it may have occurred by chance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up a confidence interval\n",
    "In the previous section, we discussed a situation where 200 students were randomly placed into two groups. These students were going to be given a test on a math concept. One group of students was allowed to use an online video to prepare for the exam. The second group used a traditional textbook to prepare for the same exam. The exam had 20 questions. \n",
    "\n",
    "For this section, let's provide some updated data. The group that used the online video to prepare ended up with 120 students. This group averaged 16.2 correct questions on the exam. The standard deviation of these test scores was 2.5. The group that used the traditional textbook to prepare for the exam, they had 80 students. The textbook group averaged 14.1 correct questions on the exam. The standard deviation of their scores was 3.6. As we can see, the average score for the online video group was 2.1 correct questions higher than that of the textbook group. \n",
    "\n",
    "<img src=\"images/two_populations_mean_07.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "This of course is only the difference for these two random groups of students from this one school. If the online video was available to the entire population of students that took this course across the country, what would be the difference in the average exam scores? Well, since we don't have data for every student, `we can use these two samples to create a confidence interval. A confidence interval that would contain the true difference between the population mean score` of students that prepared using the online video versus the population mean score of students that prepared using the textbook. Again, we go back to confidence intervals, and again, we see a familiar formula. Let's begin to fill in the numbers for our math test example. \n",
    "\n",
    "<img src=\"images/two_populations_mean_08.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "First, we know that the difference in mean scores for these two samples is 2.1. Next, our critical value. Let's say we want to build the typical 95% confidence interval, an interval that excludes 2.5% of the outcomes on either end of the distribution. We go to the Z distribution chart and `we find the appropriate critical score, the one that coincides with 0.9750`. \n",
    "\n",
    "<img src=\"images/two_populations_mean_09.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "Thus, our critical Z score is the very familiar 1.96. So we have 2.1, we have 1.96, we're just missing our standard error. Since our sample sizes are 80 and 120, `we can utilize the standard deviations of our samples as reasonable estimates for the population standard deviations`. So as you can see here, the standard error for this situation is the standard deviation of the video sample squared, divided by the sample size of the video sample plus the standard deviation of the textbook sample squared, divided by the sample size of the textbook sample. And once we have the sum, we take the square root. \n",
    "\n",
    "<img src=\"images/two_populations_mean_10.png\" alt=\"\" style=\"width: 300px;\"/>\n",
    "\n",
    "In this case, the standard deviation of the video sample is 2.5, and this sample size is 120, and the standard deviation of the textbook sample is 3.6 and this sample size is 80. Once we plug in our numbers, we can find the standard error for this problem. We get a standard error of about 0.462. \n",
    "\n",
    "<img src=\"images/two_populations_mean_11.png\" alt=\"\" style=\"width: 300px;\"/>\n",
    "\n",
    "<img src=\"images/two_populations_mean_12.png\" alt=\"\" style=\"width: 200px;\"/>\n",
    "\n",
    "And now we can calculate the limits of our confidence interval. Our upper and lower limits are 2.1 plus our critical value 1.96, times our standard error, 0.462. So we get an upper limit of 3.01. We get a lower limit of 1.19. \n",
    "\n",
    "<img src=\"images/two_populations_mean_13.png\" alt=\"\" style=\"width: 400px;\"/>\n",
    "\n",
    "So what does this mean? Well it means that we are 95% confident that the online video helps improve the exam scores for this particular test by at least 1.19 questions, and perhaps as high as 3.01 questions. Remember, we're only 95% confident, but since our lower limit is 1.19, we're pretty confident that the online video will improve the average test score of the population by at least one question versus the average score of this population if the students instead use the textbook to study."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis testing\n",
    "So let's continue our same example. 200 students preparing for a math test. 120 had access to an online video. This group averaged 16.2 correct questions out of 20 on the exam. The Standard deviation for this group is 2.5. In another group, 80 students learn from a textbook. This group averaged 14.1 correct questions on the exam. The Standard deviation of their scores was 3.6. The average score for the online video group was 2.1 questions higher than that of the textbook group. \n",
    "\n",
    "<img src=\"images/two_populations_mean_07.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "So does this data make you feel like the online video is more effective than the textbook? To find out, let's perform a Hypothesis Test. Let's begin by setting up our two hypotheses. \n",
    "\n",
    "<img src=\"images/two_populations_mean_14.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "- Our Null hypothesis will be that the online video and the textbook are equally effective. There is no difference in their ability to prepare the students for this exam. In other words, the difference between the population means of these two groups would be 0. We would state this Null hypothesis as the Population mean exam score for the online video group minus the Population mean exam score for the textbook group is equal to 0. \n",
    "- Our alternative hypothesis would be that the online video is more effective in preparing students for this math exam. We would state this alternative hypothesis as the Population mean exam score for the online video group minus the Population mean exam score for the textbook group is greater than 0. \n",
    "- For this Hypothesis Test let's use a 1% significance level, or, an Alpha of value 0.01. \n",
    "\n",
    "Let's take a look at what we're testing. Here we have a normal curve. \n",
    "\n",
    "<img src=\"images/two_populations_mean_15.png\" alt=\"\" style=\"width: 300px;\"/>\n",
    "\n",
    "The curve is centered at 0. Remember, our Null hypothesis states that this would be the difference between these two populations. Our two samples, though, revealed that the video group scored 2.1 questions higher than the textbook group, so the difference between our two samples is out here to the right. \n",
    "\n",
    "<img src=\"images/two_populations_mean_16.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "Our Hypothesis Test has a 1% significance level. This means that if our outcome, 2.1, has less than a 1% chance of occurring we will reject the Null hypothesis. As you can see, they really want to make sure the evidence is strong before they declare the online video as being a more effective learning tool. To figure this out, we need a z score that is associated with 0.99. On a z distribution chart we find that that z score would be 2.33. So if our outcome of 2.1 is more than 2.33 Standard deviations from 0, then we will reject our Null hypothesis. \n",
    "\n",
    "<img src=\"images/two_populations_mean_17.png\" alt=\"\" style=\"width: 400px;\"/>\n",
    "\n",
    "Well, then to figure this out we'll need to know the size of 1 Standard deviation. Remember, we do not have our population data, we have sample data. But `since our sample sizes are rather large we can use the Standard error of the difference between the two Standard means as our Standard deviation`. We plug in our Standard deviations for the two different samples as our two sigmas in this formula. We then plug in the appropriate sample sizes into the denominators. This gives us a Standard error of 0.462.\n",
    "\n",
    "<img src=\"images/two_populations_mean_18.png\" alt=\"\" style=\"width: 200px;\"/>\n",
    "\n",
    "Remember, we're going to use 0.462 as our Standard deviation, so to find our z score we use this formula. \n",
    "\n",
    "<img src=\"images/two_populations_mean_19.png\" alt=\"\" style=\"width: 300px;\"/>\n",
    "\n",
    "As you can see, that big denominator is just the Standard deviation we've just calculated, 0.462. X1 minus X2 is the difference between our sample means. That was 2.10. And from our Null hypothesis we know that mu1 minus mu2 is equal to 0. So we find that our z score is 4.54. \n",
    "\n",
    "Remember, on our chart, the threshold to reject the Null hypothesis was that 2.10 would be more than 2.33 Standard deviations from the center of our distribution, 0. Well, as we just saw, we are far beyond 2.33 Standard deviations from the mean. We are 4.54 Standard deviations from the mean. This puts us in the zone where we can reject our Null hypothesis. This means that we reject that the online video and textbook were equally effective in preparing students, which means we feel there is strong support for the alternative hypothesis. It does seem that the online video is more effective than the textbook in preparing students for the exam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1\n",
    "What is a null hypothesis?\n",
    "- a proposal that cannot be disputed by statistics\n",
    "- a proposal that all phenomena are unrelated\n",
    "- (correct) a statement of that which is presumed true in the absence of evidence\n",
    "- a statement indicating that nothing is true\n",
    "\n",
    "We wish to find out if information or data can lead us to reject the null hypothesis.\n",
    "\n",
    "### Example 2\n",
    "```\n",
    "Given two samples (one with size 100 and a standard deviation of 4.2, and the other with size 200 and a standard deviation of 9.8), what is the standard error of the mean representing both samples?\n",
    "SE = 0.81\n",
    "Take the square root of the sum of the sample variances.\n",
    "```\n",
    "\n",
    "### Example 3\n",
    "Why is re-randomizing population samples useful?\n",
    "- It permits the use of data visualization techniques.\n",
    "- It reduces the data analysis problem to simple arithmetic.\n",
    "- (correct) It demonstrates the probability of obtaining the original result by chance.\n",
    "- It allows the presentation of graphical data.\n",
    "\n",
    "This is a way of deciding whether a result was a fluke."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chi-Square\n",
    "\n",
    "### Introduction to chi-square\n",
    "I know it may seem strange, but there are some parts of the year where the rate of births are a bit higher than others. Yeah, there are more babies born in the summer and fall months than in the winter and spring months. Let's say that at a recent conference, a certain hospital administrator reported that at her hospital, historically, the birth rates by season of the year were this. \n",
    "\n",
    "<img src=\"images/chi_square_01.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "So, 15% of their annual births were in winter. 25% in spring, and 30% for both summer and fall. In an effort to see if the hospital's birth rates followed the stated seasonal distribution, the birth totals by season for last year were collected. And here they are. \n",
    "\n",
    "<img src=\"images/chi_square_02.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "So in winter, they had 45 births. In spring 48, summer 55, and 52 births in fall. As you can see, the numbers for last year do not seem to match up neatly with the historical rates that were quoted by the hospital administrator. For the 200 total births for this one year, 22.5% of the births occurred in winter versus the expected percentage, 15%. We can also see the data for the other seasons. \n",
    "\n",
    "Does this look like the hospital administrator's report was inaccurate? Before you make a judgement, we need to remember this data is for only a single year. But, suppose we wanted to know if the observed frequencies for this one year provided sufficient evidence to support the seasonal birthrates quoted by the hospital administrator? What can we do? Well, we could utilize something called a `Goodness of Fit Test`. This is one type of a `chi-square technique that we can use to perform hypothesis tests that compare two or more populations`. You may be wondering, why can't we just use a t-test here? Well, notice that **our data is divided into categories**. The categories are the seasons. `Chi-squared, or the Goodness of Fit Test, is more appropriate for evaluating data sets in which data is categorized`. With a t-test you're evaluating the null hypothesis when two sets of data are collected but not categorized. This Goodness of Fit Test will help us decide if our observed data for this single year follows the probability distribution that was provided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Curves and distribution\n",
    "Chi-square is an interesting distribution in that, as you might guess, we square something but what is it that we square? Let's say we have at normal distribution. The distribution is centered at zero and the x-axis represents the different numbers in this distribution. \n",
    "\n",
    "<img src=\"images/chi_square_03.png\" alt=\"\" style=\"width: 300px;\"/>\n",
    "\n",
    "These values on the x-axis are the ones that we're going to square. Since we're squaring these values, our chi-square distribution will only have positive numbers. The height of the curve represents the likelihood of that particular outcome. In other words, the y-axis represents probability. \n",
    "\n",
    "<img src=\"images/chi_square_04.png\" alt=\"\" style=\"width: 300px;\"/>\n",
    "\n",
    "According to the distribution, most of the numbers in the distribution are near zero. So, our chi-square distributions will often look like this. \n",
    "\n",
    "Now, chi-square distributions allow us to see how multiple independent variables interacts. In other words, instead of just one normally distributed variable, we can have two or more. Using our seasonal birthrate example, imagine that winter is normally distributed, spring is normally distributed, and so our summer and fall. Each is an individual and independent normal variable. Each has its own normal distribution curve. For each season, we take a value from its distribution curve. Those values would be squared and summed. \n",
    "\n",
    "One other important factor we'll need to consider is the number of degrees of freedom. You might remember that the number of degrees of freedom is often just our sample size minus one. Why do we need the number of degrees of freedom? \n",
    "\n",
    "<img src=\"images/chi_square_05.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "Well, `for each degree of freedom, we have a different chi-square distribution curve, and the degrees of freedom tell us the mean of the associated curve`. So, where the mean of a curve with one degree of freedom would be 1.0, the mean of a curve with three degrees of freedom would be 3.0. I know it doesn't look like it but remember, this tail goes off right toward infinity. So, the mean of the curve with five degrees of freedom would be five and, of course, the mean of the curve with 10 degrees of freedom would be 10. \n",
    "\n",
    "<img src=\"images/chi_square_06.png\" alt=\"\" style=\"width: 400px;\"/>\n",
    "\n",
    "As you can see, the greater our degrees of freedom, the closer our chi-square distribution gets to a normal distribution. And just like with Z distributions and T distributions, we have a `table with our chi-square critical values`. Here's a small excerpt of our chi-square distribution table. \n",
    "\n",
    "<img src=\"images/chi_square_07.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "How do you read the chart? We identify our degrees of freedom. \n",
    "\n",
    "<img src=\"images/chi_square_08.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "This would be on the left hand side of the chart, as with all of our distribution charts, we then identify a probability threshold. This is along the top of our chart. \n",
    "\n",
    "<img src=\"images/chi_square_09.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "So, for five degrees of freedom, if we wanted to know the critical chi-square value for a 10% significance level, our chi-square value would be 9.236. \n",
    "\n",
    "<img src=\"images/chi_square_10.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "This is the value which we would compare to our calculated chi-square. So, with this bit of knowledge, let's go ahead and complete our test of the seasonal birth distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goodness-of-fit test\n",
    "So before we perform our chi-square goodness of fit test, let's recap our situation. Let's take a look at our stated distribution one more time. \n",
    "\n",
    "<img src=\"images/chi_square_11.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "In an effort to see if the hospital's birth rates follow the stated seasonal distribution, the birth totals by season for last year were collected. There were a total of 200 births. As you can see, the numbers for last year do not seem to match up neatly with the historical rates that were quoted by the hospital administrator. For the 200 total births for this one year, 45 total babies were born in winter. This means 22.5% of the births occurred in winter versus the expected percentage 15%. We can also see the data for the other seasons. \n",
    "\n",
    "Now, we want to know if the observed frequencies for this one year provides sufficient evidence to support the seasonal birth rates quoted by the hospital administrator. To do this, we perform a `goodness of fit test`. This is `a type of chi-square hypothesis test used to compare two or more populations`. \n",
    "\n",
    "<img src=\"images/chi_square_12.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "So, let's begin our goodness of fit hypothesis test. Our null hypothesis, h naught, is as follows. In this case, we will say that the hospital administrator's distribution was accurate, and of course, our alternative hypothesis would then be that the stated null hypothesis was not accurate. Let's set our significance level at 5%. Using the birth data from our table, we can now calculate a chi-square test statistic using this formula. \n",
    "\n",
    "<img src=\"images/chi_square_13.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "Notice this is not an X. It's the Greek letter chi, so our chi-square critical value is the sum of our observed value minus our expected value squared divided by our expected value. This means for each season, we calculate a chi-square value, and then we add up those individual chi-square values, so let's go ahead and do this. \n",
    "\n",
    "<img src=\"images/chi_square_14.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "Let's start with winter. Our chi-square for winter is our observed value, 45, minus our expected value, 30, squared, divided by our expected value, 30. This gives us a chi-square for winter for 7.50. \n",
    "\n",
    "<img src=\"images/chi_square_15.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "We then do the same calculation for spring where we had 48 observed births, and we expected 50 births. As you can see, our chi-square for spring is our 48 observed births minus our 50 expected births, squared, divided by our expected births, 50. This gives us a chi-square for spring of 0.08. Notice, since we always square the numerator, our chi-square values will always be positive. So, here are the chi-square values for all four seasons.\n",
    "\n",
    "<img src=\"images/chi_square_16.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "Notice, our chi-square values are very small when the observed value is very close to our expected value, so `the smaller our chi-square value, the better the goodness of fit`. Now that we have all four seasonal chi-square values, we can add them and get our chi-square test statistic. We add up 7.50, 0.08, 0.42, and 1.07, and we get our chi-square test statistic of 9.07. \n",
    "\n",
    "<img src=\"images/chi_square_17.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "Now that we have our chi-square test statistic, we need to compare that to our chi-square critical value, so let's find our degrees of freedom first. For chi-square, our degrees of freedom are expressed as k minus one, where k is the number of categories. We had four seasons, so k is equal to four. This means we have three degrees of freedom. \n",
    "\n",
    "<img src=\"images/chi_square_07.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "We then go to our chi-square table. We find the row for three degrees of freedom. Our significance level is 5%, so we go to the column labeled 0.05, and we find that our chi-square critical value for three degrees of freedom and a 5% significance level is 7.815. \n",
    "\n",
    "<img src=\"images/chi_square_18.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "\n",
    "Let's look at this on our chi-square distribution. \n",
    "\n",
    "<img src=\"images/chi_square_19.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "7.815 is here. What does this mean? If we are to the left of 7.815, our one year of birth data would be a relatively likely outcome given the stated distribution. \n",
    "\n",
    "<img src=\"images/chi_square_20.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "On the other hand, if we are on the right side of 7.815, our one year of birth data would be a rather extreme outcome given the stated distribution.\n",
    "\n",
    "<img src=\"images/chi_square_21.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "Our calculated chi-square value was 9.06. This means we are to the right to 7.815, so we reject our null hypothesis. The goodness of fit test helped us see that based on our single year of birth data, we can say with 95% confidence that the hospital administrator's stated seasonal distribution was extremely unlikely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1\n",
    "```\n",
    "For a chi-square calculation with five categories, how many degrees of freedom are there?\n",
    "4\n",
    "DOF= k-1\n",
    "```\n",
    "\n",
    "### Example 2\n",
    "```\n",
    "What is chi-square for one observation of 10.2 with an expected value of 13.4?\n",
    "X^2 = (10.2 - 13.4)^2 / 13.4 = 0.76\n",
    "It is the square of the deviation divided by the expected value.\n",
    "```\n",
    "\n",
    "### Example 3\n",
    "When is chi-square useful?\n",
    "- when there are more than five degrees of freedom\n",
    "- when the data is widely scattered\n",
    "- (correct) when there is more than one independent variable\n",
    "- when the data is collected at different times\n",
    "\n",
    "One often uses chi-square when data is divided into categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANOVA - Analysis of Variance\n",
    "\n",
    "### What is analysis of variance?\n",
    "A luxury resort recently gathered survey data from their guests. These results were all captured within a one-week period in June. The hotel guests were asked to rate their resort experience from a score of zero to ten. Here are the survey results reported by these hotel guests, broken down by the age of the guest. \n",
    "\n",
    "<img src=\"images/anova_01.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "The question is, are these scores different because of the difference in the age of the guest, or is it possible that the reported scores and their differences were the result of random chance? \n",
    "\n",
    "In this section, we're going to discuss the Analysis of Variance, often referred to as ANOVA. `ANOVA is a procedure used to determine if the variation between reported output is the result of some particular factor, or if the variation is simply the result of randomness`. \n",
    "\n",
    "In this case, we have `one factor`: age. We also consider the `number of levels`. In this case, we had three levels, since guests were divided into three age ranges. We're going to look at ANOVA procedures in action. But, before we begin, we need to understand that `ANOVA relies on some assumptions`, including each population in our comparison is normally distributed, the observations are independent of one another, so, the guests do not influence each other's opinion, each of the populations being compared has an equivalent variance. \n",
    "\n",
    "<img src=\"images/anova_02.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "We're going to look at `one-way ANOVA`, a procedure that allows us to compare the means of different levels of one factor. This is the most basic form of ANOVA, but it will help us understand the basic goals and capabilities of ANOVA. There are other types of ANOVA, though. For example, we have randomized block ANOVA. `Randomized block ANOVA` would allow us to see if other factors may be influencing the outcomes. For example, you may think the annual income of the hotel guests plays a role in the survey scores. There is also `two-way ANOVA`. Similar to one-way ANOVA, we are comparing means from different levels. But, as you might guess, here we are using two factors. So here, we might be able to look at survey scores based on the age group, our first factor, and also based on the type of room the guest stayed in during their visit, our second factor. Two-way ANOVA can allow us to look at the intersection between these two factors.\n",
    "\n",
    "### One-way ANOVA and the total sum of squares (SST)\n",
    "Suppose, there are four different mobile service companies, Air Mobile, Binge Tech, ComMobile, Data Roam. Customers are asked to rank their mobile service from a scale of 1 to 10. One being very poor, 10 being excellent. Here is the individual data for four different customers for each company. \n",
    "\n",
    "<img src=\"images/anova_03.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "Before we move on, I want to note that while our very simple example has the same number of responses for each of the four companies, `ANOVA actually allows us to have a different number of data points for each individual level`. In any case, if we calculate the mean score for each company, we find Air has a mean score of four, Binge and Data have means of five, and ComMobile has a mean of six. If we average all 16 individual scores, we can get the `grand mean` of all data values. Thus, our grand mean, the average of all 16 data values is 5.0. We are now in position to calculate the `total sum of squares`, this is easy but it requires a bit of work. Let's begin with our Air Mobile data. We will take our first data point, five, and subtract from it the grand mean, five. So, we have five minus five and we're going to square that. We will then do this for every data point under Air Mobile. So, we have three minus our grand mean of five and we'll square that. We have five minus our grand mean of five and we'll square that, and finally, our last data point, three, will subtract the grand mean again, five, and we'll square that. \n",
    "\n",
    "<img src=\"images/anova_04.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "Actually, we're going to do this for all 16 values. The values for Air, Binge, ComMobile, and Data Roam. If we add all these up, the sum of squares for Air is eight, the sum of squares for Binge is 14, the sum of squares for ComMobile is 10, and the sum of squares for Data is 20. If we add up all of those, we get a `total sum of squares` of 52. \n",
    "\n",
    "<img src=\"images/anova_05.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "Notice, the total sum of squares is often noted as `SST`. What does this mean? It means that the total amount of variation between each data value and the grand mean is 52. \n",
    "\n",
    "So far, ANOVA has allowed us to calculate the level of variance between all 16 points in our complete data set and the grand mean of the entire set but if we look at our table of data, we can see that while there is variance between all the data points, there's also variance between the data values for each company. Let's calculate that next.\n",
    "\n",
    "### Variance within and variance between (SSW and SSB)\n",
    "So far, for our mobile service data, ANOVA has allowed us to calculate the level of variance between all 16 data values in each complete data set and the grand mean of the entire set, but if we look at our table of data, we can see that while there is variance between all the data points and the grand mean, there is also variance between the data values for each company and the mean for each mean score for each company.\n",
    "\n",
    "<img src=\"images/anova_06.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "This type of variance is called the variance within. So, let's find the variance within the individual data values for Air Mobile and the mean score for Air Mobile which is 4.0. Just as before, we will add up our squares. The difference this time is that instead of taking our data value, five, and subtracting the grand mean, here, we will subtract the mean for Air Mobile which is four. We do the same for the other three data values under Air Mobile. If we add all of our squares, we get a sum of squares within for our Air Mobile data of four but I don't want to do this for only Air Mobile. I'm going to do this for all four companies. The important thing to remember is that instead of subtracting our grand mean, we will be subtracting the individual means for each company. \n",
    "\n",
    "<img src=\"images/anova_07.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "So, you can see that this is very similar to calculating our total sum of squares. The big difference that for Air, we subtract by four. For Binge and Data, we subtract by five. Since both of those companies had mean scores of five. And for ComMobile, we will subtract by their mean score, six. So, when we add up all of these squares, we get 44. That means that our `total sum of squares within` each company's data, often noted as `SSW` is 44. We'll come back to this number shortly. \n",
    "\n",
    "There's another type or variance. The variance between the mean score for each company and the grand mean. Let's find the squares between each company's data and the grand mean. Remember, the grand mean, the average of all 16 of our data values is 5.0. So, for Air Mobile, we take the Air Mobile mean, 4.0, and subtract the grand mean, five, and, of course, we square it. Here's what we get for each company. But since each company had four data values, we multiply each square by four, why? Remember, each company's mean was made up of four data values. So, we need to multiply it by four so each square is representative of each data value. We add all of these up and we find that our `sum of squares between` the companies, often noted as `SSB`, is eight. \n",
    "\n",
    "<img src=\"images/anova_08.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "So, let's recap. We know that the total sum of squares, SST, was 52. We found that our sum of squares within, SSW, was 44. And we also found that our sum of squares between, SSB, was eight. Look at that. The sum of squares within, 44, and the sum of squares between, eight. They add up to 52 which just happens to be our total sum of squares. This didn't just happen by chance. The sum of squares within plus the sum of squares between always gives us the total sum of squares.\n",
    "\n",
    "<img src=\"images/anova_09.png\" alt=\"\" style=\"width: 300px;\"/>\n",
    "\n",
    "### Hypothesis test and f-statistic\n",
    "So far, we've been able to calculate our sum of squares, our sum of squares within, and our sum of squares between. We also found the relationship between these three values. Our sum of squares is the sum of the squares within, and the squares between. The next, and perhaps obvious question would be so, what can we do with this? Well, we can start to test our data set. \n",
    "\n",
    "<img src=\"images/anova_10.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "Presently, it looks like ComMobile is providing better service than its competitors. And it looks like Air Mobile is providing the worst service. I'm simply basing this off of their mean values. The question is, is it possible that these data values happen by chance and that perhaps the services are equal? In other words, perhaps if the entire population of mobile service users got to try all four companies we would find that there really is not a difference between these companies. \n",
    "\n",
    "So, if we wanted to establish a hypothesis test, we would begin by stating our hypotheses. \n",
    "\n",
    "<img src=\"images/anova_11.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "Here's the nice thing about ANOVA: the null hypothesis is always the same. `The null hypothesis always states that all populations are equal`. So for our situation, H-naught is that the population mean for all mobile service providers is equal. Which means our alternative hypothesis states that not all of the population means are equal. So, if we reject the null hypothesis, this would help us see that there is a difference between the four companies, and that some companies are better than others. Let's establish a **5% significance level**. The question is, how will we test this? Well, for this we're going to introduce something new, called the F-statistic. \n",
    "\n",
    "<img src=\"images/anova_12.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "The `F-statistic` is our SSB divided by m minus one over SSW divided by n-sub t minus m. We can already sort of see what this is doing. `The formula is comparing the variants between the companies versus the variants within the individual company data`. So, if our F-statistic is big, that means there's probably a big difference between the companies. This pushes us to reject the null hypothesis. But of course, if the F-statistic turns out to be small, that means there probably is not a big difference between the companies. This means most of our variants is the result of chance. This would guide us to not reject our null hypothesis. \n",
    "\n",
    "<img src=\"images/anova_13.png\" alt=\"\" style=\"width: 300px;\"/>\n",
    "\n",
    "So, let's get to work. As we look at our F-statistic formula, we know what `SSB` and `SSW` stand for. But how about m and n-sub t? Well, `m` is the number of levels, or groups. In this case, we had four companies, so m is equal to four. That means m minus one is three. You may also recognize this as our `degrees of freedom between the companies`. Now we need n-sub t. `N-sub t` is the total number of observations in our data set. We had 16 different values in our data set. So this is our n-sub t. Therefore, n-sub t minus m is equal to 12. Notice n-sub t minus m is our `degrees of freedom within`. We have 12 degrees of freedom within the data set. Why? Because for each of the four companies, we had four values. That means for each individual company, we had three degrees of freedom. Four companies, each with three degrees of freedom, that gives us a total of twelve degrees of freedom. So, now we have everything we need to calculate our F-statistic. Our F-statistic is equal to 0.727. \n",
    "\n",
    "<img src=\"images/anova_14.png\" alt=\"\" style=\"width: 300px;\"/>\n",
    "\n",
    "Now it's time to go to our F distribution table. This could get ugly for you because `there is a different F distribution table for every level of significance`. So, there's a whole table for a significance level of 1%. Another for 10%, and still another for 5% significance level. Here's a small excerpt of this 5% table. \n",
    "\n",
    "<img src=\"images/anova_15.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "Notice along the top we have the degrees of freedom in our numerator. We had three degrees of freedom in our numerator. \n",
    "\n",
    "<img src=\"images/anova_16.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "And along the left, we have the degrees of freedom in our denominator. We had 12 degrees of freedom in our denominator. \n",
    "\n",
    "<img src=\"images/anova_17.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "So, if we go to the inner section of those two, we find that our critical F value is 3.49. \n",
    "\n",
    "<img src=\"images/anova_18.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "If our F-statistic is greater than 3.49, we reject our null hypothesis. If our F-statistic is less than 3.49, we do not reject our null hypothesis. Well, our F-statistic was 0.727, so we definitely do not reject our null hypothesis. This means that there is not enough evidence to support that there is a significant level of difference between these four mobile service providers. It's likely that the reported difference in their mean scores occurred by chance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1\n",
    "```\n",
    "For a dataset containing five groups and each containing six samples, how many degrees of freedom are there within the total data set?\n",
    "25 = 30 samples - 5 groups\n",
    "```\n",
    "\n",
    "### Example 2\n",
    "Which statement is true for the total sum of squares?\n",
    "- It is independent of the total number of observations.\n",
    "- (correct) It depends upon the total number of observations.\n",
    "- It is proportional to the total number of observations.\n",
    "- It can be negative.\n",
    "\n",
    "It is the sum of the square of each deviation.\n",
    "\n",
    "### Example 3\n",
    "In ANOVA, each population variance _____.\n",
    "- has zero value\n",
    "- is unique\n",
    "- (correct) is equal to the others\n",
    "- takes on one of two values\n",
    "\n",
    "All the variances are the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Regression\n",
    "\n",
    "### What is regression?\n",
    "Do you ever wonder about the correlation between variables? Whether they're somehow related? Well, `a regression helps us investigate the relationship between two variables`. \n",
    "\n",
    "For example, let's take a look at a couple of questions that are asked pretty frequently about education. Does education level impact lifetime earnings? Or, does more studying improve test scores? We can infer some things about these questions through the use of regression. The basics are pretty simple. Usually, we express this graphically with a simple scatter plot. \n",
    "\n",
    "<img src=\"images/regression_01.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "So to begin, on the X axis we have one variable, study time. On our Y axis we have a second variable, exam score. Let's say these are our data points. \n",
    "\n",
    "<img src=\"images/regression_02.png\" alt=\"\" style=\"width: 400px;\"/>\n",
    "\n",
    "So now, we sprinkle our data points onto our graph. Once we have our data points, the goal of regression is to help us find a line that will fit our data points. Now, obviously, we aren't going to have a single line that can hit every one of these data points, so `regression analysis tries to find the formula for the line that will best fit this distribution`. \n",
    "\n",
    "<img src=\"images/regression_03.png\" alt=\"\" style=\"width: 400px;\"/>\n",
    "\n",
    "Now before we move on, let's refresh our knowledge about lines and the formulas that describe them. The formula for a line is often expressed in slope intercept form. Y equals MX plus B. What does that mean? Let's take this line. \n",
    "\n",
    "<img src=\"images/regression_04.png\" alt=\"\" style=\"width: 400px;\"/>\n",
    "\n",
    "The variable B represents the Y intercept, the point where the line crosses the origin on the Y intercept. Then we have M. M represents the slope of our line. It tells us how steep our line is. A positive M indicates a positive slope, meaning that the line is rising from left to right. A negative M tells us our line is falling, from left to right. An M of zero means our line is flat horizontally. \n",
    "\n",
    "So, let's say that M is equal to two and B is equal to three. That means our line looks like this. \n",
    "\n",
    "<img src=\"images/regression_05.png\" alt=\"\" style=\"width: 400px;\"/>\n",
    "\n",
    "If we plug in one for X, into the formula, we find that Y would be five. This line tells us that for X equals one, we would expect Y to be five. But it's rarely quite that easy in regression. Let's go back to our example. \n",
    "\n",
    "<img src=\"images/regression_06.png\" alt=\"\" style=\"width: 400px;\"/> \n",
    "\n",
    "Let's say that this was found to be our regression line. As you can see the regression line doesn't hit any of our points directly. The line is simply the one that bests fits this data. And I think this is a good setup for our next few sections. We're going to learn how to find the formula for our regression line. We're going to use something called `R squared` to understand the relationship in variation between our X and Y variables. And we'll also look at something called the `correlation coefficient`, which will help us understand our regression line and how it fits our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The best-fitting line\n",
    "The starters for a certain men's college basketball team have the following heights and weights. \n",
    "\n",
    "<img src=\"images/regression_07.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "Here's what this would look like on our scatter plot. \n",
    "\n",
    "<img src=\"images/regression_08.png\" alt=\"\" style=\"width: 400px;\"/>\n",
    "\n",
    "Now, let's find the line that best fits this data. Remember, we're going to express the line using the slope intercept form, y = ax + b. \n",
    "\n",
    "<img src=\"images/regression_09.png\" alt=\"\" style=\"width: 400px;\"/>\n",
    "\n",
    "In algebra, we often see slope expressed as m. But here, we'll be using a as our slope. B will be the y-intercept. You're going to want to set up six columns. You'll then want to start to fill in your data. \n",
    "\n",
    "<img src=\"images/regression_10.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "Here are our given data points. For our fourth column, (xy), we multiply height and weight. Our fifth column, x squared, we square our heights. And, for our sixth column, we square our weights. We then add up the sum of columns two through six. We now have everything we need. The sums at the bottom of each column will be what we end up using to find our slope and intercept. So, let's plug in our sums into the formula for our slope, a. \n",
    "\n",
    "<img src=\"images/regression_11.png\" alt=\"\" style=\"width: 400px;\"/>\n",
    "\n",
    "Once we calculate this, we find our slope is 8.832, which means that, for every inch in height, the weight of the player is expected to increase by 8.832 pounds. Let's now move on to our y-intercept, b. \n",
    "\n",
    "<img src=\"images/regression_12.png\" alt=\"\" style=\"width: 400px;\"/>\n",
    "\n",
    "Again, we plug in our sums from our chart into this formula. Notice, we'll be using a, our calculated slope, in this formula. Once we plug everything and calculate, we find our y-intercept, b, is -479.3 pounds. \n",
    "\n",
    "So, based on our limited data set, if we had a player that was zero inches tall, we would expect him to weigh -479.3 pounds. Hey, we never said the line was perfect. In any case, we can now use the line to make some educated guesses about player weights and heights. Imagine that we have a player that's five feet, 10 inches tall. That's 70 inches. I can plug in 70 inches, for the value of x in our formula. According to the regression line, we might expect him to weigh 138.9 pounds. \n",
    "\n",
    "So, we have our line, and while it may not work for everyone, perhaps it works well for male college basketball players. The question is, was this formula a good fit? Or, just the best we could do, given the data? To answer that question, we'll be introducing a new concept, the `coefficient of determination`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The coefficient of determination - R2\n",
    "For our sample of basketball player weights and heights, we have only five values in each category. Graphing these on our scatter plot is simple. Evaluating them by sight is also rather simple, and the appropriate regression line was relatively easy to anticipate visually. For our data, we could see a pattern, and we could tell the formula for our regression line was a good fit, but what happens when you have a huge data set, when the scatter plot is messy, and the regression line is not necessarily logical? How can you tell if the regression line is a good fit for your data? Well, for this, we have something called `r-squared`. It's called `the coefficient of determination`. `R-squared is a number between zero and one. Zero indicates that our regression line is a very poor fit for our data points. An r-squared of 1.0 on the other hand indicates that the line is a perfect fit for our data`. So, how do we calculate r-squared? Well, the formula for r-squared looks pretty simple. \n",
    "\n",
    "<img src=\"images/regression_13.png\" alt=\"\" style=\"width: 400px;\"/>\n",
    "\n",
    "R-squared is equal to SSR divided by SST. `SSR` is equal to the sum of squares regression. `SST` is the total sum of squares. Calculating SSR and SST is not going to be fun though. Let's go to the table with our original data. \n",
    "\n",
    "<img src=\"images/regression_14.png\" alt=\"\" style=\"width: 400px;\"/>\n",
    "\n",
    "Notice, we also calculated the means for x and y values. Next, we'll add a column with our expected y's. In other words, using the formula for our regression line from our previous section, y hat is equal to 8.83 x minus 479.3. We'll plug in each x and calculate the associated y hat. \n",
    "\n",
    "Now, let's calculate SSR, the sum of squares regression. \n",
    "\n",
    "<img src=\"images/regression_15.png\" alt=\"\" style=\"width: 400px;\"/>\n",
    "\n",
    "We'll use this formula to find the individual square regressions. So here we have y hat minus our mean y, and we'll square that. Our first player was expected to be 156.5 pounds. This is our y hat. The mean y was 199. The square regression for this player is 1809.7. We do the same calculation for our four other players. So, our sum of squares regression is the sum of 1809.7, 257.6, 109, 2.6, and 2094. This gives us a SSR of 4272.8. \n",
    "\n",
    "<img src=\"images/regression_16.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "Now, for our total sum of squares, SST. To calculate our individual squares here, we use a very similar formula except instead of using the predicted weight for each player, we'll use the observed weight, so here our formula is our observed y minus our mean y, and again, we're going to take the difference and square it. \n",
    "\n",
    "<img src=\"images/regression_17.png\" alt=\"\" style=\"width: 400px;\"/>\n",
    "\n",
    "Our first player actually weighed 160 pounds. This is our observed y. The mean y was 199. The square for this player is 1521. We do the same calculation for our four other players. So, our total sum of squares is the sum of 1521, 361, 441, 81, and 2116. This gives us a SST of 4520. \n",
    "\n",
    "<img src=\"images/regression_18.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "I want to point out that throughout all the calculations, we have rounded differently, so your answers maybe slightly different than mine, and that's perfectly okay. Going through the process is what matters most. \n",
    "\n",
    "So now, we're ready to calculate r-squared. \n",
    "\n",
    "<img src=\"images/regression_19.png\" alt=\"\" style=\"width: 200px;\"/>\n",
    "\n",
    "Remember, r-squared is equal to SSR divided by SST. SSR was the sum of squares regression. That was 4272.8. SST is our total sum of squares. That's 4520. Therefore, r-squared is equal to 0.945. That's very close to 1.0 which means our regression line is an excellent fit for our data points just as we expected. \n",
    "\n",
    "`R-squared is 0.945. What exactly does that mean? It means that 94.5% of the variation is explained by height`. The other 5.5% of the variation can be attributed to error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The correlation coefficient\n",
    "In our previous section, we calculated `R squared`. For our basketball player data set, R squared was zero point nine four five and note, the r in R squared is a capital r. In our world of regression though, we also have a lowercase r. This `lowercase r is our correlation coefficient and there's a relationship between R squared and r`. \n",
    "\n",
    "<img src=\"images/regression_20.png\" alt=\"\" style=\"width: 300px;\"/>\n",
    "\n",
    "R, the correlation coefficient, is equal to the square root of R squared and the sign of our correlation coefficient is the same as the slope of our regression line. In the case of our basketball player data set, the regression line had a slope of positive eight point eight three. So if we want to calculate r, the correlation coefficient, first, we know the sign would be positive. Second, we take the square root of R squared, zero point nine four five. That means that r, the correlation coefficient, is positive zero point nine seven two and what does that mean? Well, let's look at this graphically. \n",
    "\n",
    "<img src=\"images/regression_21.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "This is what a r of positive one would look like (on the right). And here is the data for a r of negative one (on the left). In both cases, the data is organized in a way where if we connected the dots, we'd get a perfect straight line. The positive and negative signs simply tell us if the line is climbing from left to right or if it's declining. \n",
    "\n",
    "How about a r of negative zero point eight five? \n",
    "\n",
    "<img src=\"images/regression_22.png\" alt=\"\" style=\"width: 300px;\"/>\n",
    "\n",
    "Here we can see that the dots are heading in a downward direction as we move to the right. We can also see that, while we can imagine a regression line, the fit of these dots on the line would not be perfect. \n",
    "\n",
    "Here's a r of positive zero point four two. \n",
    "\n",
    "<img src=\"images/regression_23.png\" alt=\"\" style=\"width: 300px;\"/>\n",
    "\n",
    "The dots are generally following an upward trend from left to right, but any regression line we would draw would miss a majority of the dots by a rather big margin. \n",
    "\n",
    "How about a r of zero? \n",
    "\n",
    "<img src=\"images/regression_24.png\" alt=\"\" style=\"width: 300px;\"/>\n",
    "\n",
    "As you can see, no real trend is visible and the dots are sort of, just a disorganized mess. \n",
    "\n",
    "So while `r`, `the correlation coefficient`, does not provide specific information about the regression line, it does `tell us the tightness of fit of our data points and also the upward or downward trend of the data from left to right on our axis and now when you see someone report a r, use that as a guide to tell you about the trend of the data and whether there seems to be a strong linear relationship between our two variables`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1\n",
    "```\n",
    "For a coefficient determination of 0.917 and slope estimate of -0.342, \n",
    "what is the value of the correlation coefficient r?\n",
    "\n",
    "r = -0.957\n",
    "\n",
    "The result is the negative of the square root of 0.917.\n",
    "```\n",
    "\n",
    "### Example 2\n",
    "```\n",
    "In the equation y = -(1/12) x + 0.05, what is the slope when plotted as y versus x?\n",
    "\n",
    "slope = -(1/12)\n",
    "\n",
    "The slope is the multiplier of x.\n",
    "```\n",
    "\n",
    "### Example 3\n",
    "```\n",
    "What is the value of the coefficient of determination for a perfect linear fit?\n",
    "\n",
    "R squared = 1.0\n",
    "```\n",
    "\n",
    "### Example 4\n",
    "```\n",
    "For a mean of 7 and observations of 5, 6, and 9, what is the total sum of squares?\n",
    "\n",
    "SST = (5-7)^2 + (6-7)^2 + (9-7)^2 = 4 + 1 + 4 = 9\n",
    "```\n",
    "\n",
    "### Example 5\n",
    "```\n",
    "Which type of hypothesis test do we use to evaluate proportions from categorized data?\n",
    "\n",
    "goodness of fit\n",
    "```\n",
    "\n",
    "### Example 6\n",
    "```\n",
    "Which statistic is used in hypothesis testing for population proportions?\n",
    "z\n",
    "```\n",
    "\n",
    "### Example 7\n",
    "```\n",
    "Which formula becomes slightly messy when calculating the confidence intervals when using two population samples?\n",
    "\n",
    "standard error\n",
    "```\n",
    "\n",
    "### Example 8\n",
    "```\n",
    "For a sample size of 10, what is the number of degrees of freedom?\n",
    "9\n",
    "```\n",
    "\n",
    "### Example 9\n",
    "```\n",
    "For the same confidence interval, the t-score is always _____ the z-score?\n",
    "- equal to\n",
    "- less than\n",
    "- larger than (correct - t-distribution is lower than normal distribution and has more data in the tails)\n",
    "```\n",
    "https://www.scribbr.com/statistics/t-distribution/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
