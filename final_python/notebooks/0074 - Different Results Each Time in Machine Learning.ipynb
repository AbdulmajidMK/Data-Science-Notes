{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "From: https://github.com/ksatola\n",
    "Version: 0.0.1\n",
    "\n",
    "TODOs\n",
    "1. \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why Do I Get Different Results Each Time in Machine Learning?\n",
    "In applied machine learning, we run a machine learning `algorithm` on a dataset to get a machine learning `model`. The model can then be evaluated on data not used during training or used to make predictions on new data, also not seen during training.\n",
    "- **Algorithm:** Procedure run on data that results in a model (e.g. training or learning).\n",
    "- **Model:** Data structure and coefficients used to make predictions on data.\n",
    "\n",
    "`Supervised machine learning` means we have examples (rows) with input and output variables (columns). We cannot write code to predict outputs given inputs because it is too hard, so we use machine learning algorithms to learn how to predict outputs from inputs given historical examples. This is called `function approximation`, and we are learning or searching for a function that maps inputs to outputs on our specific prediction task in such a way that it has skill, meaning the performance of the mapping is better than random and ideally better than all other algorithms and algorithm configurations we have tried.\n",
    "\n",
    "In this sense, a `machine learning model` is a program we intend to use for some project or application; it just so happens that the program was learned from examples (using an algorithm) rather than written explicitly with if-statements and such. It’s a type of `automatic programming`.\n",
    "\n",
    "Unlike the programming that we may be used to, **the automatic programs may not be entirely deterministic**. The **machine learning models may be different each time they are trained**. In turn, the models may make different predictions, and when evaluated, may have a different level of error or accuracy.\n",
    "\n",
    "There are at least four cases where you will get different results; they are:\n",
    "- Different results because of differences in training data.\n",
    "- Different results because of stochastic learning algorithms.\n",
    "- Different results because of stochastic evaluation procedures.\n",
    "- Different results because of differences in platform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differences Caused by Training Data\n",
    "You will get different results when you run the same algorithm on different data.\n",
    "\n",
    "This is referred to as **the variance of the machine learning algorithm**. You may have heard of it in the context of the `bias-variance trade-off`. The `variance` is a measure of how sensitive the algorithm is to the specific data used during training.\n",
    "\n",
    "A more sensitive algorithm has a larger/higher variance, which will result in more difference in the model, and in turn, the predictions made and evaluation of the model. Conversely, a less sensitive algorithm has a smaller variance and will result in less difference in the resulting model with different training data, and in turn, less difference in the resulting predictions and model evaluation.\n",
    "\n",
    "All useful machine learning algorithms will have some variance, and some of the most effective algorithms will have a high variance. **Algorithms with a high variance often require more training data than those algorithms with less variance**. This is intuitive if we consider the model approximating a mapping function from inputs and outputs and the law of large numbers.\n",
    "\n",
    "When you train a machine learning algorithm on different training data, you will get a different model that has different behavior. This means different training data will give models that make different predictions and have a different estimate of performance (e.g. error or accuracy).\n",
    "\n",
    "The amount of difference in the results will be related to:\n",
    "- how different the training data is for each model, and \n",
    "- the variance of the specific model and model configuration you have chosen.\n",
    "\n",
    "You can often **reduce the variance of the model** by:\n",
    "- changing a **hyper-parameter of the algorithm**. For example, the k in k-nearest neighbors controls the variance of the algorithm, where small values like k=1 result in high variance and large values like k=21 result in low variance.\n",
    "- changing **the algorithm**. For example, simpler algorithms like linear regression and logistic regression have a lower variance than other types of algorithms.\n",
    "- by increasing the size of the training dataset, meaning you may need to collect more data. This works best with a high variance algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differences Caused by Learning Algorithm\n",
    "You can get different results when you run the same algorithm on the same data due to the nature of the learning algorithm. It’s not a bug, it’s a feature.\n",
    "\n",
    "Some machine learning algorithms are `deterministic`. Just like the programming that you’re used to. That means, when the algorithm is given the same dataset, it learns the same model every time. An example is a linear regression or logistic regression algorithm.\n",
    "\n",
    "Some algorithms are not deterministic; instead, they are `stochastic`. This means that their behavior incorporates elements of randomness. **Stochastic does not mean random**. Stochastic machine learning algorithms are not learning a random model. They are learning a model conditional on the historical data you have provided. Instead, the specific small decisions made by the algorithm during the learning process can vary randomly.\n",
    "\n",
    "The impact is that each time the `stochastic machine learning algorithm` is run on the same data, it learns a slightly different model. In turn, the model may make slightly different predictions, and when evaluated using error or accuracy, may have a slightly different performance. Adding randomness to some of the decisions made by an algorithm can improve performance on hard problems. Learning a supervised learning mapping function with a limited sample of data from the domain is a very hard problem.\n",
    "\n",
    "Finding a good or best `mapping function` for a dataset is a type of `search problem`. We test different algorithms and test algorithm configurations that define the shape of the search space and give us a starting point in the search space. We then run the algorithms, which then navigate the search space to a single model.\n",
    "\n",
    "Adding randomness can help avoid the good solutions and help find the really good and great solutions in the search space. They allow the model to escape local optima or deceptive local optima where the learning algorithm might get such, and help find better solutions, even a global optima.\n",
    "\n",
    "An example of an algorithm that uses randomness during learning is a neural network. It uses randomness in two ways:\n",
    "- Random initial weights (model coefficients).\n",
    "- Random shuffle of samples each epoch.\n",
    "\n",
    "Neural networks (deep learning) are a stochastic machine learning algorithm. The random initial weights allow the model to try learning from a different starting point in the search space each algorithm run and allow the learning algorithm to break symmetry during learning. The random shuffle of examples during training ensures that each gradient estimate and weight update is slightly different.\n",
    "\n",
    "Another example is ensemble machine learning algorithms that are stochastic, such as bagging (random forest). Here the randomness is used in the sampling procedure of the training dataset that ensures a different decision tree is prepared for each contributing member in the ensemble. In ensemble learning, this is called ensemble diversity and is an approach to simulating independent predictions from a single training dataset.\n",
    "\n",
    "**The randomness used by learning algorithms can be controlled**. For example, you set the `seed` used by the pseudorandom number generator to ensure that each time the algorithm is run, it gets the same randomness. This can be a good approach for tutorials, but not a good approach in practice. It leads to questions like: What is the best seed for the pseudorandom number generator? **There is no best seed for a stochastic machine learning algorithm**. You are fighting the nature of the algorithm, forcing `stochastic` learning to be `deterministic`.\n",
    "\n",
    "You could make a case that the final model is fit using a fixed seed to ensure the same model is created from the same data before being used in production prior to any pre-deployment system testing. Nevertheless, as soon as the training dataset changes, the model will change.\n",
    "\n",
    "A better approach is to embrace the stochastic nature of machine learning algorithms. Consider that there is not a single model for your dataset. Instead, there is a `stochastic process` (the algorithm pipeline) that can generate models for your problem. You can use `k-fold cross validation` technique as well (see below). You can then summarize the performance of these models - of the algorithm pipeline - as a distribution with mean expected error or accuracy and a standard deviation. You can then ensure you achieve the average performance of the models by fitting multiple final models on your dataset and averaging their predictions when you need to make a prediction on new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differences Caused by Evaluation Procedure\n",
    "You can get different results when running the same algorithm with the same data due to the evaluation procedure. The two most common evaluation procedures are a `train-test split` and `k-fold cross-validation`. \n",
    "- A **train-test split** involves randomly assigning rows to either be used to train the model or evaluate the model to meet a predefined train or test set size. \n",
    "- The **k-fold cross-validation** procedure involves dividing a dataset into k non-overlapping partitions and using one fold as the test set and all other folds as the training set. A model is fit on the training set and evaluated on the holdout fold and this process is repeated k times, giving each fold an opportunity to be used as the holdout fold.\n",
    "\n",
    "Both of these model evaluation procedures are `stochastic`. This does not mean that they are random; it means that small decisions made in the process involve randomness. Specifically, the choice of which rows are assigned to a given subset of the data. This use of **randomness is a feature, not a bug**. The use of randomness, allows the resampling to approximate an estimate of model performance that is independent of the specific data sample drawn from the domain. This approximation is biased because we only have a small sample of data to work with rather than the complete set of possible observations.\n",
    "\n",
    "**Performance estimates provide an idea of the expected or average capability of the model when making predictions** in the domain on data not seen during training. Regardless of the specific rows of data used to train or test the model, at least ideally. As such, each evaluation of a deterministic machine learning algorithm, like a linear regression or a logistic regression, can give a different estimate of error or accuracy.\n",
    "\n",
    "The solution in this case is much like the case for stochastic learning algorithms:\n",
    "- The seed for the pseudorandom number generator can be fixed or \n",
    "- the randomness of the procedure can be embraced.\n",
    "\n",
    "Unlike stochastic learning algorithms, both solutions are quite reasonable.\n",
    "- If a large number of machine learning algorithms and algorithm configurations are being evaluated systematically on a predictive modeling task, **it can be a good idea to fix the random seed of the evaluation procedure**  . Any value will do. \n",
    "    - The idea is that each candidate solution (each algorithm or configuration) will be evaluated in an identical manner. This ensures an apples-to-apples comparison. \n",
    "    - It also allows for the use of `paired statistical hypothesis tests` later, if needed, to check if `differences between algorithms are statistically significant`.\n",
    "- **Embracing the randomness can also be appropriate**. This involves repeating the evaluation procedure many times and reporting a summary of the distribution of performance scores, such as the mean and standard deviation. Perhaps the least biased approach to repeated evaluation would be to use `repeated k-fold cross-validation`, such as three repeats with 10 folds (3×10), which is common, or five repeats with two folds (5×2), which is commonly used when comparing algorithms with statistical hypothesis tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differences Caused by Platform\n",
    "You can get different results when running the same algorithm on the same data on different computers. This can happen even if you fix the random number seed to address the stochastic nature of the learning algorithm and evaluation procedure. The cause in this case is the platform or development environment used to run the example, and the results are often different in minor ways, but not always. They can be different in the:\n",
    "- System architecture, e.g. CPU or GPU.\n",
    "- Operating system, e.g. MacOS or Linux.\n",
    "- Underlying math libraries, e.g. LAPACK or BLAS.\n",
    "- Python version, e.g. 3.6 or 3.7.\n",
    "- Library version, e.g. scikit-learn 0.22 or 0.23.\n",
    "\n",
    "**Machine learning algorithms are a type of numerical computation**. This means that they typically involve a lot of math with floating point values. Differences in aspects, such as the `architecture` and `operating system`, can result in differences in round errors, which can compound with the number of calculations performed to give very different results. Additionally, differences in the `version of libraries` can mean the fixing of bugs and the changing of functionality that too can result in different results. This also explains why you will get different results for the same algorithm on the same machine implemented by `different languages`, such as R and Python. Small differences in the implementation and/or differences in the underlying `math libraries` used will cause differences in the resulting model and predictions made by that model.\n",
    "\n",
    "This does not mean that the platform itself can be treated as a `hyper-parameter` and tuned for a predictive modeling problem. Instead, it means that **the platform is an important factor when evaluating machine learning algorithms and should be fixed or fully described to ensure full reproducibility when moving from development to production**, or in reporting performance in academic studies.\n",
    "\n",
    "One approach might be to use `virtualization`, such as `docker` or a virtual machine instance to ensure the environment is kept constant, if full reproducibility is critical to a project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
