{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "From: https://github.com/ksatola\n",
    "Version: 0.0.1\n",
    "\n",
    "TODOs\n",
    "1. see also CRISP-DM\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applied Machine Learning Process\n",
    "Each machine learning project is different because the specifc data at the core of the project is different. That does not mean that others have not worked on similar prediction tasks or perhaps even the same high-level task, but you may be the frst to use the specifc data that you have collected (unless you are using a standard dataset for practice). This makes each machine learning project unique. No one can tell you what the best results are or might be, or what algorithms to use to achieve them. You must establish a baseline in performance as a point of reference to compare all of your models and you must discover what algorithm works best for your specifc dataset.\n",
    "\n",
    "Even though your project is unique, the steps on the path to a good or even the best result are generally the same from project to project. This is sometimes referred to as the `applied machine learning process`, `data science process`, or the older name `knowledge discovery in databases (KDD)`. The process of applied machine learning consists of a sequence of steps. The steps are the same, but the names of the steps and tasks performed may differ from description to description:\n",
    "- **Step 1: Define Problem.** This step is concerned with learning enough about the project to select the framing or framings of the prediction task. For example, is it classification or regression, or some other higher-order problem type? It involves collecting the data that is believed to be useful in making a prediction and clearly defining the form that the prediction will take. It may also involve talking to project stakeholders and other people with deep expertise in the domain. This step also involves taking a close look at the data, as well as perhaps exploring the data using summary statistics and data visualization.\n",
    "\n",
    "    Defining the problem may involve the following sub-tasks:\n",
    "    - Gather data from the problem domain.\n",
    "    - Discuss the project with subject matter experts.\n",
    "    - Select those variables to be used as inputs and outputs for a predictive model.\n",
    "    - Review the data that has been collected.\n",
    "    - Summarize the collected data using statistical methods.\n",
    "    - Visualize the collected data using plots and charts.\n",
    "    \n",
    "    \n",
    "- **Step 2: Prepare Data.** This step is concerned with transforming the raw data that was collected into a form that can be used in modeling.\n",
    "\n",
    "    Preparing data may involve the following sub-tasks:\n",
    "    - **Data Cleaning:** Identifying and correcting mistakes or errors in the data.\n",
    "    - **Feature Selection:** Identifying those input variables that are most relevant to the task.\n",
    "    - **Data Transforms:** Changing the scale or distribution of variables.\n",
    "    - **Feature Engineering:** Deriving new variables from available data.\n",
    "    - **Dimensionality Reduction:** Creating compact projections of the data.\n",
    "\n",
    "\n",
    "- **Step 3: Evaluate Models.** This step is concerned with evaluating machine learning models on your dataset. It requires that you design a robust test harness used to evaluate your models so that the results you get can be trusted and used to select among the models that you have evaluated. This involves tasks such as selecting a `performance metric` for evaluating the skill of a model, `establishing a baseline` or floor in performance to which all model evaluations can be compared, and a `resampling technique` for splitting the data into training and test sets to simulate how the final model will be used. \n",
    "    \n",
    "    Model evaluation may involve the following sub-tasks:\n",
    "    - Select a performance metric for evaluating model predictive skill.\n",
    "    - Select a model evaluation procedure.\n",
    "    - Select algorithms to evaluate.\n",
    "    - Tune algorithm hyperparameters.\n",
    "    - Combine predictive models into ensembles.\n",
    "    \n",
    "    \n",
    "    For quick and dirty estimates of model performance, or for a very large dataset, a single `train-test split` of the data may be performed. It is more common to use `k-fold cross-validation` as the data resampling technique, often with repeats of the process to improve the robustness of the result. This step also involves tasks for getting the most out of well-performing models such as hyperparameter tuning and ensembles of models.\n",
    "\n",
    "\n",
    "- **Step 4: Finalize Model.** This step is concerned with selecting and using a final model. Once a suite of models has been evaluated, you must choose a model that represents the solution to the project. This is called `model selection` and may involve further evaluation of candidate models on a hold out validation dataset, or selection via other project-specific criteria such as model complexity. It may also involve summarizing the performance of the model in a standard way for project stakeholders, which is an important step. Finally, there will likely be tasks related to the `productization of the model`, such as integrating it into a software project or production system and designing a `monitoring and maintenance schedule for the model`.\n",
    "\n",
    "**There may be a lot of interplay between the definition of the problem and the preparation of the data. There may also be interplay between the data preparation step and the evaluation of models.** Information known about the choice of algorithms and the discovery of well-performing algorithms can also inform the selection and configuration of data preparation methods. For example, the choice of algorithms may impose requirements and expectations on the type and form of input variables in the data. This might require variables to have a specific probability distribution, the removal of correlated input variables, and/or the removal of variables that are not strongly related to the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
