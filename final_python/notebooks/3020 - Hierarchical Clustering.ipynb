{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "From: https://github.com/ksatola\n",
    "Version: 0.0.1\n",
    "\n",
    "TODOs\n",
    "1. \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Clustering\n",
    "Hierarchical clustering, as the name suggests is an algorithm that builds hierarchy of clusters. This algorithm starts with all the data points assigned to a cluster of their own. Then two nearest clusters are merged into the same cluster. In the end, this algorithm terminates when there is only a single cluster left. This technique operate on the simplest principle, which is data-point closer to base point will behave more similar compared to a data-point which is far from base point. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does it work?\n",
    "The results of hierarchical clustering can be shown using dendrogram. The dendrogram can be interpreted as:\n",
    "\n",
    "<img src=\"images/hierarchical_clustering_01.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "At the bottom, we start with 25 data points, each assigned to separate clusters. Two closest clusters are then merged till we have just one cluster at the top. The height in the dendrogram at which two clusters are merged represents the distance between two clusters in the data space.\n",
    "\n",
    "The decision of the no. of clusters that can best depict different groups can be chosen by observing the dendrogram. The best choice of the no. of clusters is the no. of vertical lines in the dendrogram cut by a horizontal line that can transverse the maximum distance vertically without intersecting a cluster.\n",
    "\n",
    "In the above example, the best choice of no. of clusters will be 4 as the red horizontal line in the dendrogram below covers maximum vertical distance AB.\n",
    "\n",
    "<img src=\"images/hierarchical_clustering_02.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "Two important things that you should know about hierarchical clustering are:\n",
    "\n",
    "- This algorithm has been implemented above using bottom up approach. It is also possible to follow top-down approach starting with all data points assigned in the same cluster and recursively performing splits till each data point is assigned a separate cluster.\n",
    "- The decision of merging two clusters is taken on the basis of closeness of these clusters. There are multiple metrics for deciding the closeness of two clusters :\n",
    "    - Euclidean distance: ||a-b||2 = √(Σ(ai-bi))\n",
    "    - Squared Euclidean distance: ||a-b||22 = Σ((ai-bi)2)\n",
    "    - Manhattan distance: ||a-b||1 = Σ|ai-bi|\n",
    "    - Maximum distance:||a-b||INFINITY = maxi|ai-bi|\n",
    "    - Mahalanobis distance: √((a-b)T S-1 (-b))   {where, s : covariance matrix}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "See 3000 Clustering algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
