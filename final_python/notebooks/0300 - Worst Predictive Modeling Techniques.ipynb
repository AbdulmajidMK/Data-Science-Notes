{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "From: https://github.com/ksatola\n",
    "Version: 0.1.0\n",
    "\n",
    "TODOs\n",
    "1. \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Worst Predictive Modeling Techniques\n",
    "\n",
    "Modern data sets are considerably more complex and different than the old data sets used when these techniques were initially developed. In short, these techniques have not been developed for modern data sets. There's no perfect statistical technique that would apply to all data sets, but there are many poor techniques. Typically, these bad techniques are still widely used.\n",
    "\n",
    "- **Linear regression**. Relies on the normal, heteroscedasticity and other assumptions, does not capture highly non-linear, chaotic patterns. Prone to over-fitting. Parameters difficult to interpret. Very unstable when independent variables are highly correlated. Sensitive to both ouliers and cross-correlations.\n",
    "    - **Fixes:** variable reduction, apply a transformation to your variables, use constrained regression (e.g. ridge or Lasso regression)\n",
    "\n",
    "- **Traditional decision trees**. Very large decision trees are very unstable and impossible to interpret, and prone to over-fitting. \n",
    "    - **Fix:** combine multiple small decision trees together instead of using a large decision tree.\n",
    "\n",
    "\n",
    "- **Linear discriminant analysis**. Used for supervised clustering. Bad technique because it assumes that clusters do not overlap, and are well separated by hyper-planes. In practice, they never do. \n",
    "    - **Fix:** Use density estimation techniques instead.\n",
    "\n",
    "- **K-means clustering**. Used for clustering, tends to produce circular clusters. Does not work well with data points that are not a mixture of Gaussian distributions. \n",
    "\n",
    "\n",
    "- **Neural networks**. Difficult to interpret, unstable, subject to over-fitting.\n",
    "\n",
    "\n",
    "- **Maximum Likelihood estimation**. Requires your data to fit with a prespecified probabilistic distribution. Not data-driven. In many cases the pre-specified Gaussian distribution is a terrible fit for your data.\n",
    "\n",
    "\n",
    "- **Density estimation in high dimensions**. Subject to what is referred to as the curse of dimensionality. \n",
    "    - **Fix:** use (non parametric) kernel density estimators with adaptive bandwidths.\n",
    "\n",
    "\n",
    "- **Naive Bayes**. Used e.g. in fraud and spam detection, and for scoring. Assumes that variables are independent, if not it will fail miserably. In the context of fraud or spam detection, variables (sometimes called rules) are highly correlated. \n",
    "    - **Fix:** group variables into independent clusters of variables (in each cluster, variables are highly correlated). Apply naive Bayes to the clusters. Or use data reduction techniques. Bad text mining techniques (e.g. basic \"word\" rules in spam detection) combined with naive Bayes produces absolutely terrible results with many false positives and false negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
