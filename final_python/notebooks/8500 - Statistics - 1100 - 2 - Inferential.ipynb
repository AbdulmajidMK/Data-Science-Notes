{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "From: https://github.com/ksatola\n",
    "Version: 0.0.1\n",
    "\n",
    "TODOs\n",
    "1. \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "## Statistics Basics 2 - Inferential\n",
    "\n",
    "- [Inferential Statistics](#toc01)\n",
    "- [Sampling](#toc02)\n",
    "    - Random sample\n",
    "    - Sample size\n",
    "    - The central limit theorem\n",
    "    - Standard error (for proportions)\n",
    "    - Sampling distrribution of the mean\n",
    "    - Standard error (for means)\n",
    "- [Confidence Intervals](#toc03)\n",
    "- [Hypothesis Testing](#toc04)\n",
    "    - 4 step procedure\n",
    "    - One-tailed vs. two-tailed tests\n",
    "    - Small Sample Sizes (T-statistic vs Z-statistic)\n",
    "    - Type I and type II errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='toc01'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferential Statistics\n",
    "Based on samples of data infer about whole population."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='toc02'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling\n",
    "The challenge is getting the right answers, especially when the world, even your small slice of it is very big. \n",
    "\n",
    "Measuring everything is just way \n",
    "- too expensive, \n",
    "- too time consuming and \n",
    "- in some cases, it's just impossible. \n",
    "\n",
    "Political operatives can't poll every voter. Cell phone companies can't measure the quality level of every single item the produce. A farmer can't measure the actual size of every tomato grown. Scientists, they can't track the health of every single person in the country. Instead of measuring everything, they just measure a small group or subset of the total population. That small subset of measurements is a `sample`. And under the right circumstances, this `sample can act as a representative of the entire population`. The best samples are chosen at random.\n",
    "\n",
    "### Random Sample\n",
    "The most dependable type of data comes from what we call a `simple random sample`. This means that \n",
    "- the sample is chosen such that each individual in the population has the same probability of being chosen at any stage during the sampling process. \n",
    "- And each subset of k individuals has the same probability of be chosen for the sample as any other subset of k individuals.\n",
    "\n",
    "The simple random sample can be rather elusive. Eliminating bias and maintaining data independence is quite challenging. As a result, `alternatives to the simple random sample` are sometimes utilized (but the simple random sample is still the only way to get dependable statistical outcomes). These alternative methods are simpler to organize, easier to carry out, and often, they seem both logical and sound:\n",
    "- **Systematic sample** - Choose one unit and then every k unit thereafter. So if we're measuring customer satisfaction at a store, perhaps you might ask the first person to come out of the store for their opinions, then you might ask every tenth customer after them, for their opinion.\n",
    "- **Sources of bias** - The sampling time and sampling location as well as the presence of a sampler, may introduce bias or inhibit independence.\n",
    "- **Opportunity sample** - the sampler simply takes the first n number of units that come along.\n",
    "- **Stratified sample** - Is one where the total population is broken up into homogeneous groups. Let's say, we're trying to figure out the average amount of sugar in a single cookie, regardless of the type of cookie. We could break up the population into so many different cookie types. Chocolate chip, peanut butter, oatmeal, sugar, ginger, snickerdoodle, oatmeal raisin. From there, we might take a sample of 30 cookies from each category. Perhaps chocolate chip cookies make up 50% of all cookies and ginger cookies make up only 3% of all the cookies. Our very fair-looking system might actually be biased against the most popular cookies.\n",
    "- **Cluster sample** - is similar to stratified samples in that we are breaking things up into groups. What's the difference? In stratified groups, all the members of each group were the same. In clusters, the groups are likely to have a mix of characteristics. They're heterogeneous. Suppose we are testing a new product. We might ask for samples of people in 20 major cities, what they think about the new product. While the people in a single sample might all be from the same city, each sample might contain men and women, people of different races, politics and socio-economic backgrounds.\n",
    "\n",
    "The `simple random sample` will always be the gold standard, but these `alternative sampling methods` should not be completely dismissed.\n",
    "\n",
    "### Sample Size\n",
    "A `sample` is a group of units drawn from a population, and the `sample size` is the number of units drawn and measured for that particular sample. The total population itself may be very large or, perhaps, immeasurable, so a `sample` is just looking at a slice of the population in the hopes of providing us a representative picture of the entire population. The larger the sample size, the more accurate our measurement or, at least, the more confidence we have that our sample is actually providing us a glimpse of the whole population. \n",
    "\n",
    "<img src=\"images/sample_size.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "And this is where sample size becomes important. Why? Well, let's take a look at the formula for standard deviation, where `n` is our sample size. `P` and the quantity `one minus p`, they won't change, but sample size will. So, `the bigger the sample size, the smaller our standard deviation`. \n",
    "\n",
    "<img src=\"images/sample_size4.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "For this sample, if n is equal to five, then one standard deviation would be 13.4% from 90% in either direction, which means that, with a sample size of five, we would expect 68% of all of our samples to have between 76.6% and 100% good forks, since we can't have more than 100% good forks in any sample. \n",
    "\n",
    "<img src=\"images/sample_size2.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "If n equals 25, one standard deviation would be 6%. At n equals 100, one standard deviation would be 3%. And at n equals 400, one standard deviation would be 1.5%, which means that, with a sample size of 400, we would expect at least 68% of all of our samples to have between 88.5% and 91.5% good forks. As you can see, a larger sample size can really make us feel so much more comfortable with our results. It gives us more confidence when we apply the sample results to our larger population.\n",
    "\n",
    "<img src=\"images/sample_size3.png\" alt=\"\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The central limit theorem\n",
    "The central limit theorem. Let's start simple. A distribution of discrete numbers. We start on the left, where we have five values of five. We move right along our distribution. Two units of 10. Four units of 15. Six units of 20. And on the right of our distribution, we have three values of 25. 20 different readings in our entire population. If we average out the values of our 20 different readings, we get an average of 15.0. \n",
    "\n",
    "<img src=\"images/central_limit_theorem.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "Now suppose we didn't want to tally up all 20 values, but we still wanted to find the average of the data set. Could we use samples to direct us to the population mean? Let's try it. Let's take samples of four units every day. Here's our first sample. Sample one, 10, 15, 20, 25. Our sample mean for this sample is 17.5. Sample two, five, 15, 20, and 20. Our sample mean for this sample is 15. Sample three, five, five, 15, and 20. Our sample mean here is 11.25. We have three samples, thus we have three sample means, 17.5, 15, and 11.25. If we average those, we get the mean of our means, 14.58. Not too far off from our actual population mean of 15.0. \n",
    "\n",
    "<img src=\"images/central_limit_theorem2.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "`The central limit theorem tells us the more samples we take, the closer the means of our sample means will get to the population mean`. Actually, it's even more interesting, because as we start to take many more samples, dozens of samples, hundreds of samples, even thousands of samples, the sample means, if plotted as a histogram, our sampling distribution of our sample means would start to look like a normal distribution. \n",
    "\n",
    "<img src=\"images/central_limit_theorem3.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "`Not only does the central limit theorem work with our example with a tiny population and discrete values, it works with massive populations and continuous values`. So no matter if you're interested in learning about the average test scores of a small school, the average weight of watermelons grown in North America, or the political preferences of voters in the United States, the central limit theorem is there to provide you the guidance to understand the overall population with the assistance of some simple random samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Error (for proportions)\n",
    "We've already begun to see the impact of sample size. In general, `the larger the sample size, the more confidence we have in our results`. Now let's shift our attention to the standard error. In short, `the standard error` is the standard deviation of our proportion distribution. \n",
    "\n",
    "Through an example, let's take a look at how we calculate the standard error and also what that calculated number would mean to us. In the cell phone industry, companies struggle to keep their clients happy. Suppose a reputable, national poll finds that 60% of adults are satisfied with their cell phone provider. Let's take that as our population proportion `p`=0.60. We'd like to see if cell phone service in our city reflects what is being seen nationally. In an attempt to measure this, we'll take simple random samples of 100 cell phone users in our city. Now, we know that 100 people can't possibly reflect the satisfaction levels of everyone in our city, therefor we can assume that each sample will carry with it some level of `standard error`. \n",
    "\n",
    "So, how big is this standard error? Well the answer to that question really depends on you guessed it: sample size. `The standard error is ultimately related to the standard deviation`, our formula for standard deviation is seen here, `p` is the proportion population, in this case 0.60 and `n` is sample size. In this case, 100.\n",
    "\n",
    "<img src=\"images/standard_error_proportions.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "So we can see that for n=100, our standard deviation is approximately 0.05 or 5%. And remember, if we assume a normal distribution, 68% of all the samples taken should fall within one standard deviation of the population proportion. So in this situation, for simple random samples with 100 ratings, we would expect 68% of the samples to provide `sample proportions` or `p-hats` between 55% and 65%. 55% is our lower limit. 65% is our upper limit and 60% our population proportion that would be at the center. \n",
    "\n",
    "<img src=\"images/standard_error_proportions2.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "So if tomorrow we gathered a simple random sample of 100 cell phone customers and 57% of those customers were satisfied, we can say that our city was likely on par with the national proportion of 60% because we were within the 5% standard error. \n",
    "\n",
    "Then again, if we could afford to take simple random samples with sample sizes of 1000 cell phone customers. Notice what happens here, since n is now 1000. Our standard deviation drops to about 0.015 or 1.5%. So if n=1000, we would expect 68% of those larger samples to have p-hats between 58.5% and 61.5%. \n",
    "\n",
    "<img src=\"images/standard_error_proportions3.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "So let's recap, `the standard error in situations where we are looking at proportions is the standard deviation`. This is the formula for the standard deviation of a sample proportion, `p-hat`. The bigger our sample size, the smaller our standard deviation. This standard deviation is our standard error. `The standard error allows us to set up a range around the population proportion that extends the equivalent of one standard deviation in both the positive and negative direction`. The formula for our upper limit is p plus the standard deviation and for the lower the limit, p minus our standard deviation. Once the range is established, if we assume the probability distribution is nearly normal, then we would expect that 68% of the simple random samples gathered in the upcoming weeks, that they would fall within one standard deviation or the standard error. \n",
    "\n",
    "What happens when 68% of our samples are not falling within our calculated upper and lower limits? \n",
    "- Perhaps, it signals that something in our city is different form the overall nation. Customers, companies, or a combination of the two might create a unique environment in our city. \n",
    "- Perhaps there is a flaw in the reported national average of 60%, maybe their data gathering techniques were flawed. \n",
    "- Perhaps, the market has changed since that number was first reported or perhaps our sampling method was biased. \n",
    "\n",
    "Don't forget, while standard errors are there to help us judge and analyze future samples, samples that fall beyond the standard error, they should be analyzed not necessarily judged as failures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling distribution of the mean\n",
    "Suppose we know that the average player in a men's college basketball league weighs 180 pounds. Let's also say that the median player weighs about 190 pounds, so that means quite a few of the smaller players in the league are bringing down that average. This league has over 4,000 players. \n",
    "\n",
    "Would we have to weigh everyone of those 4,000 plus players to know the average weight of a player in the league? \n",
    "\n",
    "Well, if you remember, `the Central Limit Theorem tells us that by taking some simple random samples, we can get a very good approximation of the true population average`. \n",
    "\n",
    "If we take five random samples, with a sample size of only four, we might find that those five tiny samples will have sample means that average to perhaps 182 pounds. Now, if we take five random samples, but increase the sample size to 25, we would likely see the mean of the sample means closer to 180.5 pounds. \n",
    "\n",
    "<img src=\"images/sampling_distribution_of_the_mean.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "Only five simple random samples with sample sizes as low as four can get us very close to our true population mean of 180 pounds. Now, this basketball league has the capability of tracking all 4,000 plus players, so we knew that the average weight of the players in this league was actually 180 pounds, but hopefully the example has helped convince you that even when we have a massive population, the Central Limit Theorem tells us we can trust our simple random samples to point us in the direction of the true population mean. \n",
    "\n",
    "Let's say instead of 4,000 well-tracked male college basketball players, we wanted to know the average weight of 18 to 24 year old men in United States colleges. There are millions of young males in college, and these men are not tracked nearly as well as the basketball players, but that shouldn't be a concern. The Central Limit Theorem tells us that if we are diligent in collecting simple random samples, we should trust our sample means in this scenario, with a population of over three million students, to be as accurate as the example of five samples collected from the pool of 4,000 college basketball players. And it works the other way too. If you have a school of only 50 students, a very small population, you can approximate the population mean for those 50 students by simply taking a few simple random samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Error (for means)\n",
    "Through the use of the central limit theorem, we've seen how taking just a few random samples can guide us in the direction of the population mean. Of course when we use only a few samples to try and figure out a population mean, we understand that `the average of our sample means comes with a standard error`. \n",
    "\n",
    "So how do we figure out the standard error for our simple random samples? Let's say we're trying to figure out how long it takes to get our coffee drinks from our local cafe between 7 a.m. and 8 a.m. on a Monday morning. We take samples on four different Mondays. Our sample size for each of these samples is five. Here is our data set for those days, times are in minutes. \n",
    "\n",
    "<img src=\"images/standard_error_means.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "So, for sample A you can see that the time it took our five customers to get coffee ranged from 0.6 minutes to 2.4 minutes. The average for sample A was 1.58 minutes. If we take the average of the sample means, we will find that the average time to get a coffee drink was about 1.52 minutes. And the standard deviation of those four sample means is 0.25 minutes. \n",
    "\n",
    "`The standard deviation of our sample means this is our estimated standard error`. What's interesting is that `there's a relationship between the standard error of our population means and the standard deviation of the population`. Take a look at this formula, Sigma Xbar is equal to Sigma over the square root of our sample size n. Sigma Xbar is our standard error. So it is the standard deviation of our four sample means. On the other side of the equation, we have another Signma, this Sigma is the standard deviation for the entire population.\n",
    "\n",
    "<img src=\"images/standard_error_means2.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "So by plugging in our calculated standard error from our cafe example which was 0.25 minutes, and then plugging in 5 for n, our sample size. We can then solve for Sigma, our populations standard deviation. We can see that based on these four samples with a sample size of five, we can estimate the population's standard deviation is 0.56 minutes. Again, we can see how sample size can have a huge impact on working with samples to find out information about the entire population. In essence, what the formula tells us is that `if we use larger sample sizes, our standard error gets smaller`. This is also important as we collect samples in the future, why, well, when we collect a sample of drink service times from our cafe tomorrow and the rest of the week, we will know that 68% of our samples should have sample means that fall within 0.25 minutes of 1.52 minutes, our average. Upper limit would then be 1.77 minutes and our lower limit, 1.23 minutes. The standard error formula is very simple, but still very informative, by understanding the simple relationship between sample size, the standard deviation of our population and the stand deviation of our sample means, we can better understand our population as well as the samples we take going forward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1\n",
    "80% of customers pay with a debit or credit card. 25 customers are chosen at random each day. 68% of the samples would have p-hats between:\n",
    "```\n",
    "Sigma p-hat = sqrt((0.80*0.20)/25) = sqrt(0.0064). Next take the square root of 0.0064, this provides a standard dev. of 0.08 or 8% (standard error). The p-hats would be between both 80%-8% and 80%+8%.\n",
    "```\n",
    "### Example 2\n",
    "Three of the choices are true of the Central Limit Theorem. Which is not?\n",
    "- the larger the sample size, the taller and more narrow our distribution\n",
    "- the central limit theorem works for both small and large population sizes\n",
    "- the mean of our random sample means directs us to the population mean\n",
    "- (wrong) the greater the size of our samples, the greater the standard deviation\n",
    "\n",
    "### Example 3\n",
    "A school has 70% female students. 25 random students win a prize each day. We would expect 68% of the daily pools of winners to have between _____ females.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='toc03'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confidence Intervals\n",
    "At this point, you should hopefully feel comfortable with the concepts of sample size and the central limit theorem. `The central limit theorem tells us that if we take enough simple random samples, we can get an excellent approximation of our population means`. In other words, rather than measure everything in the population, we can take some random samples. Those random samples will provide us with the measurements that will be nearly normal in our distribution, and will direct us to the population mean. \n",
    "\n",
    "And you also, hopefully, remember that `the larger the sample size of those random samples, the smaller the standard deviation of our distributions`, so the more certain we are about our resulting population mean. Lots of samples make us feel confident about our population numbers.\n",
    "\n",
    "In this section, in which we will cover `confidence intervals`, we're going to go in the opposite direction. `What happens when we have only one sample?` If we have only one sample, how confident are we that this single sample mean is near our actual population mean? \n",
    "\n",
    "In this section, you'll often see results that look like this. We are 95% confident that the average adult in the United States drinks between two and three liters of beverages per day. As you can see, one random sample will allow us to calculate our range and attach to it a level of confidence. Think about how incredibly powerful this is, the efficient use of resources, the overall savings, and the ability to be 95% confident, or perhaps even more confident than that. \n",
    "\n",
    "But before we move on, let's take a moment to discuss what a 95% confidence level means. It means that if, instead of taking a poll only once, we took a similar poll 20 times. 19 times, the results of the poll, in other words, the resulting ranges of those 19 polls, they would capture the population mean. But of course, one of the 20 times, the reported range would not include our population mean. Remember that the next time a pre-election poll predicts the wrong candidate to win. So, let's get started with that exact example. Let's see how they create confidence intervals for those pre-election polls.\n",
    "\n",
    "### What exactly is a confidence interval?\n",
    "Let's create a 95% confidence interval for an election poll where the voters have two choices: Candidate A and Candidate B. As you may have guessed, we'll be working with proportions. Before we start creating a 95% confidence interval for this scenario, let's recap a few things. \n",
    "\n",
    "- First, if we took a lot of voter samples, the distribution would be approximately normal. \n",
    "- Second, the larger the voter sample size, the smaller the variation, and thus, the smaller the standard deviation of the resulting distribution. \n",
    "- Third, a 95% interval is one where we are 95% certain that our interval which will be centered at the sample's proportion will contain the actual population proportion. \n",
    "\n",
    "Now what we're going to do is take a single sample. This single random sample might include 50 eligible voters, but the resulting sample proportion of eligible voters that favor Candidate A is a single number we can call `p-hat`. This single sample proportion is a single dot on this distribution. \n",
    "\n",
    "<img src=\"images/confidence_intervals.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "Now the question is, is it a dot that is close to the population's true proportion or is it a dot that is very far away from the true proportion. I also want you to remember we don't actually know the true population proportion. So while our single sample is here, we don't know if the true population proportion is here or here or here. So, what are we actually doing? Well, let's imagine that this is the true population proportion distribution. We're going to take a sample and build an interval around that sample. And we're going to hope that the true population proportion falls within that interval. Now, what we really want to create is an interval, an interval of a certain length we will call `y`. This interval will have a lower limit and an upper limit. And this interval will be centered on our sample proportion, `p-hat`. This interval would need to be big enough where if we took 20 samples, 19 of the 20 intervals would contain the true population proportion. In other words, 95% of my samples would have an interval within range of `p`. \n",
    "\n",
    "So let's recap. When we gather a sample with a certain sample proportion `p-hat` and when we create an interval around that p-hat, we are 95% certain that the real population proportion is somewhere between the lower and upper limit of that interval.\n",
    "\n",
    "### 95% confidence intervals for population proportions\n",
    "So there's an upcoming election for mayor of a large city between two candidates. Let's simply call the candidates Candidate A and Candidate B. You work for Candidate A. Candidate A's campaign team wants to know where Candidate A stands so they ask you to conduct a poll. You gather a random sample of 100 voters and ask if they will be voting for Candidate A or Candidate B. 55% of the 100 voters polled said they planned on voting for Candidate A. Anything over 50% in the real election would result in a win for your candidate. So far, based on the results of the poll, things look promising for your candidate but remember, this was just one sample with a sample size of 100. \n",
    "\n",
    "<img src=\"images/confidence_intervals2.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "Now, look, I understand that my small pre-election poll likely didn't provide the actual percentage of votes Candidate A will get on the actual day of the election but maybe we're close. So let's create a 95% confidence interval. In other words, let's use our sample result to create an interval that very likely includes the actual percentage of votes Candidate A will get on election day. \n",
    "\n",
    "Let's take a look at a normal distribution curve. If we want a 95% confidence level, that would mean we'd want to capture 95% of the area under the curve between two points equidistant from the sample proportion which means 2.5% of the area under the curve on the right side of the curve would not be included and 2.5% of the area under the curve on the left side of the curve would not be included. So how do we find these two points which establish our interval? We'll have to take a look at `z-scores`. \n",
    "\n",
    "<img src=\"images/confidence_intervals3.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "`Z-scores tell us how many standard deviations away from the mean we would need to be to capture a certain percentage of the total distribution`. So we pull up a z-score table. \n",
    "\n",
    "<img src=\"images/confidence_intervals4.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "We find 0.975. This means 97.5% of the data points are to the left of this point and thus, 2.5% of the data points would fall to the right of this point. As you can see, our z-score is 1.96. This means if we go 1.96 standard deviations in the positive direction and 1.96 standard deviations in the negative direction, 95% of the area under our distribution will fall between these two limits. \n",
    "\n",
    "<img src=\"images/confidence_intervals5.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "So let's see where we are so far. `Our sample proportion is 0.55`. `This will be the center of our interval`. Therefore, the upper limit will be 0.55 plus 1.96 times our standard deviation and our lower limit will be 0.55 minus 1.96 times our standard deviation. So of course now, we need to find our standard deviation. \n",
    "\n",
    "In the absence of the population proportion and the population's standard deviation, we can use the formula for the standard error. It's basically the formula for the standard deviation but we use the sample proportion as our p hat. Since we use the sample proportion, p hat, instead of the population proportion, p, we can't call it the standard error. When you use the sample proportion, it's called `the sampling error`. So we put p hat into our formula and of course, in our particular example, we polled 100 voters so n is equal to 100. As you can see, our standard deviation or sampling error in this case is 0.05. \n",
    "\n",
    "<img src=\"images/confidence_intervals6.png\" alt=\"\" style=\"width: 300px;\"/>\n",
    "\n",
    "So when we plug that standard deviation into our formula, we get an interval that has a lower limit of 0.452 and an upper limit of 0.648. \n",
    "\n",
    "<img src=\"images/confidence_intervals7.png\" alt=\"\" style=\"width: 300px;\"/>\n",
    "\n",
    "Uh oh, our interval goes all the way down to 0.452 which means that our margin of error tells us that losing is still possible. Then again, a large proportion of our interval is over 50%. Still, your candidate is probably a bit nervous but how about if we took a bigger sample? \n",
    "\n",
    "How about if the campaign is willing to fund a poll of one thousand voters? The numbers on this poll are a bit lower for your candidate. This poll tells us 54% of the voters are for Candidate A. Let's calculate our sampling error with our new sample proportion and sample size of one thousand. \n",
    "\n",
    "<img src=\"images/confidence_intervals8.png\" alt=\"\" style=\"width: 300px;\"/>\n",
    "\n",
    "Our sampling error is now 0.16. So let's calculate our interval limits. \n",
    "\n",
    "<img src=\"images/confidence_intervals9.png\" alt=\"\" style=\"width: 300px;\"/>\n",
    "\n",
    "Our new interval stretches from 0.509 to 0.571. If the election were to have a result identical to anything within our 95% confidence interval, Candidate A would win. Remember, according to this sample, `there is now a 95% chance that on election day, Candidate A will receive between 50.9% and 57.1% of the vote`. There's only a 5% chance that the election day results will fall outside of that interval. And don't forget, it's possible that those 5% might include results that are even better than 57.1% of the vote for Candidate A. No matter how you slice it, that should make Candidate A's team feel pretty good, right? Yeah, there's always that one person on the team that asks, can we get a 96% interval or what about 98%? So for those people, next, we'll create confidence intervals that are greater than 95%. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do you want to be more than 95% confident?\n",
    "For some people, 95% just isn't good enough. So what happens if someone demands a 98% confidence interval? Well, let's remember a 95% confidence interval stretches in equal distances in opposite directions from our sample proportion. How far? Enough to include 95% of the probability distribution, which means that a 98% confidence interval would have to stretch a little bit farther so our interval would include 98% of the probability distribution. Notice my numbers didn't really get any better. It's more like saying, \"I'm 75% sure \"my lost car keys are in my living room, \"but I'm 99% sure my lost car keys \"are somewhere in this house. I simply increase the likely location of my keys and that increased the likelihood that this area contained my keys. \n",
    "\n",
    "So, when someone demands that we provide a 98% confidence interval instead of a confidence interval of 95%, it's important that they understand what the difference is between the two intervals. With that in mind, let's go ahead and figure out how to calculate the limits of this expanded interval. Remember, to find the limits of our 95% confidence interval, we used these formulas. \n",
    "\n",
    "<img src=\"images/confidence_intervals10.png\" alt=\"\" style=\"width: 300px;\"/>\n",
    "\n",
    "Our sample proportion, p-hat plus or minus our sampling error, which is really just our sample's standard deviation, times 1.96. Why 1.96? Because that was the appropriate z-score for 95%. So really, the only thing we will change to adjust our interval is the z-score of 1.96. But how do we find the right z-score for, let's say, 98%? Don't get fooled, you do not want the z-score for 0.98. You actually need the z-score for 0.99. Why? Well, let's take a look at our distribution. We want to set limits where 98% of the data is under the curve between the limits, so 2% of the distribution falls outside of the limits. 1% of the distribution on the right end of the curve and 1% on the left end of the curve. So we need to find the z-score for 0.99. Here's a z-score table. \n",
    "\n",
    "<img src=\"images/confidence_intervals11.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "Within the table, we are looking for 0.9900 or the closest number that is greater than that. In this case, the appropriate z-score is 2.33, and since the interval will stretch in equal distances in opposite directions, we will use 2.33 for both our upper and lower limit. \n",
    "\n",
    "<img src=\"images/confidence_intervals12.png\" alt=\"\" style=\"width: 300px;\"/>\n",
    "\n",
    "So, if we poll 1,000 people and 540 of those people favor Candidate A instead of Candidate B, then we know we have a p-hat of 0.54 and we know that n is equal to 1,000.\n",
    "\n",
    "<img src=\"images/confidence_intervals13.png\" alt=\"\" style=\"width: 300px;\"/>\n",
    "\n",
    "Using this formula, we can calculate our sampling error. \n",
    "\n",
    "<img src=\"images/confidence_intervals14.png\" alt=\"\" style=\"width: 300px;\"/>\n",
    "\n",
    "Our sampling error, therefore, is 0.017. \n",
    "\n",
    "We now have what we need to calculate our 98% confidence interval.\n",
    "\n",
    "<img src=\"images/confidence_intervals15.png\" alt=\"\" style=\"width: 300px;\"/>\n",
    "\n",
    "Look at that. If an election victory requires at least 50% of the vote, victory is still within our margin of error. I know, I know there's always the person that wants more. How about 99%? Using the same logic as with 98%, we realize we need to find the z-score for 0.995, which is 2.58. Let's calculate our 99% interval. \n",
    "\n",
    "<img src=\"images/confidence_intervals16.png\" alt=\"\" style=\"width: 300px;\"/>\n",
    "\n",
    "Here we see that the chance for a narrow election loss is within our margin of error. Nonetheless, it looks like, based on our simple random sample, we can feel fairly confident that an election win is likely. Still, even with this strong statistical evidence, the candidate can lose. If the candidate lost after we reported that a win seemed rather likely, how might that loss be explained? Let's try and figure that out next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explaining unexpected outcomes\n",
    "Suppose before an election, a polling organization reports a 95% confidence interval for candidate A. This confidence interval stretches from 0.51 to 0.54. In other words, the poll believes that Candidate A will get between 51% and 54% of the vote. Then election day comes around and Candidate A looses. Candidate A would probably be furious. Before the election, they were very confident of a win. And now they realize that they actually lost. \n",
    "\n",
    "How could this have happened? Well this is where it helps to be a well rounded statistician. `Beyond having a knowledge of the numbers and formulas, you need to understand the real environment that surrounds the poll`. In this case, it would be helpful if we understood how political polls are done and also the nature of the actual election. What might go wrong during the actual poll? \n",
    "- Lying, \n",
    "- Correspondents might want to throw off the polls, they may just lie. \n",
    "- Or perhaps, they're embarrassed to tell a pollster about their true opinions, and thus they would rather give them an answer that would please the pollster. \n",
    "- Maybe they didn't lie. Perhaps the respondent just changed their mind between the time of the poll and the actual date of the election. It's possible some people were unsure who they wanted to vote for on the day of the poll. But they chose a candidate for the poll just to please the pollster. \n",
    "- Perhaps, there were issue in gathering a random sample. The location of the poll. The people chosen. How they were chosen. The incentives used to entice more participants. \n",
    "- It takes a very experienced organization to gather a truly random sample. Sometimes, in an effort to influence voters that are still uncertain about who they will vote for, some politically biased polling organizations might actually seek biased polling results that they can use in the media to show that their candidate is popular among likely voters. These organizations may have had poor sample selection, poorly worded questions, or other questionable if not deceitful practices. \n",
    "\n",
    "As you can see, the polling process itself is filled with challenges. So let's move on to election day. What might go wrong on Election Day. \n",
    "- Bad weather. \n",
    "- A health epidemic. \n",
    "- Unsafe travel conditions. \n",
    "- Car trouble. \n",
    "- Work or family commitments. \n",
    "\n",
    "This is just a short list of reasons people might not be able to get to the voting booth on Election day. Perhaps between the time of the pre election poll, and election day, something changed. \n",
    "- An event occurred that changed the way voters made their decisions. \n",
    "- Perhaps a scandal involving one of the candidates was uncovered. \n",
    "\n",
    "Sometimes, voters just choose to stay home. Why? \n",
    "- They don't really care that much. \n",
    "- Maybe they just forgot. \n",
    "- Perhaps they heard the lines were long. \n",
    "- Maybe they had been watching the news, and analysts made it seem as though the outcomes were certain. \n",
    "- Perhaps the voter thought the election is a done deal. \n",
    "- My vote isn't going to make a difference at this point. \n",
    "\n",
    "Hopefully you aren't growing distrustful of statistics. If you investigate confidence intervals that have been reported over the last few decades, you'll see that very often, they are very accurate. Nonetheless, when the confidence interval misses the mark, it's important to know where the poll might have gone wrong. Actually, it's best to know these things before the study is even performed. `If you're looking to use confidence intervals to make important decisions, be sure you investigate how the study was done and which assumptions and simplifications were included in the development of the study. As I said, a good statistician has to know more than numbers and formulas. They need to really understand the environment that they're looking to measure`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 95% confidence intervals for population means\n",
    "Political campaigns rely heavily on developing confidence intervals for voter preference. In many cases, these types of confidence intervals are for proportions, but how about if we wanted to develop confidence intervals for these types of situations? \n",
    "- What's the average salary of a cardiologist? \n",
    "- What's the weight of the average grapefruit grown in the state of Texas? \n",
    "- How long does it take the average female, 20 to 29 years old, to run a mile? \n",
    "\n",
    "In these cases, we're not looking at proportions. Instead, `we're looking for means`. So, how do we create a confidence interval for a population mean? Well, it's not really that much different than the reasoning we use to develop confidence intervals for proportions. Here are the `formulas used to develop a 95% confidence interval for proportions`. \n",
    "\n",
    "<img src=\"images/confidence_intervals17.png\" alt=\"\" style=\"width: 300px;\"/>\n",
    "\n",
    "The sample proportion plus or minus 1.96, times the sample's sampling error. Remember, `1.96` is the appropriate `Z-Score for a 95% interval`. So, if we wanted a different confidence interval, we would just find the appropriate Z-Score. Now let's move from proportions to means. Here are the `formulas for developing a 95% confidence interval for means`. \n",
    "\n",
    "<img src=\"images/confidence_intervals18.png\" alt=\"\" style=\"width: 300px;\"/>\n",
    "\n",
    "Pretty much the same thing, except we substitute in the sample mean where we had the population proportion. \n",
    "\n",
    "So, suppose we wanted to create a 95% confidence interval for the average time it takes females, in their twenties, to run a mile. We could take a simple random sample of women in their twenties. We would then time their miles. Suppose these are the one mile run times in minutes for a simple random sample of nine women in their twenties. \n",
    "\n",
    "<img src=\"images/confidence_intervals19.png\" alt=\"\" style=\"width: 100px;\"/>\n",
    "\n",
    "So, all we need to do is find the sample mean. In this case, it's 8.96 minutes. The standard deviation for these nine times is 1.36. We then use this standard deviation, 1.36, to compute our sampling error. \n",
    "\n",
    "<img src=\"images/confidence_intervals20.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "If we plug these numbers into our formulas for the 95% confidence interval, we will get an interval from 8.08 minutes to 9.84 minutes. \n",
    "\n",
    "<img src=\"images/confidence_intervals21.png\" alt=\"\" style=\"width: 300px;\"/>\n",
    "\n",
    "According to this simple random sample, `there is a 95% chance that the population mean for the one mile run time for females in their twenties is somewhere between 8.08 minutes and 9.84 minutes`. Just as with our proportion intervals, by adjusting our sample size, which influences our sampling error, we can impact the size of the interval. And, of course, when we ask for an interval with a different confidence level, we can just adjust our Z-Score, which will also influence our confidence interval.\n",
    "\n",
    "You are now ready to develop confidence intervals for situations that use proportions as well as situations that require means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1\n",
    "Which of the following is NOT an explanation of why poll results may differ from an election outcome?\n",
    "- (correct) the group polled was a random sample\n",
    "- the respondents were still unsure at time of poll\n",
    "- a major event occurred between the poll date and the election date\n",
    "- the polling organization was biased\n",
    "\n",
    "### Example 2\n",
    "```\n",
    "When looking for a 95% confidence interval we need the z-score for ___.\n",
    "0.975\n",
    "In order for to have 0.95 within the interval, 0.05 would be outside the interval, 0.025 on each end of the interval. Thus a z-score of 0.975 would account for all but the 0.025.\n",
    "```\n",
    "\n",
    "### Example 3\n",
    "```\n",
    "A 95% confidence interval means that if a similar random sample were taken 20 times, we would expect the population mean to fall within the resulting range ____ times.\n",
    "19\n",
    "20 samples * 0.95 = 19 samples within the range.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='toc04'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis Testing\n",
    "\n",
    "Have you ever come upon situations, outcomes, or events that just seem odd? In a city made up of 51% women, where jury pools are said to be chosen at random, a certain jury pool of 50 people contains only eight women. A national restaurant chain provides a game piece for every drink a customer buys. There are 10 prizes worth over $100,000. Two of those prizes are won by relatives of restaurant employees. Three employees from a particular chemical factory with 400 employees are diagnosed with brain cancer in a two-year period. When you hear things like this, they make you think. It doesn't seem right. Is that even possible? And if it is possible, how likely is it that it could have happened at random? Sometimes these questions and the related answers could impact our careers and companies. They may help us make decisions. They might influence our superiors to act. Perhaps you work at a healthcare company. Your company has developed a drug to treat the common cold. It's reported that the average adult with the common cold will experience cold symptoms for about 8.5 days. When testing this new medicine on a random sample of 250 people with the common cold, it's found that these patients recovered about 1.2 days sooner that those that did not take this drug. Is this significant? Could this sample just be the result of chance, or did this drug have an impact? Should the drug be tested further? Does this mean this new drug should be approved for use? This is where hypothesis testing comes in. \n",
    "\n",
    "**Hypothesis testing** is an extremely popular method for exploring outcomes. In general, statisticians will: \n",
    "- make an assumption about a population. \n",
    "- collect a random sample from the population. \n",
    "- measure the sample. \n",
    "- see whether or not the sample measurement supports their assumption. \n",
    "\n",
    "It can be complex, but when done properly, hypothesis testing can be extremely powerful. Hypothesis testing, it really requires you to use all of your statistical muscles, but once you have hypothesis testing at your disposal, you'll be able to provide valuable input in almost any career. Science, medicine, business, education, public policy, and even sports and entertainment. No matter your field, make sure you understand the most basic elements of hypothesis testing.\n",
    "\n",
    "### How to test a hypothesis in four steps\n",
    "\n",
    "<img src=\"images/hypothesis_testing_01.png\" alt=\"\" style=\"width: 300px;\"/>\n",
    "\n",
    "The adult residents of a large town with an adult population of 35,000 are half male and half female. Each week, 50 adults are chosen at random to participate in jury duty. Women have complained that they are getting called to jury duty more often than the men. Jury administrators contend the system is random and fair. A committee is setup to investigate. They use the next jury pool as a sample. They find that in that pool of 50 potential jurors, 14 are men and 36 are women. The jury administration contends this happened by chance. A lobbying group disagrees. What we have here is an excellent opportunity to utilize hypothesis testing. \n",
    "\n",
    "**Hypothesis testing** is really a process. In our case, a four step process. \n",
    "\n",
    "<img src=\"images/hypothesis_testing_02.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "In our first step, we need to setup our hypotheses. There will typically be two hypotheses. H sub-zero or H not, this is our `null hypothesis`. We might refer to this as what we consider to be the status quo. In our case, this basically accepts that these jury numbers did happen by chance. In this hypothesis test, the null hypothesis states that everything's okay. And thus, the odds of a women being picked for the jury duty was at least 50%. So, our null hypothesis is p is less than or equal to 0.50. In other words, women had a 50% chance, or perhaps even less than a 50% chance, of being chosen for jury duty. \n",
    "\n",
    "Let's move to our `alternative hypothesis`, H sub-a, This would be the opposite of the null hypothesis. This one would say that women did not have a 50% chance of being chosen for jury duty. In fact, the chance of a women being chosen for the jury are greater than 50%. So here, our alternative hypothesis is p is greater than 0.50. \n",
    "\n",
    "<img src=\"images/hypothesis_testing_03.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "With our two hypotheses now stated, we'll then also want to state as `significance level`. Essentially, this sets a threshold for our test. In other words, suppose through our test we find that 36 or more women might end up on a 50 person jury pool by chance 30% of the time or 20% of the time. Or, what if it's only 10% of the time? Would you believe that this actually happened by chance or would you say, if it is below some `significance level alpha`, you would think that something was wrong. \n",
    "\n",
    "So, let's set our significance level at 5%. If 36 or more women ending up on a jury has less than a 5% chance of occurring at random, then, we will reject our null hypothesis. \n",
    "\n",
    "<img src=\"images/hypothesis_testing_04.png\" alt=\"\" style=\"width: 600px;\"/> \n",
    "\n",
    "In our second step, we look to find a `statistic` that will assess the validity of our null hypothesis. How could we see if this outcome, 36 women and 14 men, or an outcome that is even more extreme, could happen at random under these circumstances. Here we will use binomial probabilities. Our `p` here is equal to 0.50. That's the probability of a women being chosen for the jury panel. And, we will have 50 trials since that's how many seats there are on the jury panel. Finally, the number of successful trials will be 36. We have our null hypothesis. We also have our `test statistic`. Now, we find the `p value for that test statistic`. \n",
    "\n",
    "The `p value` is the probability that this outcome, 36 women and 14 men, or an outcome even more extreme, could occur by chance. So, we're looking for the probability that the number of women chosen for the jury panel would be 36 or more. What do we find? Whether you did the calculation the hard way, with a binomial table, with a spreadsheet, or perhaps with an online binomial calculator, you would find the probability of this particular outcome would be 0.0013 or 0.13%. \n",
    "\n",
    "<img src=\"images/hypothesis_testing_05.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "Those are some pretty long odds. So, our final step, time to compare our p value to our fixed significance level. \n",
    "\n",
    "<img src=\"images/hypothesis_testing_06.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "What we found was that assuming that the odds that a man and a women were equally likely to be chosen for a jury there was only a 0.13% chance that at random 36 or more women would be chosen for a panel of 50 potential jurors. Our fixed significance level, alpha, was 0.05 or 5%. Clearly, the p value fell short of our significance level and thus, we must reject the null hypothesis. This means, we believe that something is making it much more likely for a women to be chosen versus a man. \n",
    "\n",
    "Now, it doesn't prove that the cause is evil or intentional, nor does it prove that the cause is unintentional and innocent. It simply means, we reject the null hypothesis. Let's briefly talk about that too. Our only outcomes possible for this test would have been to reject the hypothesis, which is what we just saw. The alternative would have been do not reject the null hypothesis. Notice, that does not mean we said accept the hypothesis. We were looking to contradict the hypothesis. It's sort of like saying, a person on trial is guilty or not guilty. Guilty means the evidence is there to convict. Not guilty means there was a lack of evidence. Not guilty does not necessarily mean the jury believed the person was innocent, they just lacked the evidence to prove guilt. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-tailed vs. two-tailed tests\n",
    "Let's consider three different statements. \n",
    "- First, a recent national study found that the average American between the ages of 18 and 24 checks their phone 74 times per day. A mobile service provider questions these results. \n",
    "- Second, the average amount of time it takes an adult to recover from the common cold is 8.5 days. A new medicine was tested on a sample of adults suffering from the common cold. The average recovery time for the people in this group was 7.3 days. The company that developed this medicine thinks the drug should be considered for federal approval. \n",
    "- Finally, consider the national average for the college entrance exam, 1000 points. The Regent Test Prep Academy claims that their students consistently beat that national average. \n",
    "\n",
    "These are all situations where `hypothesis testing` would be useful. But each of these situations would require a different type of hypothesis test. Let's look at each situation individually. \n",
    "\n",
    "#### Scenario 1\n",
    "In our first situation, we had a claim that said people between the ages of 18 and 24 checked their phones 74 times per day. Some folks doubted that claim though. Notice, this group did not say the number was too high, nor did they contend the number was too low. They just expressed doubt in the stated average of 74 times per day. In this case, out hypotheses look like this. \n",
    "\n",
    "<img src=\"images/hypothesis_testing_07.png\" alt=\"\" style=\"width: 300px;\"/>\n",
    "\n",
    "Our null hypothesis, H sub zero, or H naught. Mu is equal to 74.0. Our alternative hypothesis H sub A, Mu is not equal to 74.0 if we look at our normal distribution, what we have is this, 74.0 is the mean of our null hypothesis. Let's say that we thought that anything more than 1.7 standard deviations from the mean in either direction, would mean we could reject our null hypothesis.\n",
    "\n",
    "<img src=\"images/hypothesis_testing_08.png\" alt=\"\" style=\"width: 300px;\"/>\n",
    "\n",
    "On the the other hand, anything that was less than 1.7 standard deviations, from the null hypothesis mean, would tell us that we could not reject the null hypothesis. \n",
    "\n",
    "<img src=\"images/hypothesis_testing_09.png\" alt=\"\" style=\"width: 300px;\"/>\n",
    "\n",
    "As you can see, we have two rejections areas here, one rejection area in the positive direction, greater than the mean. The other in the negative direction, less than the mean. This is considered a `two tailed test` because the null hypothesis is tested in both directions. \n",
    "\n",
    "#### Scenario 2\n",
    "On the other hand, in our example where the average person recovers from the cold in 8.5 days, our test group recovers in 7.3 days. This is a `one tail test`. Why? Well in this case, our hypotheses look like this. \n",
    "\n",
    "<img src=\"images/hypothesis_testing_10.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "Our null hypothesis H sub zero, Mu is greater than or equal to 8.5 days. Our alternative hypothesis H sub a, Mu is less than 8.5 days. So our null hypothesis is saying that patients do not recover faster with the drug and perhaps they may even take longer to recover. Both of these situations would indicate the medicine was not helpful. The alternative hypothesis is indicating that the medicine does in fact have an impact. \n",
    "\n",
    "<img src=\"images/hypothesis_testing_11.png\" alt=\"\" style=\"width: 300px;\"/>\n",
    "\n",
    "On our normal distribution graph, we have 8.5 as our null hypothesis mean. We can set two areas, 1.7 standard deviations from the mean. The difference here is that the drug can only be considered helpful if the patients actually get better faster. Which means that this area to the left, this small single tail, represents the area where we would reject the null hypothesis. \n",
    "\n",
    "<img src=\"images/hypothesis_testing_12.png\" alt=\"\" style=\"width: 300px;\"/>\n",
    "\n",
    "The large area the right would indicate that we could not reject the null hypothesis. That would be bad news for the drug company. \n",
    "\n",
    "#### Scenario 3\n",
    "So lets take a look at our test prep school. This example is very similar to the cold medicine example. The difference here is that we are looking for an increase in the test scores. Here are our hypotheses for this situation. \n",
    "\n",
    "<img src=\"images/hypothesis_testing_13.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "Our null hypothesis, H sub zero, H naught. Mu is less than or equal to 1000 points. Our alternative hypothesis H sub A, Mu is greater than 1000 points. Our null hypothesis is saying students of the Regent School do not see increased test scores. Instead they see average scores or perhaps even below average scores. The alternative hypothesis says that the Regent students do score over the national average. \n",
    "\n",
    "<img src=\"images/hypothesis_testing_14.png\" alt=\"\" style=\"width: 300px;\"/>\n",
    "\n",
    "What do we see on our normal distribution? Again, we see one tail. If we land in this area on the right, we would reject the null hypothesis. \n",
    "\n",
    "<img src=\"images/hypothesis_testing_15.png\" alt=\"\" style=\"width: 300px;\"/>\n",
    "\n",
    "If we land anywhere else in the large area to the left, we would not reject the null hypothesis. \n",
    "\n",
    "As you start to look for opportunities to utilize hypothesis testing, be sure you consider whether your hypothesis test is a one tailed test, or a two tailed test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Significance test for proportions\n",
    "A candidate's campaign finds that in a random sample of 500 eligible voters, 54% of those polled said they planned on voting for this candidate. The candidate needs over 50% of the vote to win the election. This candidate would like to test the hypothesis that he will win this election. Let's go through our four step process. \n",
    "\n",
    "<img src=\"images/hypothesis_testing_16.png\" alt=\"\" style=\"width: 300px;\"/>\n",
    "\n",
    "Step one, we're going to develop the hypotheses and state the significance level. \n",
    "\n",
    "<img src=\"images/hypothesis_testing_17.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "H sub zero, our null hypothesis, will be p is less than or equal to 0.50. This hypothesis states that the candidate would get 50% or less of the votes, and thus not have enough of the votes to win the election. H sub a, our alternative hypothesis, the candidate wins. This would be the opposite of the null hypothesis. This one would say that this candidate would get a majority of the vote and thus win the election. Our alternative hypothesis is p is greater than 0.5. \n",
    "\n",
    "<img src=\"images/hypothesis_testing_18.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "Our significance level for this test will be 5%. If this has less than a 5% chance of occurring, then we reject our null hypothesis. We're looking at a one-tailed test, where the rejection region is on the right-hand side of our distribution. If our proportion does not fall in the rejection region, we'll not have enough evidence to reject the null hypothesis. \n",
    "\n",
    "<img src=\"images/hypothesis_testing_19.png\" alt=\"\" style=\"width: 300px;\"/>\n",
    "\n",
    "So let's go to step two, we're going to identify the test statistic. In this situation, our test statistic is a `z-score`. We will call it `z sub p`. This `z-score` will establish the point on our distribution which divides the do not reject area from the rejection area. `p hat` is the sample proportion. `p sub zero` is the proportion from our null hypothesis. `n` is our sample size. \n",
    "\n",
    "<img src=\"images/hypothesis_testing_20.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "So in our case, p hat is equal to 0.54. p sub zero is equal to 0.50. And n our sample size is 500. And if we use these numbers, we get a z-score of 1.79. \n",
    "\n",
    "So now we move on to step three, our `p-value`. \n",
    "\n",
    "<img src=\"images/hypothesis_testing_21.png\" alt=\"\" style=\"width: 300px;\"/>\n",
    "\n",
    "So our test statistic z sub p is 1.79. If we look at this number on our z-score chart, you'll find that 1.79 leads you to 0.9633. So our p-value is 1 minus 0.9633 which gives us a p-value of 0.0367. \n",
    "\n",
    "Step four, so now we're going to compare our p-value to our fixed significance level. \n",
    "\n",
    "<img src=\"images/hypothesis_testing_22.png\" alt=\"\" style=\"width: 300px;\"/>\n",
    "\n",
    "Our fixed significance level, alpha, was 5% or 0.05. Our p-value was 0.0367. That's smaller than our 0.05 significance level, thus `we can reject the null hypothesis`. \n",
    "\n",
    "Graphically, we can look at this a few ways. Here's our distribution. \n",
    "\n",
    "<img src=\"images/hypothesis_testing_27.png\" alt=\"\" style=\"width: 300px;\"/>\n",
    "\n",
    "We established that the left side of the distribution is the do not reject the null hypothesis area. The right part of the distribution is the reject the null hypothesis area. Our alpha was 0.05, which means that 95% of the distribution was on the left side of the distribution, and 5% was to the right. \n",
    "\n",
    "We can also look at this by comparing z-scores. The z-score for 0.05 on a one-tailed test is 1.65. That would be 1.65 standard deviations from the null hypothesis population proportion, which was 0.50. \n",
    "\n",
    "<img src=\"images/hypothesis_testing_23.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "Our calculated z sub p though, was 1.79. 1.79 standard deviations from the population proportion. \n",
    "\n",
    "<img src=\"images/hypothesis_testing_24.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "Again, we land in this region on the right. So we reject the null hypothesis. The politician can breathe easy. Unless they demand `a hypothesis test with a 2% significance level`. So here is where the 2% significance level would be on our distribution. \n",
    "\n",
    "<img src=\"images/hypothesis_testing_25.png\" alt=\"\" style=\"width: 300px;\"/>\n",
    "\n",
    "Here is where our p-value of 0.0367 would put us. \n",
    "\n",
    "If we wanted to use our z-scores, the z-score for 0.02 on a one-tailed test is 2.06. That would be 2.06 standard deviations from the null hypothesis population proportion which was 0.50. Our calculated z sub p though was 1.79. \n",
    "\n",
    "<img src=\"images/hypothesis_testing_26.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "No matter the method, we are now in the do not reject the null hypothesis area. In this case, the hypothesis test tells us `we cannot reject the hypothesis` that the candidate will get 50% or less of the vote. As you can see, this hypothesis test hinged on significance level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Significance test for means (acceptance sampling)\n",
    "K-Nosh is a national gourmet dog food company. They sell thousands of bags of dog food each day. They sell dog food in eight, 20, and 40-pound bags. And the 20-pound bag is by far the most popular size. K-Nosh's high-end customers demand outstanding products and excellent service. Customers don't want a bag with less than 20 pounds. So while the bag is labeled as 20 pounds, K-Nosh sets the desired weight of each bag at 20.15 pounds to ensure customers get at least 20 pounds in each bag. \n",
    "\n",
    "<img src=\"images/hypothesis_testing_28.png\" alt=\"\" style=\"width: 300px;\"/>\n",
    "\n",
    "Each day, K-Nosh employees pull a random sample of 100 bags out of the thousands they ship. Based on the 100-bag sample, they will either send out the shipment or they will reject the shipment for that day. Today's sample had an average weight of 20.10 pounds, and the population standard deviation is 0.26 pounds. So let's start our four-step process. \n",
    "\n",
    "Step one, develop the hypotheses and state the significance level. So, let's develop our hypotheses. \n",
    "\n",
    "<img src=\"images/hypothesis_testing_29.png\" alt=\"\" style=\"width: 300px;\"/>\n",
    "\n",
    "Our null hypothesis, H sub zero or H-naught, mu is greater than or equal to 20.15 pounds. This hypothesis states that the bags of dog food are equal to or greater than 20.15 pounds. It's what we would consider the standard state. Our alternative hypothesis, H sub a, this one says the bags of dog food weigh less than 20.15 pounds. This would be the opposite of the null hypothesis. Our alternative hypothesis is mu is less than 20.15 pounds. As usual, we will see whether or not we will reject the null hypothesis. If we reject the null hypothesis, that would mean K-Nosh would not make any shipments of 20-pound bags on that date. \n",
    "\n",
    "<img src=\"images/hypothesis_testing_30.png\" alt=\"\" style=\"width: 300px;\"/>\n",
    "\n",
    "Our significance level for this test will be five percent. If this has less than a five percent chance of occurring, then we reject our null hypothesis. We're looking at a one-tail test where the rejection region is on the left-hand side of our distribution. If our sample falls in the rejection region, we will reject the entire shipment. \n",
    "\n",
    "So now we move on to step two, identify the test statistic. \n",
    "\n",
    "<img src=\"images/hypothesis_testing_31.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "In this situation, our test statistic is a z-score. This `z-score` will establish the point on our distribution which divides the \"do not reject\" area from the rejection area. `x-bar` is the sample mean. `Mu` is the mean from our null hypothesis. `n` is our sample size. And `sigma` is the population standard deviation. In our case, x-bar was equal to 20.10 pounds. Mu was 20.15 pounds. n, our sample size, is 100. And sigma, the population standard deviation, is 0.26 pounds. So if we use these numbers, we get a z-score of -1.92. \n",
    "\n",
    "<img src=\"images/hypothesis_testing_32.png\" alt=\"\" style=\"width: 300px;\"/>\n",
    "\n",
    "Step three, our p-value. Our test statistic z is -1.92. If we look this up on our z-score chart, you will find that -1.92 leads you 0.0274 or 2.74%. That is our p-value, 0.0274. \n",
    "\n",
    "<img src=\"images/hypothesis_testing_33.png\" alt=\"\" style=\"width: 300px;\"/>\n",
    "\n",
    "Step four, we're going to compare our p-value to our fixed significance level. Our fixed significance level alpha was five percent or 0.05. Our p-value was 0.0274. That's smaller than our 0.05 significance level. Thus, we have to reject the null hypothesis. \n",
    "\n",
    "<img src=\"images/hypothesis_testing_35.png\" alt=\"\" style=\"width: 300px;\"/>\n",
    "\n",
    "Our alpha was 0.05 which means that 95% of the distribution was on the right side of the alpha and five percent was to the left. Remember, we want to be close to our goal of 20.15 pounds. If we're too far to the left of 20.15 pounds, the bags are likely too light. If we compare z-scores, the z-score for 0.05 on a one-tail test is -1.65. That would be 1.65 standard deviations from the null hypothesis mean, 20.15. Our calculated z though was -1.92, 1.92 standard deviations from the mean. No matter how you look at it, `we must reject the null hypothesis`. The bags are too light. And so, we must reject the entire shipment. I'm guessing some of you might see this as harsh. But believe it or not, this quality control technique which is called acceptance sampling was very popular in the past and is still used in some industries today."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type I and type II errors\n",
    "In our hypothesis tests, we've always set up a null hypothesis and an alternative hypothesis. The null hypothesis typically assumes that the status quo prevails. The null hypothesis might state that the system works, it might tell us that nothing has changed in our system. Our alternative hypothesis assumes the opposite. The alternative hypothesis might tell us that the system is broken. It might tell us that things have changed. \n",
    "\n",
    "Let's use a special type of cancer screening test as an example. This fictional screening would provide a reading based on your blood. The average reading is 100. People that get a reading over 125 get a positive result. This would indicate they have cancer. If we were going to equate this to a hypothesis test, we would say the cancer screening had two hypotheses. \n",
    "\n",
    "<img src=\"images/hypothesis_testing_36.png\" alt=\"\" style=\"width: 300px;\"/>\n",
    "\n",
    "The null hypothesis would be that everything is okay. The person being tested does not have cancer. The alternative hypothesis would state that the person being tested does, in fact, have cancer. Let's say that the incidence of cancer is normally distributed. \n",
    "\n",
    "<img src=\"images/hypothesis_testing_37.png\" alt=\"\" style=\"width: 300px;\"/>\n",
    "\n",
    "So, if we were going to look at this on a normal distribution, we might say that 100 is the mean. Anything to the right of 125 would be considered a positive result for cancer. So, left of 125, we do not reject the null hypothesis, but to the right, we would reject the null hypothesis. \n",
    "\n",
    "Up until now, we've assumed that if you are beyond 125, the patient has cancer, but remember, even if 125 represented an alpha of 0.02 or 2%, it would mean that it is extremely unlikely that someone with a reading over 125 is cancer-free. It's unlikely, but with an alpha of 2%, it's not impossible. Just as political polls sometimes predict the wrong candidate to win, cancer screening tests also make mistakes. But there are two types of mistakes or errors. \n",
    "\n",
    "Let's look at this small grid. \n",
    "\n",
    "<img src=\"images/hypothesis_testing_38.png\" alt=\"\" style=\"width: 300px;\"/>\n",
    "\n",
    "At the top, we see the true state of the system. The patient does not have cancer, which agrees with our null hypothesis. And the patient has cancer, this would agree with our alternative hypothesis. Along the side, we have the two possible outcomes of the test. The test comes back positive, which means that according to the test, they have cancer. This is the equivalent of rejecting our null hypothesis. \n",
    "\n",
    "How about the second outcome for our screening test? The test comes back negative, which means that according to the test, they do not have cancer. This is the equivalent of not rejecting our null hypothesis. \n",
    "\n",
    "Now, let's look at the possible results. If we get a negative test and the patient does not have cancer, the hypothesis test worked. If we get a positive test and the patient actually has cancer, the hypothesis test worked. But how about these other two quadrants? It's possible a person might get a positive test but not actually have cancer. This is what we would call a `Type One Error`. Typically, we refer to this as a `false positive`. This is the same as a person getting a reading over 125, but not actually having cancer. If we start to see lots of Type One Errors, Perhaps our screening test is not sensitive enough. You might start to question if there are better ways of testing the null hypothesis. \n",
    "\n",
    "The opposite is also possible. A person might get a negative test, even though they do have cancer. This is what is called a `Type Two Error`. This might also be referred to as a `false negative`. This would be the same as a person getting a reading under 125, even though they have cancer. If we start to see lots of Type Two Errors, that may mean our screening test is too sensitive. Again, we may need to question how we are testing our null hypothesis. \n",
    "\n",
    "Hypothesis tests, even when they are done the right way, can be flawed. So, it's important to understand that a hypothesis test might make a mistake. And by knowing the different types of errors, Type One and Type Two, it can help you in developing and interpreting our hypothesis tests and the subsequent results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1\n",
    "```\n",
    "What is the probability that AT LEAST 36 of 50 randomly chosen people will be women, where the probability of choosing a woman is p=0.5? (Assuming the choosing of each person is independent of the others)\n",
    "This is a binomial (discrete) distribution problem.\n",
    "\n",
    "1. How many DIFFERENT combinations of choosing X women out of the 50 people are there? \n",
    "This would be C(50, X) = 50! / (X!*(50-X)!)\n",
    "\n",
    "2. What is the probability of getting ONE of these choices? \n",
    "This would be p^X*(1-p)^(50-X), which would be in our case 0.5^X*0.5^(50-X)=0.5^50\n",
    "\n",
    "So what is the probability of getting ANY such choice? \n",
    "This would be C(50, X)*0.5^50\n",
    "\n",
    "What is the probability of choosing 36 women and 14 men? C(50, 36)*0.5^50\n",
    "What is the probability of choosing 37 women and 13 men? C(50, 37)*0.5^50\n",
    ".....\n",
    "What is the probability of choosing 49 women and one man? C(50, 49)*0.5^50\n",
    "What is the probability of choosing 50 women and zero men? C(50, 50)*0.5^50=1 (because 0!=1) \n",
    "\n",
    "The sum of these, is the probability of choosing AT LEAST 36 women (choosing 36 OR MORE women) which is ~0.13%\n",
    "```\n",
    "\n",
    "### Example 2\n",
    "A smoke detector fails to beep even though there was a fire. This is an example of a:\n",
    "- true negative\n",
    "- (correct - see the errors table above) type II error\n",
    "- false positive\n",
    "- type I error\n",
    "A type II error is when we have a fire but the alarm does not sound.\n",
    "\n",
    "### Example 3\n",
    "If our significance level is 5% and our p-value is calculated as 0.016 we should _____.\n",
    "reject the null hypothesis\n",
    "If the p-value is below our stated significance level you must reject the null hypothesis.\n",
    "\n",
    "### Example 4\n",
    "What is the first step in a hypothesis test?\n",
    "The first step is to develop the hypothesis which will be tested and identify the significance level.\n",
    "\n",
    "### Example 5\n",
    "```\n",
    "You measure a value of 1.73 for a random variable with a mean of 2.20 and a standard deviation of 0.22. \n",
    "What is the z-value of the measurement?\n",
    "z = (2.20-1.73)/0.22\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
