{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "From: https://github.com/ksatola\n",
    "Version: 0.0.1\n",
    "\n",
    "TODOs\n",
    "1. https://towardsdatascience.com/explaining-feature-importance-by-example-of-a-random-forest-d9166011959e\n",
    "2. https://pypi.org/project/rfpimp/\n",
    "3. https://explained.ai/rf-importance/\n",
    "4. https://medium.com/@vigneshmadanan/linear-regression-basics-and-regularization-methods-b40359b0aea5\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features Selection\n",
    "We use feature selection to select features that are useful to the model. Irrelevant features may have a negative effect on a model. Correlated features can make coefficients in regression (or feature importance in tree models) unstable or difficult to interpret.\n",
    "\n",
    "The `curse of dimensionality` is another issue to consider. As you increase the number of dimensions of your data, it becomes more sparse. This can make it difficult to pull out a signal unless you have more data. Neighbor calculations tend to lose their usefulness as more dimensions are added.\n",
    "\n",
    "Also, training time is usually a function of the number of columns (and sometimes it is worse than linear). If you can be concise and precise with your columns, you can have a better model in less time.\n",
    "\n",
    "Statistical-based feature selection methods involve evaluating the relationship between each input variable and the target variable using statistics and selecting those input variables that have the strongest relationship with the target variable. These methods can be fast and effective, although the choice of statistical measures depends on the data type of both the input and output variables.\n",
    "\n",
    "Perhaps the simplest case of feature selection is the case where there are numerical input variables and a numerical target for regression predictive modeling. This is because the strength of the relationship between each input variable and the target can be calculated, called correlation, and compared relative to each other.\n",
    "\n",
    "Having irrelevant features in your data can decrease the accuracy of many models, especially linear algorithms like linear and logistic regression.\n",
    "\n",
    "Three **benefits of performing feature selection before modeling your data** are:\n",
    "\n",
    "- **Reduces Overfitting:** Less redundant data means less opportunity to make decisions based on noise.\n",
    "- **Improves Accuracy:** Less misleading data means modeling accuracy improves.\n",
    "- **Reduces Training Time:** Less data means that algorithms train faster.\n",
    "\n",
    "## Methods\n",
    "`Feature selection` methods are intended to reduce the number of input variables to those that are believed to be most useful to a model in order to predict the target variable. `Feature selection` is primarily focused on removing non-informative or redundant predictors from the model.\n",
    "\n",
    "One way to think about feature selection methods are in terms of `supervised` and `unsupervised` methods. The difference has to do with whether features are selected based on the target variable or not. `Unsupervised feature selection` techniques ignores the target variable, such as methods that remove redundant variables using correlation. `Supervised feature selection` techniques use the target variable, such as methods that remove irrelevant variables\n",
    "\n",
    "Another way to consider the mechanism used to select features which may be divided into `wrapper` and `filter` methods. These methods are almost always supervised and are evaluated based on the performance of a resulting model on a hold out dataset.\n",
    "\n",
    "`Wrapper feature selection` methods create many models with different subsets of input features and select those features that result in the best performing model according to a performance metric. These methods are unconcerned with the variable types, although they can be computationally expensive. [RFE](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html) is a good example of a wrapper feature selection method. \n",
    "\n",
    "`Filter feature selection` methods use statistical techniques to evaluate the relationship between each input variable and the target variable, and these scores are used as the basis to choose (filter) those input variables that will be used in the model. Filter methods evaluate the relevance of the predictors outside of the predictive models and subsequently model only the predictors that pass some criterion.\n",
    "\n",
    "Finally, there are some machine learning algorithms that perform feature selection automatically (built-in feature selection) as part of learning the model. We might refer to these techniques as `intrinsic/embedded feature selection` methods. In these cases, the model can pick and choose which representation of the data is best. This includes algorithms such as penalized regression models like `MARS`, `Lasso` and `decision trees`, including `ensembles of decision trees` like `random fores`t.\n",
    "\n",
    "Feature selection is also related to `dimensionally reduction techniques` in that both methods seek fewer input variables to a predictive model. The difference is that feature selection select features to keep or remove from the dataset, whereas dimensionality reduction create a projection of the data resulting in entirely new input features. As such, **dimensionality reduction is an alternate to feature selection rather than a type of feature selection**.\n",
    "\n",
    "**Feature Selection:** Select a subset of input features from the dataset.\n",
    "- **Unsupervised:** Do not use the target variable (e.g. remove redundant variables).\n",
    "    - Variation\n",
    "    - Correlation (multicolinearity - between independent variables)\n",
    "- **Supervised:** Use the target variable (e.g. remove irrelevant variables).\n",
    "    - **Wrapper:** Search for well-performing subsets of features.\n",
    "        - RFE\n",
    "    - **Filter:** Select subsets of features based on their relationship with the target.\n",
    "        - Correlation (between a feature and the target)\n",
    "        - Statistical Methods\n",
    "        - Feature Importance Methods\n",
    "    - **Intrinsic:** Algorithms that perform automatic feature selection during training.\n",
    "        - Linear Regression with regularization (Lasso, Ridge, ElasticNet) - https://medium.com/@vigneshmadanan/linear-regression-basics-and-regularization-methods-b40359b0aea5\n",
    "        - (Ensembles of) Decision Trees\n",
    "        - XGBoost, CATBoost\n",
    "**Dimensionality Reduction:** Project input data into a lower-dimensional feature space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection Checklist\n",
    "Isabelle Guyon and Andre Elisseeff the authors of [An Introduction to Variable and Feature Selection](https://jmlr.csail.mit.edu/papers/volume3/guyon03a/guyon03a.pdf) (PDF) provide an excellent checklist that you can use the next time you need to select data features for you predictive modeling problem.\n",
    "\n",
    "1. Do you have domain knowledge? If yes, construct a better set of ad hoc features.\n",
    "1. Are your features commensurate? If no, consider normalizing them.\n",
    "1. Do you suspect interdependence of features? If yes, expand your feature set by constructing conjunctive features or products of features, as much as your computer resources allow you.\n",
    "1. Do you need to prune the input variables (e.g. for cost, speed or data understanding reasons)? If no, construct disjunctive features or weighted sums of feature\n",
    "1. Do you need to assess features individually (e.g. to understand their influence on the system or because their number is so large that you need to do a first filtering)? If yes, use a variable ranking method; else, do it anyway to get baseline results.\n",
    "1. Do you need a predictor? If no, stop\n",
    "1. Do you suspect your data is “dirty” (has a few meaningless input patterns and/or noisy outputs or wrong class labels)? If yes, detect the outlier examples using the top ranking variables obtained in step 5 as representation; check and/or discard them.\n",
    "1. Do you know what to try first? If no, use a linear predictor. Use a forward selection method with the “probe” method as a stopping criterion or use the 0-norm embedded method for comparison, following the ranking of step 5, construct a sequence of predictors of same nature using increasing subsets of features. Can you match or improve performance with a smaller subset? If yes, try a non-linear predictor with that subset.\n",
    "1. Do you have new ideas, time, computational resources, and enough examples? If yes, compare several feature selection methods, including your new idea, correlation coefficients, backward selection and embedded methods. Use linear and non-linear predictors. Select the best approach with model selection\n",
    "1. Do you want a stable solution (to improve performance and/or understanding)? If yes, subsample your data and redo your analysis for several “bootstrap”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso Regression\n",
    "If you use lasso regression, you can set an `alpha` parameter that acts as a regularization parameter. As you increase the value, it gives less weight to features that are less important. Here we use the LassoLarsCV model to iterate over various values of alpha and track the feature coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-e4ce118e8f7c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlinear_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLassoLarsCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_n_alphas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "model = linear_model.LassoLarsCV(cv=10, max_n_alphas=10).fit(X_train, y_train)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "cm = iter(plt.get_cmap(\"tab20\")(np.linspace(0, 1, X.shape[1])))\n",
    "\n",
    "for i in range(X.shape[1]):\n",
    "    c = next(cm)\n",
    "    ax.plot(\n",
    "        model.alphas_,\n",
    "        model.coef_path_.T[:, i],\n",
    "        c=c,\n",
    "        alpha=0.8,\n",
    "        label=X.columns[i],\n",
    "    )\n",
    "    \n",
    "ax.axvline(\n",
    "    model.alpha_,\n",
    "    linestyle=\"-\",\n",
    "    c=\"k\",\n",
    "    label=\"alphaCV\",\n",
    ")\n",
    "\n",
    "plt.ylabel(\"Regression Coefficients\")\n",
    "ax.legend(X.columns, bbox_to_anchor=(1, 1))\n",
    "plt.xlabel(\"alpha\")\n",
    "plt.title(\"Regression Coefficients Progression for Lasso Paths\")\n",
    "fig.savefig(\"images/feature_selection_regression_coefs.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursive Feature Elimination\n",
    "Recursive feature elimination will remove the weakest features, then fit a model. It does this by passing in a scikit-learn model with a `.coef_` or `.feature_importances_` attribute. We will use recursive feature elimination to find the 10 most important features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "model = ensemble.RandomForestClassifier(n_estimators=100)\n",
    "rfe = RFE(model, 4)\n",
    "rfe.fit(X, y)\n",
    "agg_X.columns[rfe.support_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
