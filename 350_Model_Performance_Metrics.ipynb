{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Perfomance Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Regression Models\n",
    "    - Coefficient of Determination or R-squared\n",
    "    - Residual Sum of Squares (RSS)\n",
    "    - Mean Squared Error (MSE)\n",
    "    - Mean Absolute Error (MAE)\n",
    "    - Median Absolute Error / Median Absolute Deviation (MAD)\n",
    "    - Root mean squared error (RMSE)\n",
    "    - Explained variance score\n",
    "    \n",
    "- Classification Models\n",
    "    - Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've built a model, it's important to understand how well it works. To do so, we evaluate the model against one or more metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Np.newaxis comes in very handy when you want to explicitly convert an 1D array \n",
    "# to either a row vector or a column vector.\n",
    "\n",
    "X = (np.array(sorted(list(range(5))*20)) \n",
    "     + np.random.normal(size=100, scale=0.5))[:, np.newaxis]\n",
    "\n",
    "y = (np.array(sorted(list(range(5))*20)) \n",
    "     + np.random.normal(size=100, scale=0.25))[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.24835708],\n",
       "       [-0.06913215],\n",
       "       [ 0.32384427],\n",
       "       [ 0.76151493],\n",
       "       [-0.11707669],\n",
       "       [-0.11706848],\n",
       "       [ 0.78960641],\n",
       "       [ 0.38371736],\n",
       "       [-0.23473719],\n",
       "       [ 0.27128002]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.35384269],\n",
       "       [-0.10516133],\n",
       "       [-0.08567863],\n",
       "       [-0.20056932],\n",
       "       [-0.04032143],\n",
       "       [ 0.10101271],\n",
       "       [ 0.47154648],\n",
       "       [ 0.04364445],\n",
       "       [ 0.0643876 ],\n",
       "       [-0.01861148]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X,y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LinearRegression()\n",
    "clf.fit(X, y)\n",
    "y_pred = clf.predict(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R2 (R-squared) - Coefficient of Determination\n",
    "\n",
    "The first and most immediately useful metric to use in regression is the **R-squared**, also known as the **coefficient of determination**. \n",
    "\n",
    "<img src=\"images/metric_rsquared.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "From: [Model Fit Metrics](https://www.kaggle.com/residentmario/model-fit-metrics/)\n",
    "\n",
    "The **coefficient of determination** is a measure of how well future samples will be predicted by the model. `The best possible score is 1. A constant model which always predicts the average will recieve a score of 0.` A model which is arbitrarily worse than an averaging model will recieve a negative score (this shouldn't happen in practice obviously!).\n",
    "\n",
    "In practice, it is a \"best default\" model score: other metrics may be better to use, depending on what you are optimizing for, but the  R2  is just generally very good, and should be the first number you look at in most cases.\n",
    "\n",
    "R2 is such a popular metric that there are artificial R2 scores, designed to work in a similar way but with completely different underlying mathematics, which are defined for other non-regression operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9846558399170495"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def r2_score(y, y_pred):\n",
    "    n = len(y)\n",
    "    y_avg = (1 / n) * np.sum(y) # y bar\n",
    "    pred_error = np.sum((y - y_pred)**2)\n",
    "    avg_error = np.sum((y - y_avg)**2)\n",
    "    return 1 - pred_error / avg_error\n",
    "\n",
    "r2_score(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9846558399170495"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2_score(y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual Sum of Squares (RSS)\n",
    "\n",
    "The **residual sum of squares** is the top term in the  R2  metric (albeit adjusted by 1 to account for degrees of freedom). It takes the distance between observed and predicted values (the **residuals**), squares them, and sums them all together. Ordinary least squares regression is designed to minimize exactly this value.\n",
    "\n",
    "<img src=\"images/metric_rss.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "From: [Model Fit Metrics](https://www.kaggle.com/residentmario/model-fit-metrics/)\n",
    "\n",
    "RSS is not very interpretable on its own, because it is the sum of many (potentially very large) residuals. For this reason it is rarely used as a metric, but because it is so important to regression, it's often included in statistical fit assays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.147418578949139"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def rss_score(y, y_pred):\n",
    "    return np.sum((y - y_pred)**2)\n",
    "\n",
    "rss_score(y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Squared Error (MSE)\n",
    "\n",
    "**Mean squared error** is the interpretable version of RSS. MSE divides RSS (again adjusted be 1, to account for degrees of freedom) by the number of samples in the dataset to arrive at the average amount of squared error in the model:\n",
    "\n",
    "<img src=\"images/metric_mse.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "From: [Model Fit Metrics](https://www.kaggle.com/residentmario/model-fit-metrics/)\n",
    "\n",
    "This is easily interpretable, because it makes a lot of intrinsic sense. Ordinary least squares regression asks that we minimize quadratic error; MSE measures, on average, how much such error is left in the model. However, due to the squaring involved, it is not very robust against outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03147418578949139"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mean_squared_error(y, y_pred):\n",
    "    return (1 / len(y)) * np.sum((y - y_pred)**2)\n",
    "\n",
    "mean_squared_error(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03147418578949139"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Absolute Error (MAE)\n",
    "\n",
    "**Mean absolute error** computes the expected absolute error (or L1-norm loss). Because it involves means, not squared residuals, mean absolute error is more resistant to outliers than MSE is.\n",
    "\n",
    "<img src=\"images/metric_mae.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "From: [Model Fit Metrics](https://www.kaggle.com/residentmario/model-fit-metrics/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15371923162949003"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mean_absolute_error(y, y_pred):\n",
    "    return (1 / len(y)) * np.sum(np.abs(y - y_pred))\n",
    "    \n",
    "mean_absolute_error(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15371923162949003"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error   \n",
    "mean_absolute_error(y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Median Absolute Error / Median Absolute Deviation (MAD)\n",
    "\n",
    "**Median absolute error** is the most resistant metric to outliers that's possible using simple methods.\n",
    "\n",
    "<img src=\"images/metric_mad.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "From: [Model Fit Metrics](https://www.kaggle.com/residentmario/model-fit-metrics/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15371923162949003"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def median_absolute_error(y, y_pred):\n",
    "    return np.median(np.abs(y - y_pred))\n",
    "    \n",
    "mean_absolute_error(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15514813323997423"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import median_absolute_error\n",
    "median_absolute_error(y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Root mean squared error (RMSE)\n",
    "\n",
    "**Root mean squared error** is an error metric that's popular in the literature. It is defined as the square root of mean squared error:\n",
    "\n",
    "<img src=\"images/metric_rmse.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "From: [Model Fit Metrics](https://www.kaggle.com/residentmario/model-fit-metrics/)\n",
    "\n",
    "`RMSE is directly comparable to, and serves a similar role as, the MAE, mean absolute error`.\n",
    "\n",
    "The computational effect is that RMSE is less resistant to outliers, and thus reports a poorer-fitting model when outliers are not properly accounted for. This is considered a good thing when doing cetain things, like performing hyperparameter searches. However, `MAE is a more useful reporting statistic because MAE is interpretable, while RMSE is not`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17740965528823788"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def root_mean_squared_error(y, y_pred):\n",
    "    return np.sqrt((1 / len(y)) * np.sum((y - y_pred)**2))\n",
    "\n",
    "root_mean_squared_error(y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explained variance score\n",
    "\n",
    "The **explained variance score** is a very clever (IMO) metric which looks at the ratio between the variance of the model/truth differences and the variance of the ground truth alone:\n",
    "\n",
    "<img src=\"images/metric_evs.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "From: [Model Fit Metrics](https://www.kaggle.com/residentmario/model-fit-metrics/)\n",
    "\n",
    "Hence the moniker \"explained variance\". `The best possible score is 1 (all variance is explained)` and the score goes down from there. \n",
    "\n",
    "Variance explained does not actually mean that we have explained anything, at least in a causal sense. That is, it does not imply that we know what is going on. `It simply means that we can use one or more variables to predict things more accurately than before`.\n",
    "\n",
    "In many models, if X is correlated with Y, X can be said to `“explain”` variance in Y even though X does not really cause Y. However, in some situations the term variance explained is accurate in every sense:\n",
    "\n",
    "<img src=\"images/xcausesy1.png\" alt=\"X Causes Y\" style=\"width: 400px;\"/>\n",
    "\n",
    "From: [Two visualizations for explaining “variance explained](https://assessingpsyche.wordpress.com/2014/07/10/two-visualizations-for-explaining-variance-explained/)\n",
    "\n",
    "In the model above, the arrow means that X really is a partial cause of Y. Why does Y vary? Because of variability in X, at least in part. In this example, 80% of Y’s variance is due to X, with the remaining variance due to something else (somewhat misleadingly termed error). It is not an “error” in that something is wrong or that someone is making a mistake. It is merely that which causes our predictions of Y to be off. `Prediction error` is probably not a single variable. It it likely to be the sum total of many influences.\n",
    "\n",
    "Because X and error are uncorrelated z-scores in this example, the path coefficients are equal to the correlations with Y. Squaring the correlation coefficients yields the variance explained. The coefficients for X and error are actually the square roots of .8 and .2, respectively. Squaring the coefficients tells us that X explains 80% of the variance in Y and error explains the rest.\n",
    "\n",
    "Okay, if X predicts Y, then the variance explained is equal to the correlation coefficient squared. Unfortunately, this is merely a formula. It does not help us understand what it means. Perhaps this visualization will help:\n",
    "\n",
    "<img src=\"images/varianceexplained.gif\" alt=\"Variance explained\" style=\"width: 400px;\"/>\n",
    "\n",
    "From: [Two visualizations for explaining “variance explained](https://assessingpsyche.wordpress.com/2014/07/10/two-visualizations-for-explaining-variance-explained/)\n",
    "\n",
    "If you need to guess every value of Y but you know nothing about Y except that it has a mean of zero, then you should guess zero every time. You’ll be wrong most of the time, but pursuing other strategies will result in even larger errors. The variance of your prediction errors will be equal to the variance of Y. In the picture above, this corresponds to a regression line that passes through the mean of Y and has a slope of zero. No matter what X is, you guess that Y is zero. The squared vertical distance from Y to the line is represented by the translucent squares. The average area of the squares is the variance of Y.\n",
    "\n",
    "If you happen to know the value of X each time you need to guess what Y will be, then you can use a regression equation to make a better guess. Your prediction of Y is called Y-hat (Ŷ):\n",
    "\n",
    "<img src=\"images/vexplained2.png\" alt=\"Variance explained\" style=\"width: 400px;\"/>\n",
    "\n",
    "From: [Two visualizations for explaining “variance explained](https://assessingpsyche.wordpress.com/2014/07/10/two-visualizations-for-explaining-variance-explained/)\n",
    "\n",
    "When X and Y have the same variance, the slope of the regression line is equal to the correlation coefficient, 0.89. The distance from Ŷ (the predicted value of Y) to the actual value of Y is the prediction error. In the picture above, the variance of the prediction errors (0.2) is the average size of the squares when the slope is equal to the correlation coefficient.\n",
    "\n",
    "Thus, when X is not used to predict Y, our prediction errors have a variance of 1. When we do use X to predict Y, the average size of the prediction errors shrinks from 1 to 0.2, an 80% reduction. This is what is meant when we say that “X explains 80% of the variance in Y.” `It is the proportion by which the variance of the prediction errors shrinks`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9859076890259523"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def explained_variance_score(y, y_pred):\n",
    "    return 1 - (np.var(y - y_pred) / np.var(y))\n",
    "\n",
    "explained_variance_score(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9859076890259523"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import explained_variance_score\n",
    "explained_variance_score(y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Classification Problem\n",
    "\n",
    "Examplary questions:\n",
    "- Is this email spam?\n",
    "- Should we hire this candidate?\n",
    "- Is this air traveler secretly a terrorist?\n",
    "\n",
    "Given a set of labeled data and a predictive model, every data point lies in one of four categories:\n",
    "- `True positive` - This message is spam, and we correctly predicted spam.\n",
    "- `False positive (Type 1 error)` - This message is not spam, but we predicted spam.\n",
    "- `False negative (Type 2 error)` - This message is spam, but we predicted otherwise.\n",
    "- `True negative` - This message is not spam, and we correctly predicted not spam.\n",
    "\n",
    "**Confusion Matrix**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "                        Is Spam              Is Not Spam\n",
    "Predicted Spam          True Positive        False Positive\n",
    "Predicted Not Spam      False Negative       True Negative "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy\n",
    "Accuracy is the fraction of correct predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(tp: int, fp: int, fn: int, tn: int) -> float:\n",
    "    correct = tp + tn\n",
    "    total = tp + fp + fn + tn\n",
    "    return correct / total\n",
    "\n",
    "assert accuracy(70, 4930, 13930, 981070) == 0.98114"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision\n",
    "Precision measures how accurate our positive predictions were."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(tp: int, fp: int, fn: int, tn: int) -> float:\n",
    "    return tp / (tp + fp)\n",
    "\n",
    "assert precision(70, 4930, 13930, 981070) == 0.014"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall\n",
    "Recall measures what fraction of the positives our model identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(tp: int, fp: int, fn: int, tn: int) -> float:\n",
    "    return tp / (tp + fn)\n",
    "\n",
    "assert recall(70, 4930, 13930, 981070) == 0.005"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1 Score\n",
    "F1 Score is the harmonic mean of `precision` and `recall` and necessarily lies between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(tp: int, fp: int, fn: int, tn: int) -> float:\n",
    "    p = precision(tp, fp, fn, tn)\n",
    "    r = recall(tp, fp, fn, tn)\n",
    "    return 2 * p * r / (p * r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually the choice of a model involves a **tradeoff between `precision` and `recall`**. A model that predicts \"yes\" when it's even a little bit confident will probably have a high `recall` but a low `precision` (lots of False positives). A model that predicts \"yes\" only when it's extremely confident is likely to have a low `recall` and a high `precision` (lots of False negatives). Choosing the right threshold is a matter of finding the right tradeoff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions vs. actuals plot - see Logistic Regression for details\n",
    "predictions = [logistic(dot(beta, x_i)) for x_i in x_test]\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.scatter(predictions, y_test, marker='+')\n",
    "plt.xlabel(\"Predicted probability\")\n",
    "plt.ylabel(\"Actual outcome\")\n",
    "plt.title(\"Logistic Regression Predicted vs. Actual\")\n",
    "#plt.savefig('images/logistic3.png')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "- [Model Fit Metrics](https://www.kaggle.com/residentmario/model-fit-metrics/)\n",
    "- [np.newaxis](https://medium.com/@ian.dzindo01/what-is-numpy-newaxis-and-when-to-use-it-8cb61c7ed6ae)\n",
    "- [Two visualizations for explaining “variance explained\"](https://assessingpsyche.wordpress.com/2014/07/10/two-visualizations-for-explaining-variance-explained/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
