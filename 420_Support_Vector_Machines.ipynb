{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines (SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative approach to classification is to just look for the hyperplane that “best” separates the classes in the training data. This is the idea behind the `support vector machine`, which finds the hyperplane that maximizes the distance to the nearest point in each class.\n",
    "\n",
    "<img src=\"images/svm1.png\" alt=\"\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of `SVM` is to produce a model (based on the training data) which predicts the target values of the test data given only the test data attributes.\n",
    "\n",
    "<img src=\"images/svm2.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "<img src=\"images/svm3.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "From: [A Practical Guide to Support Vector Classification](https://www.csie.ntu.edu.tw/~cjlin/papers/guide/guide.pdf)\n",
    "\n",
    "### Data Preprocessing\n",
    "\n",
    "#### Categorical Feature\n",
    "`SVM requires that each data instance is represented as a vector of real numbers`. Hence, if there are categorical attributes, we first have to convert them into numeric data. We recommend `one-hot-encoding`.\n",
    "\n",
    "#### Scaling\n",
    "Scaling before applying `SVM` is very important. The main advantage of scaling is to avoid attributes in greater numeric ranges dominating those in smaller numeric ranges. Another advantage is to avoid numerical difficulties during the calculation. Because kernel values usually depend on the inner products of feature vectors, e.g. the linear kernel and the polynomial kernel, large attribute values might cause numerical problems. We recommend linearly scaling each attribute to the range [−1, +1] or [0, 1]. Of course we have to use the same method to scale both training and testing data.\n",
    "\n",
    "### Model Selection\n",
    "Though there are only four common kernels mentioned in Section 1, we must decide\n",
    "which one to try first. Then the penalty parameter C and kernel parameters are\n",
    "chosen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RBF Kernel\n",
    "In general, the `RBF kernel is a reasonable first choice`. This kernel nonlinearly maps samples into a higher dimensional space so it, unlike the `linear kernel`, can handle the case when the relation between class labels and attributes is nonlinear.\n",
    "\n",
    "The second reason is the number of hyperparameters which influences the complexity of model selection. The `polynomial kernel` has more hyperparameters than the `RBF kernel`.\n",
    "\n",
    "Finally, the `RBF kernel` has fewer numerical difficulties. \n",
    "\n",
    "There are some situations where the `RBF kernel` is not suitable. In particular, when the number of features is very large, one may just use the linear kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross-validation and Grid-search\n",
    "\n",
    "There are two parameters for an RBF kernel: `C` and `γ`. It is not known beforehand which C and γ are best for a given problem; consequently some kind of model selection (parameter search) must be done. The goal is to identify good (C, γ) so that the classifier can accurately predict unknown data (i.e. testing data). \n",
    "\n",
    "A common strategy is to separate the data set into two parts, of which one is considered unknown. The prediction accuracy obtained from the “unknown” set more precisely reflects the performance on classifying an independent data set. An improved version of this procedure is known as `cross-validation`.\n",
    "\n",
    "In `v-fold cross-validation`, we first divide the training set into `v` subsets of equalsize. Sequentially one subset is tested using the classifier trained on the remaining `v − 1` subsets. Thus, each instance of the whole training set is predicted once so the cross-validation accuracy is the percentage of data which are correctly classified. The `cross-validation procedure can prevent the overfitting problem`.\n",
    "\n",
    "We recommend a `grid-search` on `C` and `γ` using `cross-validation`. Various pairs of (C, γ) values are tried and the one with the best cross-validation accuracy is picked. We found that trying exponentially growing sequences of C and γ is a practical method to identify good parameters."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# For example:\n",
    "C = 2**(−5), 2**(−3), ..., 2**(15), \n",
    "γ = 2**(−15), 2**(−13), ..., 2**(3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "- [LIBSVM](https://www.csie.ntu.edu.tw/~cjlin/libsvm/)\n",
    "- [A Practical Guide to Support Vector Classification](https://www.csie.ntu.edu.tw/~cjlin/papers/guide/guide.pdf) - The `support vector machine (SVM)` is a popular classification technique. However, beginners who are not familiar with SVM often get unsatisfactory\n",
    "results since they miss some easy but significant steps. In this guide, we propose\n",
    "a simple procedure which usually gives reasonable results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
