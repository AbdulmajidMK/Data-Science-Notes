{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines (SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative approach to classification is to just look for the hyperplane that “best” separates the classes in the training data. This is the idea behind the `support vector machine`, which finds the hyperplane that maximizes the distance to the nearest point in each class.\n",
    "\n",
    "<img src=\"images/svm1.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "Sometimes there’s no hyperplane that separates the positive examples from the negative ones. However, look at what happens when we map this data set to two dimensions by sending the point x to `(x, x**2)`. Suddenly it’s possible to find a hyperplane that splits the data (see below).\n",
    "\n",
    "<img src=\"images/svm6.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "<img src=\"images/svm4.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "<img src=\"images/svm5.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "This is usually called the `kernel trick` because rather than actually mapping the points into the higher-dimensional space (which could be expensive if there are a lot of points and the mapping is complicated), we can use a “kernel” function to compute dot products in the higher-dimensional space and use those to find a hyperplane."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of `SVM` is to produce a model (based on the training data) which predicts the target values of the test data given only the test data attributes.\n",
    "\n",
    "<img src=\"images/svm2.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "<img src=\"images/svm3.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "From: [A Practical Guide to Support Vector Classification](https://www.csie.ntu.edu.tw/~cjlin/papers/guide/guide.pdf)\n",
    "\n",
    "### Data Preprocessing\n",
    "\n",
    "#### Categorical Feature\n",
    "`SVM requires that each data instance is represented as a vector of real numbers`. Hence, if there are categorical attributes, we first have to convert them into numeric data. We recommend `one-hot-encoding`.\n",
    "\n",
    "#### Scaling\n",
    "Scaling before applying `SVM` is very important. The main advantage of scaling is to avoid attributes in greater numeric ranges dominating those in smaller numeric ranges. Another advantage is to avoid numerical difficulties during the calculation. Because kernel values usually depend on the inner products of feature vectors, e.g. the linear kernel and the polynomial kernel, large attribute values might cause numerical problems. We recommend linearly scaling each attribute to the range [−1, +1] or [0, 1]. Of course we have to use the same method to scale both training and testing data.\n",
    "\n",
    "### Model Selection\n",
    "Though there are only four common kernels mentioned in Section 1, we must decide\n",
    "which one to try first. Then the penalty parameter C and kernel parameters are\n",
    "chosen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RBF Kernel\n",
    "In general, the `RBF kernel is a reasonable first choice`. This kernel nonlinearly maps samples into a higher dimensional space so it, unlike the `linear kernel`, can handle the case when the relation between class labels and attributes is nonlinear.\n",
    "\n",
    "The second reason is the number of hyperparameters which influences the complexity of model selection. The `polynomial kernel` has more hyperparameters than the `RBF kernel`.\n",
    "\n",
    "Finally, the `RBF kernel` has fewer numerical difficulties. \n",
    "\n",
    "There are some situations where the `RBF kernel` is not suitable. In particular, when the number of features is very large, one may just use the linear kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross-validation and Grid-search\n",
    "\n",
    "There are two parameters for an RBF kernel: `C` and `γ`. It is not known beforehand which C and γ are best for a given problem; consequently some kind of model selection (parameter search) must be done. The goal is to identify good (C, γ) so that the classifier can accurately predict unknown data (i.e. testing data). \n",
    "\n",
    "A common strategy is to separate the data set into two parts, of which one is considered unknown. The prediction accuracy obtained from the “unknown” set more precisely reflects the performance on classifying an independent data set. An improved version of this procedure is known as `cross-validation`.\n",
    "\n",
    "In `v-fold cross-validation`, we first divide the training set into `v` subsets of equalsize. Sequentially one subset is tested using the classifier trained on the remaining `v − 1` subsets. Thus, each instance of the whole training set is predicted once so the cross-validation accuracy is the percentage of data which are correctly classified. The `cross-validation procedure can prevent the overfitting problem`.\n",
    "\n",
    "We recommend a `grid-search` on `C` and `γ` using `cross-validation`. Various pairs of (C, γ) values are tried and the one with the best cross-validation accuracy is picked. We found that trying exponentially growing sequences of C and γ is a practical method to identify good parameters."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# For example:\n",
    "C = 2**(−5), 2**(−3), ..., 2**(15), \n",
    "γ = 2**(−15), 2**(−13), ..., 2**(3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, the `grid-search` can be easily parallelized because each (C, γ) is independent.\n",
    "\n",
    "In some situations the above proposed procedure is not good enough, so other techniques such as `feature selection` may be needed. Our experience indicates that the procedure works well for data which do not have many features. If there are thousands of attributes, there may be a need to choose a subset of them before giving the data to SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Kernel\n",
    "\n",
    "`If the number of features is large`, one may not need to map data to a higher dimensional space. That is, the nonlinear mapping does not improve the performance. Using the linear kernel is good enough, and one only searches for the parameter `C`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "- [LIBSVM](https://www.csie.ntu.edu.tw/~cjlin/libsvm/)\n",
    "- [A Practical Guide to Support Vector Classification](https://www.csie.ntu.edu.tw/~cjlin/papers/guide/guide.pdf) - The `support vector machine (SVM)` is a popular classification technique. However, beginners who are not familiar with SVM often get unsatisfactory\n",
    "results since they miss some easy but significant steps. In this guide, we propose\n",
    "a simple procedure which usually gives reasonable results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
