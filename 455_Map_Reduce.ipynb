{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map Reduce\n",
    "\n",
    "**MapReduce** is a programming model for performing parallel processing on large datasets. Although it is a powerful technique, its basics are relatively simple.\n",
    "\n",
    "Imagine we have a collection of items we’d like to process somehow. For instance, the items might be website logs, the texts of various books, image files, or anything else. A basic version of the MapReduce algorithm consists of the following steps:\n",
    "\n",
    "1. Use a `mapper function` to turn each item into zero or more key/value pairs. (Often this is called the map function, but there is already a Python function called map and we don’t need to confuse the two.)\n",
    "2. Collect together all the pairs with identical keys.\n",
    "3. Use a `reducer function` on each collection of grouped values to produce output values for the corresponding key.\n",
    "\n",
    "The primary benefit of **MapReduce** is that it allows us to distribute computations by moving the processing to the data. Imagine we want to word-count across billions of documents.\n",
    "\n",
    "Non-MapReduce approach requires the machine doing the processing to have access to every document. This means that the documents all need to either live on that machine or else be transferred to it during processing. More important, it means that the machine can process only one document at a time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Word Count\n",
    "Find the most frequent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non Map Reduce version\n",
    "from typing import List\n",
    "from collections import Counter\n",
    "\n",
    "def tokenize(document: str) -> List[str]:\n",
    "    \"\"\"Just split on whitespace\"\"\"\n",
    "    return document.split()\n",
    "\n",
    "def word_count_old(documents: List[str]):\n",
    "    \"\"\"Word count not using MapReduce\"\"\"\n",
    "    return Counter(word\n",
    "        for document in documents\n",
    "        for word in tokenize(document)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With millions of users the set of documents (status updates) is suddenly too big to fit on your computer. If you can just fit this into the MapReduce model, you can use some “big data” infrastructure.\n",
    "\n",
    "First, we need a `function that turns a document into a sequence of key/value pairs`. **We’ll want our output to be grouped by word, which means that the keys should be words**. And for each word, we’ll just emit the value 1 to indicate that this pair corresponds to one occurrence of the word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterator, Tuple\n",
    "\n",
    "def wc_mapper(document: str) -> Iterator[Tuple[str, int]]:\n",
    "    \"\"\"For each word in the document, emit (word, 1)\"\"\"\n",
    "    for word in tokenize(document):\n",
    "        yield (word, 1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skipping the “plumbing” step 2 for the moment, imagine that for some word we’ve collected a list of the corresponding counts we emitted. To produce the overall count for that word, then, we just need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable\n",
    "def wc_reducer(word: str,\n",
    "               counts: Iterable[int]) -> Iterator[Tuple[str, int]]:\n",
    "    \"\"\"Sum up the counts for a word\"\"\"\n",
    "    yield (word, sum(counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returning to step 2, we now need to collect the results from `wc_mapper` and feed them to `wc_reducer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def word_count(documents: List[str]) -> List[Tuple[str, int]]:\n",
    "    \"\"\"Count the words in the input documents using\n",
    "    “MapReduce\"\"\"\n",
    "\n",
    "    collector = defaultdict(list)  # To store grouped values\n",
    "\n",
    "    for document in documents:\n",
    "        for word, count in wc_mapper(document):\n",
    "            collector[word].append(count)\n",
    "\n",
    "    return [output\n",
    "            for word, counts in collector.items()\n",
    "            for output in wc_reducer(word, counts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\"data science\", \"big data\", \"science fiction\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('data', 2), ('science', 2), ('big', 1), ('fiction', 1)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine now that our billions of documents are scattered across 100 machines. With the right infrastructure (and glossing over some of the details), we can do the following:\n",
    "\n",
    "- Have each machine run the mapper on its documents, producing lots of key/value pairs.\n",
    "- Distribute those key/value pairs to a number of “reducing” machines, making sure that the pairs corresponding to any given key all end up on the same machine.\n",
    "- Have each reducing machine group the pairs by key and then run the reducer on each set of values.\n",
    "- Return each (key, output) pair.\n",
    "\n",
    "`What is amazing about this is that it scales horizontally`. If we double the number of machines, then (ignoring certain fixed costs of running a MapReduce system) our computation should run approximately twice as fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
