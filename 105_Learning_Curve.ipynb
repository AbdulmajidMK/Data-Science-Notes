{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every classifer applied to a dataset makes a fundamental trade-off between `bias`, the systematic error of the model (`underfit`), and `variance`, the amount of error from fitting overly well to the sample (`overfit`). Per the bias-variance tradeoff, these two sources of error are related; together they make up all of the error in the model. Hence, on a basal level, the task of a machine learner is to pick a model that minimizes these values.\n",
    "\n",
    "The best tool for finding what the bias-variance tradeoff of a model is is a **learning curve**. The x-axis on a learning curve is the number of observations provided to a model (e.g. the size of the training set). The y-axis on a learning curve is the amount of error in the model, according to some metric of your choosing. You then plot two curves on this graph: one for training scores, and one for cross-validation scores.\n",
    "\n",
    "Learning curves are great because the amount of progress the model makes as it gains more and more samples of data is a visual marker for how much bias and/or variance is inherent in the model:\n",
    "\n",
    "<img src=\"images/learningcurve1.png\" alt=\"Confirmation bias\" style=\"width: 800px;\"/>\n",
    "\n",
    "From: [Learning curves with Zillow Economics Data](https://www.kaggle.com/residentmario/learning-curves-with-zillow-economics-data/)\n",
    "\n",
    "In the first case, the model is systematically bad: it performs poorly on the metric no matter which split it is running on. This is an indication of an underfitted model, e.g. one that is not capturing the underlying pattern in the data. Note that it is up to you to determine what a \"bad\" metric score is!\n",
    "\n",
    "The second case is the best-case scenario. The model performs adequately well according to the metric, and adding more samples pushes the validation error towards the training error asymptotically (for at least part of the curve).\n",
    "\n",
    "The third case is one of high variance: the model is fitting the training set really well, but is fitting the validation set(s) poorly. This means that the model is overfitted, and needs regularization or tweaking in its hyperparameters to find a better fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "- [Learning curves with Zillow Economics Data](https://www.kaggle.com/residentmario/learning-curves-with-zillow-economics-data/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
