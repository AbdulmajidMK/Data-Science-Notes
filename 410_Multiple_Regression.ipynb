{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xxx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can improve a `single linear regression` model with additional features (independent variables):\n",
    "\n",
    "<img src=\"images/multiple_linear_regression1.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "In multiple regression the vector of parameters is usually called β.\n",
    "\n",
    "Assumptions:\n",
    "- The columns of `x` are `linearly independent` - there is no way to write any one as a weighted sum of some of the others. If not met, we could not estimate `beta`.\n",
    "- The columns of `x` are all uncorrelated with the `errors e`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 34, 2, 0]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scratch.linear_algebra import dot, Vector\n",
    "\n",
    "# pay attention to constant terms added to beta and x_i on the first position\n",
    "# beta = [alpha, beta_1, ..., beta_k]\n",
    "# x_i = [1, x_i1, ..., x_ik]\n",
    "\n",
    "def predict(x: Vector, beta: Vector) -> float:\n",
    "    '''assumes that the first element of x is 1'''\n",
    "    return dot(x, beta)\n",
    "\n",
    "# For example, x_i (independent variables vector):\n",
    "[\n",
    "    1,  # constant term\n",
    "    34, # feature 1\n",
    "    2,  # feature 2\n",
    "    0   # feature 3...\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we did in the simple linear model, we’ll choose `beta` to minimize the `sum of squared errors`. Finding an exact solution is not simple to do by hand, which means we’ll need to use `gradient descent`. The error function is almost identical to the one used for `simple linear regression` but instead of expecting parameters `alpha`, `beta` it will take a vector of arbitrary length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "# -e = alpha + beta_i * x_i - y_i\n",
    "def error(x: Vector, y: float, beta: Vector) -> float:\n",
    "    return predict(x, beta) - y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -e^2 == e^2\n",
    "def squared_error(x: Vector, y: float, beta: Vector) -> float:\n",
    "    return error(x, y, beta) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 30 = 4*1 + 4*2 + 4*3 + e\n",
    "\n",
    "x = [1, 2, 3]\n",
    "y = 30\n",
    "beta = [4, 4, 4] # so prediction = 4 + 8 + 12 = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-6"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error(x, y, beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squared_error(x, y, beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Gradient calculation (in Polish)](https://www.youtube.com/watch?v=woqa2CUxw00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The gradient (vector of partial derivatives)\n",
    "# err = (alpha + beta_i * x_i - y_i)\n",
    "def sqerror_gradient(x: Vector, y: float, beta: Vector) -> Vector:\n",
    "    err = error(x, y, beta)\n",
    "    return [2 * err * x_i for x_i in x] # x_0 = 1, so we have partial deriverative in respect to alpha = 2*err*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-12, -24, -36]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grad_a = 2 * err = -12\n",
    "# grad_b1 = 2 * err * x_1 = -12 * 2 = -24\n",
    "# grad_b2 = 2 * err * x_2 = -12 * 3 = -36\n",
    "\n",
    "sqerror_gradient(x, y, beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we are ready to find the optimal `beta` using `gradient descent`. The `least_squares_fit` function can work with any dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs: List[List[float]] = [[1.,49,4,0],[1,41,9,0],[1,40,8,0],[1,25,6,0],[1,21,1,0],[1,21,0,0],[1,19,3,0],[1,19,0,0],[1,18,9,0],[1,18,8,0],[1,16,4,0],[1,15,3,0],[1,15,0,0],[1,15,2,0],[1,15,7,0],[1,14,0,0],[1,14,1,0],[1,13,1,0],[1,13,7,0],[1,13,4,0],[1,13,2,0],[1,12,5,0],[1,12,0,0],[1,11,9,0],[1,10,9,0],[1,10,1,0],[1,10,1,0],[1,10,7,0],[1,10,9,0],[1,10,1,0],[1,10,6,0],[1,10,6,0],[1,10,8,0],[1,10,10,0],[1,10,6,0],[1,10,0,0],[1,10,5,0],[1,10,3,0],[1,10,4,0],[1,9,9,0],[1,9,9,0],[1,9,0,0],[1,9,0,0],[1,9,6,0],[1,9,10,0],[1,9,8,0],[1,9,5,0],[1,9,2,0],[1,9,9,0],[1,9,10,0],[1,9,7,0],[1,9,2,0],[1,9,0,0],[1,9,4,0],[1,9,6,0],[1,9,4,0],[1,9,7,0],[1,8,3,0],[1,8,2,0],[1,8,4,0],[1,8,9,0],[1,8,2,0],[1,8,3,0],[1,8,5,0],[1,8,8,0],[1,8,0,0],[1,8,9,0],[1,8,10,0],[1,8,5,0],[1,8,5,0],[1,7,5,0],[1,7,5,0],[1,7,0,0],[1,7,2,0],[1,7,8,0],[1,7,10,0],[1,7,5,0],[1,7,3,0],[1,7,3,0],[1,7,6,0],[1,7,7,0],[1,7,7,0],[1,7,9,0],[1,7,3,0],[1,7,8,0],[1,6,4,0],[1,6,6,0],[1,6,4,0],[1,6,9,0],[1,6,0,0],[1,6,1,0],[1,6,4,0],[1,6,1,0],[1,6,0,0],[1,6,7,0],[1,6,0,0],[1,6,8,0],[1,6,4,0],[1,6,2,1],[1,6,1,1],[1,6,3,1],[1,6,6,1],[1,6,4,1],[1,6,4,1],[1,6,1,1],[1,6,3,1],[1,6,4,1],[1,5,1,1],[1,5,9,1],[1,5,4,1],[1,5,6,1],[1,5,4,1],[1,5,4,1],[1,5,10,1],[1,5,5,1],[1,5,2,1],[1,5,4,1],[1,5,4,1],[1,5,9,1],[1,5,3,1],[1,5,10,1],[1,5,2,1],[1,5,2,1],[1,5,9,1],[1,4,8,1],[1,4,6,1],[1,4,0,1],[1,4,10,1],[1,4,5,1],[1,4,10,1],[1,4,9,1],[1,4,1,1],[1,4,4,1],[1,4,4,1],[1,4,0,1],[1,4,3,1],[1,4,1,1],[1,4,3,1],[1,4,2,1],[1,4,4,1],[1,4,4,1],[1,4,8,1],[1,4,2,1],[1,4,4,1],[1,3,2,1],[1,3,6,1],[1,3,4,1],[1,3,7,1],[1,3,4,1],[1,3,1,1],[1,3,10,1],[1,3,3,1],[1,3,4,1],[1,3,7,1],[1,3,5,1],[1,3,6,1],[1,3,1,1],[1,3,6,1],[1,3,10,1],[1,3,2,1],[1,3,4,1],[1,3,2,1],[1,3,1,1],[1,3,5,1],[1,2,4,1],[1,2,2,1],[1,2,8,1],[1,2,3,1],[1,2,1,1],[1,2,9,1],[1,2,10,1],[1,2,9,1],[1,2,4,1],[1,2,5,1],[1,2,0,1],[1,2,9,1],[1,2,9,1],[1,2,0,1],[1,2,1,1],[1,2,1,1],[1,2,4,1],[1,1,0,1],[1,1,2,1],[1,1,2,1],[1,1,5,1],[1,1,3,1],[1,1,10,1],[1,1,6,1],[1,1,0,1],[1,1,8,1],[1,1,6,1],[1,1,4,1],[1,1,9,1],[1,1,9,1],[1,1,4,1],[1,1,2,1],[1,1,9,1],[1,1,0,1],[1,1,8,1],[1,1,6,1],[1,1,1,1],[1,1,1,1],[1,1,5,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import tqdm\n",
    "from scratch.linear_algebra import vector_mean\n",
    "from scratch.gradient_descent import gradient_step\n",
    "\n",
    "def least_squares_fit(xs: List[Vector], ys: List[float], \n",
    "                      learning_rate: float = 0.001, num_steps: int = 1000,\n",
    "                     batch_size: int = 1) -> Vector:\n",
    "    '''Find the beta that minimizes the sum of squared errors\n",
    "        assuming the model y = dot(x, beta)'''\n",
    "    # Start with random guess\n",
    "    # xs[0] -> [1.0, 49, 4, 0]\n",
    "    guess = [random.random() for _ in xs[0]]\n",
    "    '''guess = \n",
    "    [0.2385977483303291,\n",
    "     0.6553070011285017,\n",
    "     0.5627055254613955,\n",
    "     0.3790588563128069]\n",
    "    '''\n",
    "    for _ in tqdm.trange(num_steps, desc=\"least squares fit\"):\n",
    "        for start in range(0, len(xs), batch_size):\n",
    "            batch_xs = xs[start:start+batch_size]\n",
    "            batch_ys = ys[start:start+batch_size]\n",
    "            #print(batch_xs)\n",
    "            #print(\"-------\")\n",
    "            #print(batch_ys)\n",
    "            #print(\"---------------\")\n",
    "            \n",
    "            # vector_mean computes the element-wise average\n",
    "            gradient = vector_mean([sqerror_gradient(x, y, guess)\n",
    "                                   for x, y in zip(batch_xs, batch_ys)])\n",
    "            '''gradient = \n",
    "            29.49471348565843\n",
    "            26.31325484669618\n",
    "            25.45433711032261\n",
    "            16.293954316745346\n",
    "            12.557753890325948\n",
    "            '''\n",
    "            \n",
    "            # moves `step_size` in the `gradient` direction from `v`\n",
    "            guess = gradient_step(guess, gradient, -learning_rate)\n",
    "            ''' guess = \n",
    "            14.138096962933052\n",
    "            -41.97845859126885\n",
    "            102.39558073656737\n",
    "            -149.395413507711\n",
    "            34.57855309030899\n",
    "            2.3215668126012536\n",
    "            4.5340761294465945\n",
    "            '''\n",
    "    return guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scratch.statistics import daily_hours_good\n",
    "from scratch.gradient_descent import gradient_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1.0, 49, 4, 0],\n",
       " [1, 41, 9, 0],\n",
       " [1, 40, 8, 0],\n",
       " [1, 25, 6, 0],\n",
       " [1, 21, 1, 0],\n",
       " [1, 21, 0, 0],\n",
       " [1, 19, 3, 0],\n",
       " [1, 19, 0, 0],\n",
       " [1, 18, 9, 0],\n",
       " [1, 18, 8, 0]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1,    # constant term\n",
    " 49,   # number of friends\n",
    " 4,    # work hours per day\n",
    " 0]    # doesn't have PhD\n",
    "inputs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.1461666666666666,\n",
       " 0.8541666666666666,\n",
       " 0.868,\n",
       " 0.6393333333333333,\n",
       " 0.7423333333333333,\n",
       " 0.9521666666666667,\n",
       " 0.8566666666666667,\n",
       " 0.6903333333333334,\n",
       " 0.5203333333333333,\n",
       " 0.5793333333333333]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily_hours_good[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "least squares fit: 100%|██████████| 100000/100000 [00:44<00:00, 2271.57it/s]\n"
     ]
    }
   ],
   "source": [
    "random.seed(0)\n",
    "beta = least_squares_fit(inputs, daily_hours_good, 0.0001, 100_000, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5085114289924235,\n",
       " 0.016220675437247006,\n",
       " -0.030859844307870216,\n",
       " 0.015234109417400873]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[0.5085114289924235,    # constant\n",
    " 0.016220675437247006,  # num friends\n",
    " -0.030859844307870216, # work hours per day\n",
    " 0.015234109417400873]  # has phd\n",
    "\n",
    "minutes = 0.51 + 0.016 friends - 0.03 work hours per day + 0.015 phd\n",
    "\n",
    "In practice, you wouldn't estimate a linear regression using gradient descent; you would get the exact coefficients using linear algebra techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R-squared\n",
    "from scratch.simple_linear_regression import total_sum_of_squares\n",
    "\n",
    "def multiple_r_squared(xs: List[Vector], ys: Vector, beta: Vector) -> float:\n",
    "    sum_of_squared_errors = sum(error(x, y, beta) ** 2 for x, y in zip(xs, ys))\n",
    "    return 1.0 - sum_of_squared_errors / total_sum_of_squares(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6799915717757894"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiple_r_squared(inputs, daily_hours_good, beta)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
