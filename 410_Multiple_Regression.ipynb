{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xxx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can improve a `single linear regression` model with additional features (independent variables):\n",
    "\n",
    "<img src=\"images/multiple_linear_regression1.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "In multiple regression the vector of parameters is usually called β.\n",
    "\n",
    "Assumptions:\n",
    "- The columns of `x` are `linearly independent` - there is no way to write any one as a weighted sum of some of the others. If not met, we could not estimate `beta`.\n",
    "- The columns of `x` are all uncorrelated with the `errors e`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 34, 2, 0]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scratch.linear_algebra import dot, Vector\n",
    "\n",
    "# pay attention to constant terms added to beta and x_i on the first position\n",
    "# beta = [alpha, beta_1, ..., beta_k]\n",
    "# x_i = [1, x_i1, ..., x_ik]\n",
    "\n",
    "def predict(x: Vector, beta: Vector) -> float:\n",
    "    '''assumes that the first element of x is 1'''\n",
    "    return dot(x, beta)\n",
    "\n",
    "# For example, x_i (independent variables vector):\n",
    "[\n",
    "    1,  # constant term\n",
    "    34, # feature 1\n",
    "    2,  # feature 2\n",
    "    0   # feature 3...\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we did in the simple linear model, we’ll choose `beta` to minimize the `sum of squared errors`. Finding an exact solution is not simple to do by hand, which means we’ll need to use `gradient descent`. The error function is almost identical to the one used for `simple linear regression` but instead of expecting parameters `alpha`, `beta` it will take a vector of arbitrary length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "# -e = alpha + beta_i * x_i - y_i\n",
    "def error(x: Vector, y: float, beta: Vector) -> float:\n",
    "    return predict(x, beta) - y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -e^2 == e^2\n",
    "def squared_error(x: Vector, y: float, beta: Vector) -> float:\n",
    "    return error(x, y, beta) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 30 = 4*1 + 4*2 + 4*3 + e\n",
    "\n",
    "x = [1, 2, 3]\n",
    "y = 30\n",
    "beta = [4, 4, 4] # so prediction = 4 + 8 + 12 = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-6"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error(x, y, beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squared_error(x, y, beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Gradient calculation (in Polish)](https://www.youtube.com/watch?v=woqa2CUxw00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The gradient (vector of partial derivatives)\n",
    "# err = (alpha + beta_i * x_i - y_i)\n",
    "def sqerror_gradient(x: Vector, y: float, beta: Vector) -> Vector:\n",
    "    err = error(x, y, beta)\n",
    "    return [2 * err * x_i for x_i in x] # x_0 = 1, so we have partial deriverative in respect to alpha = 2*err*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-12, -24, -36]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grad_a = 2 * err = -12\n",
    "# grad_b1 = 2 * err * x_1 = -12 * 2 = -24\n",
    "# grad_b2 = 2 * err * x_2 = -12 * 3 = -36\n",
    "\n",
    "sqerror_gradient(x, y, beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we are ready to find the optimal `beta` using `gradient descent`. The `least_squares_fit` function can work with any dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs: List[List[float]] = [[1.,49,4,0],[1,41,9,0],[1,40,8,0],[1,25,6,0],[1,21,1,0],[1,21,0,0],[1,19,3,0],[1,19,0,0],[1,18,9,0],[1,18,8,0],[1,16,4,0],[1,15,3,0],[1,15,0,0],[1,15,2,0],[1,15,7,0],[1,14,0,0],[1,14,1,0],[1,13,1,0],[1,13,7,0],[1,13,4,0],[1,13,2,0],[1,12,5,0],[1,12,0,0],[1,11,9,0],[1,10,9,0],[1,10,1,0],[1,10,1,0],[1,10,7,0],[1,10,9,0],[1,10,1,0],[1,10,6,0],[1,10,6,0],[1,10,8,0],[1,10,10,0],[1,10,6,0],[1,10,0,0],[1,10,5,0],[1,10,3,0],[1,10,4,0],[1,9,9,0],[1,9,9,0],[1,9,0,0],[1,9,0,0],[1,9,6,0],[1,9,10,0],[1,9,8,0],[1,9,5,0],[1,9,2,0],[1,9,9,0],[1,9,10,0],[1,9,7,0],[1,9,2,0],[1,9,0,0],[1,9,4,0],[1,9,6,0],[1,9,4,0],[1,9,7,0],[1,8,3,0],[1,8,2,0],[1,8,4,0],[1,8,9,0],[1,8,2,0],[1,8,3,0],[1,8,5,0],[1,8,8,0],[1,8,0,0],[1,8,9,0],[1,8,10,0],[1,8,5,0],[1,8,5,0],[1,7,5,0],[1,7,5,0],[1,7,0,0],[1,7,2,0],[1,7,8,0],[1,7,10,0],[1,7,5,0],[1,7,3,0],[1,7,3,0],[1,7,6,0],[1,7,7,0],[1,7,7,0],[1,7,9,0],[1,7,3,0],[1,7,8,0],[1,6,4,0],[1,6,6,0],[1,6,4,0],[1,6,9,0],[1,6,0,0],[1,6,1,0],[1,6,4,0],[1,6,1,0],[1,6,0,0],[1,6,7,0],[1,6,0,0],[1,6,8,0],[1,6,4,0],[1,6,2,1],[1,6,1,1],[1,6,3,1],[1,6,6,1],[1,6,4,1],[1,6,4,1],[1,6,1,1],[1,6,3,1],[1,6,4,1],[1,5,1,1],[1,5,9,1],[1,5,4,1],[1,5,6,1],[1,5,4,1],[1,5,4,1],[1,5,10,1],[1,5,5,1],[1,5,2,1],[1,5,4,1],[1,5,4,1],[1,5,9,1],[1,5,3,1],[1,5,10,1],[1,5,2,1],[1,5,2,1],[1,5,9,1],[1,4,8,1],[1,4,6,1],[1,4,0,1],[1,4,10,1],[1,4,5,1],[1,4,10,1],[1,4,9,1],[1,4,1,1],[1,4,4,1],[1,4,4,1],[1,4,0,1],[1,4,3,1],[1,4,1,1],[1,4,3,1],[1,4,2,1],[1,4,4,1],[1,4,4,1],[1,4,8,1],[1,4,2,1],[1,4,4,1],[1,3,2,1],[1,3,6,1],[1,3,4,1],[1,3,7,1],[1,3,4,1],[1,3,1,1],[1,3,10,1],[1,3,3,1],[1,3,4,1],[1,3,7,1],[1,3,5,1],[1,3,6,1],[1,3,1,1],[1,3,6,1],[1,3,10,1],[1,3,2,1],[1,3,4,1],[1,3,2,1],[1,3,1,1],[1,3,5,1],[1,2,4,1],[1,2,2,1],[1,2,8,1],[1,2,3,1],[1,2,1,1],[1,2,9,1],[1,2,10,1],[1,2,9,1],[1,2,4,1],[1,2,5,1],[1,2,0,1],[1,2,9,1],[1,2,9,1],[1,2,0,1],[1,2,1,1],[1,2,1,1],[1,2,4,1],[1,1,0,1],[1,1,2,1],[1,1,2,1],[1,1,5,1],[1,1,3,1],[1,1,10,1],[1,1,6,1],[1,1,0,1],[1,1,8,1],[1,1,6,1],[1,1,4,1],[1,1,9,1],[1,1,9,1],[1,1,4,1],[1,1,2,1],[1,1,9,1],[1,1,0,1],[1,1,8,1],[1,1,6,1],[1,1,1,1],[1,1,1,1],[1,1,5,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import tqdm\n",
    "from scratch.linear_algebra import vector_mean\n",
    "from scratch.gradient_descent import gradient_step\n",
    "\n",
    "def least_squares_fit(xs: List[Vector], ys: List[float], \n",
    "                      learning_rate: float = 0.001, num_steps: int = 1000,\n",
    "                     batch_size: int = 1) -> Vector:\n",
    "    '''Find the beta that minimizes the sum of squared errors\n",
    "        assuming the model y = dot(x, beta)'''\n",
    "    # Start with random guess\n",
    "    # xs[0] -> [1.0, 49, 4, 0]\n",
    "    guess = [random.random() for _ in xs[0]]\n",
    "    '''guess = \n",
    "    [0.2385977483303291,\n",
    "     0.6553070011285017,\n",
    "     0.5627055254613955,\n",
    "     0.3790588563128069]\n",
    "    '''\n",
    "    for _ in tqdm.trange(num_steps, desc=\"least squares fit\"):\n",
    "        for start in range(0, len(xs), batch_size):\n",
    "            batch_xs = xs[start:start+batch_size]\n",
    "            batch_ys = ys[start:start+batch_size]\n",
    "            #print(batch_xs)\n",
    "            #print(\"-------\")\n",
    "            #print(batch_ys)\n",
    "            #print(\"---------------\")\n",
    "            \n",
    "            # vector_mean computes the element-wise average\n",
    "            gradient = vector_mean([sqerror_gradient(x, y, guess)\n",
    "                                   for x, y in zip(batch_xs, batch_ys)])\n",
    "            '''gradient = \n",
    "            29.49471348565843\n",
    "            26.31325484669618\n",
    "            25.45433711032261\n",
    "            16.293954316745346\n",
    "            12.557753890325948\n",
    "            '''\n",
    "            \n",
    "            # moves `step_size` in the `gradient` direction from `v`\n",
    "            guess = gradient_step(guess, gradient, -learning_rate)\n",
    "            ''' guess = \n",
    "            14.138096962933052\n",
    "            -41.97845859126885\n",
    "            102.39558073656737\n",
    "            -149.395413507711\n",
    "            34.57855309030899\n",
    "            2.3215668126012536\n",
    "            4.5340761294465945\n",
    "            '''\n",
    "    return guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scratch.statistics import daily_hours_good, daily_minutes_good\n",
    "from scratch.gradient_descent import gradient_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1.0, 49, 4, 0],\n",
       " [1, 41, 9, 0],\n",
       " [1, 40, 8, 0],\n",
       " [1, 25, 6, 0],\n",
       " [1, 21, 1, 0],\n",
       " [1, 21, 0, 0],\n",
       " [1, 19, 3, 0],\n",
       " [1, 19, 0, 0],\n",
       " [1, 18, 9, 0],\n",
       " [1, 18, 8, 0]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1,    # constant term\n",
    " 49,   # number of friends\n",
    " 4,    # work hours per day\n",
    " 0]    # doesn't have PhD\n",
    "inputs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.1461666666666666,\n",
       " 0.8541666666666666,\n",
       " 0.868,\n",
       " 0.6393333333333333,\n",
       " 0.7423333333333333,\n",
       " 0.9521666666666667,\n",
       " 0.8566666666666667,\n",
       " 0.6903333333333334,\n",
       " 0.5203333333333333,\n",
       " 0.5793333333333333]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily_hours_good[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "least squares fit: 100%|██████████| 100000/100000 [00:43<00:00, 2324.16it/s]\n"
     ]
    }
   ],
   "source": [
    "random.seed(0)\n",
    "beta = least_squares_fit(inputs, daily_hours_good, 0.0001, 100_000, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5085114289924235,\n",
       " 0.016220675437247006,\n",
       " -0.030859844307870216,\n",
       " 0.015234109417400873]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[0.5085114289924235,    # constant\n",
    " 0.016220675437247006,  # num friends\n",
    " -0.030859844307870216, # work hours per day\n",
    " 0.015234109417400873]  # has phd\n",
    "\n",
    "minutes = 0.51 + 0.016 friends - 0.03 work hours per day + 0.015 phd\n",
    "\n",
    "In practice, you wouldn't estimate a linear regression using gradient descent; you would get the exact coefficients using linear algebra techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R-squared\n",
    "from scratch.simple_linear_regression import total_sum_of_squares\n",
    "\n",
    "def multiple_r_squared(xs: List[Vector], ys: Vector, beta: Vector) -> float:\n",
    "    sum_of_squared_errors = sum(error(x, y, beta) ** 2 for x, y in zip(xs, ys))\n",
    "    return 1.0 - sum_of_squared_errors / total_sum_of_squares(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6799915717757894"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiple_r_squared(inputs, daily_hours_good, beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, you’d often like to apply linear regression to data sets with large numbers of variables. This creates a couple of extra wrinkles:\n",
    "\n",
    "- First, the more variables you use, the more likely you are to overfit your model to the training set. \n",
    "- And second, the more nonzero coefficients you have, the harder it is to make sense of them. \n",
    "\n",
    "`If the goal is to explain some phenomenon, a sparse model with three factors might be more useful than a slightly better model with hundreds`.\n",
    "\n",
    "**Regularization** is an approach in which we add to the error term a penalty that gets larger as beta gets larger. We then minimize the combined error and penalty. The more importance we place on the penalty term, the more we discourage large coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression\n",
    "In `ridge regression`, we add a penalty proportional to the sum of the squares of the `beta_i`. (Except that typically we don’t penalize `beta_0`, the constant term.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpha is a *hyperparameter* controlling how harsh the penalty is\n",
    "# sometimes it's called \"lambda\" but that already means something in Python\n",
    "def ridge_penalty(beta: Vector, alpha: float) -> float:\n",
    "    return alpha * dot(beta[1:], beta[1:])\n",
    "\n",
    "def squared_error_ridge(x: Vector,\n",
    "                        y: float,\n",
    "                        beta: Vector,\n",
    "                        alpha: float) -> float:\n",
    "    \"\"\"estimate error plus ridge penalty on beta\"\"\"\n",
    "    return error(x, y, beta) ** 2 + ridge_penalty(beta, alpha)\n",
    "\n",
    "from scratch.linear_algebra import add\n",
    "\n",
    "def ridge_penalty_gradient(beta: Vector, alpha: float) -> Vector:\n",
    "    \"\"\"gradient of just the ridge penalty\"\"\"\n",
    "    return [0.] + [2 * alpha * beta_j for beta_j in beta[1:]]\n",
    "\n",
    "def sqerror_ridge_gradient(x: Vector,\n",
    "                           y: float,\n",
    "                           beta: Vector,\n",
    "                           alpha: float) -> Vector:\n",
    "    \"\"\"\n",
    "    the gradient corresponding to the ith squared error term\n",
    "    including the ridge penalty\n",
    "    \"\"\"\n",
    "    return add(sqerror_gradient(x, y, beta),\n",
    "               ridge_penalty_gradient(beta, alpha))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scratch.statistics import daily_minutes_good\n",
    "from scratch.gradient_descent import gradient_step\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "def least_squares_fit_ridge(xs: List[Vector],\n",
    "                            ys: List[float],\n",
    "                            alpha: float,\n",
    "                            learning_rate: float,\n",
    "                            num_steps: int,\n",
    "                            batch_size: int = 1) -> Vector:\n",
    "    # Start guess with mean\n",
    "    guess = [random.random() for _ in xs[0]]\n",
    "\n",
    "    for i in range(num_steps):\n",
    "        for start in range(0, len(xs), batch_size):\n",
    "            batch_xs = xs[start:start+batch_size]\n",
    "            batch_ys = ys[start:start+batch_size]\n",
    "\n",
    "            gradient = vector_mean([sqerror_ridge_gradient(x, y, guess, alpha)\n",
    "                                    for x, y in zip(batch_xs, batch_ys)])\n",
    "            guess = gradient_step(guess, gradient, -learning_rate)\n",
    "\n",
    "    return guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With alpha set to zero, there’s no penalty at all and we get the same results as before:\n",
    "\n",
    "random.seed(0)\n",
    "beta_0 = least_squares_fit_ridge(inputs, daily_minutes_good, 0.0,  # alpha\n",
    "                                 learning_rate, 5000, 25)\n",
    "# [30.51, 0.97, -1.85, 0.91]\n",
    "assert 5 < dot(beta_0[1:], beta_0[1:]) < 6\n",
    "assert 0.67 < multiple_r_squared(inputs, daily_minutes_good, beta_0) < 0.69\n",
    "\n",
    "# As we increase alpha, the goodness of fit gets worse, but the size of beta gets smaller:\n",
    "\n",
    "beta_0_1 = least_squares_fit_ridge(inputs, daily_minutes_good, 0.1,  # alpha\n",
    "                                   learning_rate, 5000, 25)\n",
    "# [30.8, 0.95, -1.83, 0.54]\n",
    "assert 4 < dot(beta_0_1[1:], beta_0_1[1:]) < 5\n",
    "assert 0.67 < multiple_r_squared(inputs, daily_minutes_good, beta_0_1) < 0.69\n",
    "\n",
    "\n",
    "beta_1 = least_squares_fit_ridge(inputs, daily_minutes_good, 1,  # alpha\n",
    "                                 learning_rate, 5000, 25)\n",
    "# [30.6, 0.90, -1.68, 0.10]\n",
    "assert 3 < dot(beta_1[1:], beta_1[1:]) < 4\n",
    "assert 0.67 < multiple_r_squared(inputs, daily_minutes_good, beta_1) < 0.69\n",
    "\n",
    "beta_10 = least_squares_fit_ridge(inputs, daily_minutes_good,10,  # alpha\n",
    "                                  learning_rate, 5000, 25)\n",
    "# [28.3, 0.67, -0.90, -0.01]\n",
    "assert 1 < dot(beta_10[1:], beta_10[1:]) < 2\n",
    "assert 0.5 < multiple_r_squared(inputs, daily_minutes_good, beta_10) < 0.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually you’d want to `rescale` your data before using this approach. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lasso_penalty(beta, alpha):\n",
    "    return alpha * sum(abs(beta_i) for beta_i in beta[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
