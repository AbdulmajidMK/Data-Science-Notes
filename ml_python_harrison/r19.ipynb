{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.experimental import (\n",
    "    enable_iterative_imputer,\n",
    ")\n",
    "from sklearn import (\n",
    "    ensemble,\n",
    "    impute,\n",
    "    model_selection,    \n",
    "    preprocessing,\n",
    "    tree,\n",
    ")\n",
    "from sklearn.base import (\n",
    "    BaseEstimator,\n",
    "    TransformerMixin,\n",
    ")\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier,\n",
    ")\n",
    "from sklearn.pipeline import Pipeline\n",
    "def tweak_titanic(df):\n",
    "    df = df.drop(\n",
    "        columns=[\n",
    "            \"name\",\n",
    "            \"ticket\",\n",
    "            \"home.dest\",\n",
    "            \"boat\",\n",
    "            \"body\",\n",
    "            \"cabin\",\n",
    "        ]\n",
    "    ).pipe(pd.get_dummies, drop_first=True)\n",
    "    return df\n",
    "\n",
    "class TitanicTransformer(\n",
    "    BaseEstimator, TransformerMixin\n",
    "):\n",
    "    def transform(self, X):\n",
    "        # assumes X is output\n",
    "        # from reading Excel file\n",
    "        X = tweak_titanic(X)\n",
    "        X = X.drop(columns=\"survived\")\n",
    "        return X\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "pipe = Pipeline(\n",
    "    [\n",
    "        (\"titan\", TitanicTransformer()),\n",
    "        (\"impute\", impute.IterativeImputer()),\n",
    "        (\n",
    "            \"std\",\n",
    "            preprocessing.StandardScaler(),\n",
    "        ),\n",
    "        (\"rf\", RandomForestClassifier()),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\admin\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7837150127226463"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    ")\n",
    "url = (\n",
    "    \"http://biostat.mc.vanderbilt.edu/\"\n",
    "    \"wiki/pub/Main/DataSets/titanic3.xls\"\n",
    ")\n",
    "df = pd.read_excel(url)\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(\n",
    "    df,\n",
    "    df.survived,\n",
    "    test_size=0.3,\n",
    "    random_state=42,\n",
    ")\n",
    "pipe.fit(X_train2, y_train2)\n",
    "pipe.score(X_test2, y_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('titan', TitanicTransformer()),\n",
       "                                       ('impute',\n",
       "                                        IterativeImputer(add_indicator=False,\n",
       "                                                         estimator=None,\n",
       "                                                         imputation_order='ascending',\n",
       "                                                         initial_strategy='mean',\n",
       "                                                         max_iter=10,\n",
       "                                                         max_value=None,\n",
       "                                                         min_value=None,\n",
       "                                                         missing_values=nan,\n",
       "                                                         n_nearest_features=None,\n",
       "                                                         random_state=None,\n",
       "                                                         sample_poster...\n",
       "                                                               min_samples_leaf=1,\n",
       "                                                               min_samples_split=2,\n",
       "                                                               min_weight_fraction_leaf=0.0,\n",
       "                                                               n_estimators=10,\n",
       "                                                               n_jobs=None,\n",
       "                                                               oob_score=False,\n",
       "                                                               random_state=None,\n",
       "                                                               verbose=0,\n",
       "                                                               warm_start=False))],\n",
       "                                verbose=False),\n",
       "             iid='warn', n_jobs=None,\n",
       "             param_grid={'rf__max_features': [0.4, 'auto'],\n",
       "                         'rf__n_estimators': [15, 200]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {\n",
    "    \"rf__max_features\": [0.4, \"auto\"],\n",
    "    \"rf__n_estimators\": [15, 200],\n",
    "}\n",
    "grid = model_selection.GridSearchCV(\n",
    "    pipe, cv=3, param_grid=params\n",
    ")\n",
    "grid.fit(df, df.survived)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7786259541984732"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_\n",
    "pipe.set_params(**grid.best_params_)\n",
    "pipe.fit(X_train2, y_train2)\n",
    "pipe.score(X_test2, y_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7673024091293321"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "metrics.roc_auc_score(\n",
    "    y_test2, pipe.predict(X_test2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import (\n",
    "    model_selection,\n",
    "    preprocessing,\n",
    ")\n",
    "b = load_boston()\n",
    "bos_X = pd.DataFrame(\n",
    "    b.data, columns=b.feature_names\n",
    ")\n",
    "bos_y = b.target\n",
    "bos_X_train, bos_X_test, bos_y_train, bos_y_test = model_selection.train_test_split(\n",
    "    bos_X,\n",
    "    bos_y,\n",
    "    test_size=0.3,\n",
    "    random_state=42,\n",
    ")\n",
    "bos_sX = preprocessing.StandardScaler().fit_transform(\n",
    "    bos_X\n",
    ")\n",
    "bos_sX_train, bos_sX_test, bos_sy_train, bos_sy_test = model_selection.train_test_split(\n",
    "    bos_sX,\n",
    "    bos_y,\n",
    "    test_size=0.3,\n",
    "    random_state=42,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7112260057484933"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "reg_pipe = Pipeline(\n",
    "    [\n",
    "        (\n",
    "            \"std\",\n",
    "            preprocessing.StandardScaler(),\n",
    "        ),\n",
    "        (\"lr\", LinearRegression()),\n",
    "    ]\n",
    ")\n",
    "reg_pipe.fit(bos_X_train, bos_y_train)\n",
    "reg_pipe.score(bos_X_test, bos_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.10834602,  0.80843998,  0.34313466,  0.81386426, -1.79804295,\n",
       "        2.913858  , -0.29893918, -2.94251148,  2.09419303, -1.44706731,\n",
       "       -2.05232232,  1.02375187, -3.88579002])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_pipe.named_steps[\"lr\"].intercept_\n",
    "reg_pipe.named_steps[\"lr\"].coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21.517444231177205"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "metrics.mean_squared_error(\n",
    "    bos_y_test, reg_pipe.predict(bos_X_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca_pipe = Pipeline(\n",
    "    [\n",
    "        (\"titan\", TitanicTransformer()),\n",
    "        (\"impute\", impute.IterativeImputer()),\n",
    "        (\n",
    "            \"std\",\n",
    "            preprocessing.StandardScaler(),\n",
    "        ),\n",
    "        (\"pca\", PCA()),\n",
    "    ]\n",
    ")\n",
    "X_pca = pca_pipe.fit_transform(df, df.survived)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.63591201,  0.39601221, -0.00210875,  0.10899408,  0.58278257,\n",
       "       -0.19349714, -0.19275661, -0.11258022])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca_pipe.named_steps[\n",
    "    \"pca\"\n",
    "].explained_variance_ratio_\n",
    "spca_pipe.named_steps[\"pca\"].components_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
