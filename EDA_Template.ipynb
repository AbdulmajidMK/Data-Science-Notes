{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration Data Analysis (EDA) and Cleaning\n",
    "\n",
    "- EDA - Data exploration analysis (ensure better information understanding, availability and accuracy)\n",
    "    - Variables Identification\n",
    "        - Naming convention consistency application\n",
    "    - Univariate Analysis\n",
    "    - Bi-variate Analysis\n",
    "    - Missing Values Handling\n",
    "    - Outliers\n",
    "- Data preprocessing, feature engineering (make data ready for ML, have a remarkable impact on the power of prediction)\n",
    "    - Variables transformation\n",
    "        - normalization\n",
    "        - standardization\n",
    "    - Variable creation\n",
    "        - character encoding\n",
    "    - Working with dates\n",
    "    - Inconsistent data entry\n",
    "- Re-assessment and iteration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "seed = 1234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a dataset\n",
    "# https://www.kaggle.com/c/titanic/data\n",
    "df_titanic = pd.read_csv(\"data/titanic_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(891, 12)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_titanic.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 891 `observations` / `cases` in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>887</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Montvila, Rev. Juozas</td>\n",
       "      <td>male</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>211536</td>\n",
       "      <td>13.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>888</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Graham, Miss. Margaret Edith</td>\n",
       "      <td>female</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>112053</td>\n",
       "      <td>30.00</td>\n",
       "      <td>B42</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>889</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Johnston, Miss. Catherine Helen \"Carrie\"</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>W./C. 6607</td>\n",
       "      <td>23.45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>890</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Behr, Mr. Karl Howell</td>\n",
       "      <td>male</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>111369</td>\n",
       "      <td>30.00</td>\n",
       "      <td>C148</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>891</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Dooley, Mr. Patrick</td>\n",
       "      <td>male</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>370376</td>\n",
       "      <td>7.75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Survived  Pclass                                      Name  \\\n",
       "886          887         0       2                     Montvila, Rev. Juozas   \n",
       "887          888         1       1              Graham, Miss. Margaret Edith   \n",
       "888          889         0       3  Johnston, Miss. Catherine Helen \"Carrie\"   \n",
       "889          890         1       1                     Behr, Mr. Karl Howell   \n",
       "890          891         0       3                       Dooley, Mr. Patrick   \n",
       "\n",
       "        Sex   Age  SibSp  Parch      Ticket   Fare Cabin Embarked  \n",
       "886    male  27.0      0      0      211536  13.00   NaN        S  \n",
       "887  female  19.0      0      0      112053  30.00   B42        S  \n",
       "888  female   NaN      1      2  W./C. 6607  23.45   NaN        S  \n",
       "889    male  26.0      0      0      111369  30.00  C148        C  \n",
       "890    male  32.0      0      0      370376   7.75   NaN        Q  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_titanic.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>523</th>\n",
       "      <td>524</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Hippach, Mrs. Louis Albert (Ida Sophia Fischer)</td>\n",
       "      <td>female</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>111361</td>\n",
       "      <td>57.9792</td>\n",
       "      <td>B18</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>778</th>\n",
       "      <td>779</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Kilgannon, Mr. Thomas J</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>36865</td>\n",
       "      <td>7.7375</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>760</th>\n",
       "      <td>761</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Garfirth, Mr. John</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>358585</td>\n",
       "      <td>14.5000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>497</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Eustis, Miss. Elizabeth Mussey</td>\n",
       "      <td>female</td>\n",
       "      <td>54.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>36947</td>\n",
       "      <td>78.2667</td>\n",
       "      <td>D20</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>583</th>\n",
       "      <td>584</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Ross, Mr. John Hugo</td>\n",
       "      <td>male</td>\n",
       "      <td>36.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13049</td>\n",
       "      <td>40.1250</td>\n",
       "      <td>A10</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Survived  Pclass  \\\n",
       "523          524         1       1   \n",
       "778          779         0       3   \n",
       "760          761         0       3   \n",
       "496          497         1       1   \n",
       "583          584         0       1   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "523  Hippach, Mrs. Louis Albert (Ida Sophia Fischer)  female  44.0      0   \n",
       "778                          Kilgannon, Mr. Thomas J    male   NaN      0   \n",
       "760                               Garfirth, Mr. John    male   NaN      0   \n",
       "496                   Eustis, Miss. Elizabeth Mussey  female  54.0      1   \n",
       "583                              Ross, Mr. John Hugo    male  36.0      0   \n",
       "\n",
       "     Parch  Ticket     Fare Cabin Embarked  \n",
       "523      1  111361  57.9792   B18        C  \n",
       "778      0   36865   7.7375   NaN        Q  \n",
       "760      0  358585  14.5000   NaN        S  \n",
       "496      0   36947  78.2667   D20        C  \n",
       "583      0   13049  40.1250   A10        C  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_titanic.sample(5, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables Identification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables types\n",
    "- **input** (predictors, independent features / variables / observations / attributes)\n",
    "- **output** (target, dependent feature / variable / attribute, ground truth)\n",
    "\n",
    "### Variables categories\n",
    "- Continuous / Discrete\n",
    "- Categorical\n",
    "    - Nominal\n",
    "    - Ordinal\n",
    "    \n",
    "qualitative and quantitative\n",
    "\n",
    "### Data types\n",
    "- Numeric\n",
    "- Textual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "- wypisac z dataset - zob. opis w Kaggle\n",
    "- wypisac i sklasyfikowac\n",
    "- porownac zbior z modyfikacjami (predykcja) do zbioru bez modyfikacji (pousuwane missing i outlier) oraz do zbioru bez zadnych zmian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate Analysis\n",
    "Explore variables individually.\n",
    "\n",
    "### Continuous Variables\n",
    "- Understand the central tendency and spread (dispersion).\n",
    "    - Central tendency\n",
    "        - Mean\n",
    "        - Median\n",
    "        - Mode\n",
    "        - Min\n",
    "        - Max\n",
    "    - Measures of dispersion\n",
    "        - Range\n",
    "        - Quartiles\n",
    "        - IQR\n",
    "        - Variance\n",
    "        - Standard deviation\n",
    "        - Skewness and Kurtosis\n",
    "- Identify missing values and outliers\n",
    "- Visualization methods\n",
    "    - Histogram\n",
    "    - BoxPlot\n",
    "    \n",
    "### Categorical Variables\n",
    "- Understand distribution of each category, get a sense of distribution of records across the categories\n",
    "    - One-way frequency or relative frequency table (count and percentage of values under a category).\n",
    "- Visualization methods\n",
    "    - Bar chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Survived</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>342</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0     count\n",
       "Survived       \n",
       "0           549\n",
       "1           342"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One-way frequency table\n",
    "pd.crosstab(index=df_titanic[\"Survived\"], columns=\"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Survived</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.616162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.383838</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0     frequency\n",
       "Survived           \n",
       "0          0.616162\n",
       "1          0.383838"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One-way relative frequency table\n",
    "pd.crosstab(index=df_titanic[\"Survived\"], columns=\"frequency\", normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bi-variate Analysis\n",
    "Find relationships between any two variables, continuous and categorical.\n",
    "\n",
    "### Continuous and Continuous\n",
    "Identify any linear or non-linear relationship (pattern) between two variables.\n",
    "\n",
    "- Visualization methods\n",
    "    - Scatter plot\n",
    "\n",
    "<img src=\"images/correlations.png\" alt=\"Correlations\" style=\"width: 600px;\"/>\n",
    "\n",
    "Spearman, Pearson correlation\n",
    "Correlation matrix\n",
    "Correlation heatmap\n",
    "\n",
    "Metrics\n",
    "- Co-variance\n",
    "- Variance\n",
    "- Correlation coefficient\n",
    "\n",
    "### Categorical and Categorical\n",
    "#### Methods\n",
    "Two-way `frequency or relative frequency tables` also known as `crosstabs` or `contingency` tables. A frequency table is just a data table that shows the counts of one or more categorical variables. Relative frequency tables show what percent of data points fit in each category.\n",
    "\n",
    "In the example below ([source](https://www.khanacademy.org/math/statistics-probability/analyzing-categorical-data/two-way-tables-for-categorical-data/a/two-way-tables-review)) there are two variables - gender and preference - this is where the two in two-way frequency table comes from. Each cell tells us the number (or frequency).\n",
    "\n",
    "<img src=\"images/two-way-frequency-table.png\" alt=\"Correlations\" style=\"width: 300px;\"/>\n",
    "\n",
    "Two-way relative frequency tables show what percent of data points fit in each category. We can use row relative frequencies or column relative frequencies, it just depends on the context of the problem.\n",
    "\n",
    "<img src=\"images/two-way-relative-frequency-table.png\" alt=\"Correlations\" style=\"width: 600px;\"/>\n",
    "\n",
    "Sometimes your percentages won't add up to 100% even though we rounded properly. This is called `round-off error`, and we don't worry about it too much.\n",
    "\n",
    "Two-way relative frequency tables are useful when there are different sample sizes in a dataset. In this example, more females were surveyed than males, so using percentages makes it easier to compare the preferences of males and females. From the relative frequencies, we can see that a large majority of males preferred dogs (78%) compared to a minority of females (41%).\n",
    "    \n",
    "    - Stacked column chart\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Sex</th>\n",
       "      <th>female</th>\n",
       "      <th>male</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>died</th>\n",
       "      <td>81</td>\n",
       "      <td>468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>survived</th>\n",
       "      <td>233</td>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Sex       female  male\n",
       "died          81   468\n",
       "survived     233   109"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Two-way table\n",
    "# Table of survival vs. sex\n",
    "survived_sex = pd.crosstab(index=df_titanic[\"Survived\"], columns=df_titanic[\"Sex\"])\n",
    "survived_sex.index= [\"died\", \"survived\"]\n",
    "survived_sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class1</th>\n",
       "      <th>class2</th>\n",
       "      <th>class3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>died</th>\n",
       "      <td>80</td>\n",
       "      <td>97</td>\n",
       "      <td>372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>survived</th>\n",
       "      <td>136</td>\n",
       "      <td>87</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          class1  class2  class3\n",
       "died          80      97     372\n",
       "survived     136      87     119"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Table of survival vs passenger class\n",
    "survived_class = pd.crosstab(index=df_titanic[\"Survived\"], \n",
    "                            columns=df_titanic[\"Pclass\"])\n",
    "\n",
    "survived_class.columns = [\"class1\",\"class2\",\"class3\"]\n",
    "survived_class.index= [\"died\",\"survived\"]\n",
    "\n",
    "survived_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class1</th>\n",
       "      <th>class2</th>\n",
       "      <th>class3</th>\n",
       "      <th>rowtotal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>died</th>\n",
       "      <td>80</td>\n",
       "      <td>97</td>\n",
       "      <td>372</td>\n",
       "      <td>549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>survived</th>\n",
       "      <td>136</td>\n",
       "      <td>87</td>\n",
       "      <td>119</td>\n",
       "      <td>342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coltotal</th>\n",
       "      <td>216</td>\n",
       "      <td>184</td>\n",
       "      <td>491</td>\n",
       "      <td>891</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          class1  class2  class3  rowtotal\n",
       "died          80      97     372       549\n",
       "survived     136      87     119       342\n",
       "coltotal     216     184     491       891"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the marginal counts (totals for each row and column) by including the argument margins=True\n",
    "# Table of survival vs passenger class\n",
    "survived_class = pd.crosstab(index=df_titanic[\"Survived\"], \n",
    "                            columns=df_titanic[\"Pclass\"],\n",
    "                             margins=True)   # Include row and column totals\n",
    "\n",
    "survived_class.columns = [\"class1\",\"class2\",\"class3\",\"rowtotal\"]\n",
    "survived_class.index= [\"died\",\"survived\",\"coltotal\"]\n",
    "\n",
    "survived_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class1</th>\n",
       "      <th>class2</th>\n",
       "      <th>class3</th>\n",
       "      <th>rowtotal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>died</th>\n",
       "      <td>0.089787</td>\n",
       "      <td>0.108866</td>\n",
       "      <td>0.417508</td>\n",
       "      <td>0.616162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>survived</th>\n",
       "      <td>0.152637</td>\n",
       "      <td>0.097643</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>0.383838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coltotal</th>\n",
       "      <td>0.242424</td>\n",
       "      <td>0.206510</td>\n",
       "      <td>0.551066</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            class1    class2    class3  rowtotal\n",
       "died      0.089787  0.108866  0.417508  0.616162\n",
       "survived  0.152637  0.097643  0.133558  0.383838\n",
       "coltotal  0.242424  0.206510  0.551066  1.000000"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "survived_class = pd.crosstab(index=df_titanic[\"Survived\"], \n",
    "                            columns=df_titanic[\"Pclass\"],\n",
    "                             margins=True, normalize=True)   # Include row and column totals\n",
    "\n",
    "survived_class.columns = [\"class1\",\"class2\",\"class3\",\"rowtotal\"]\n",
    "survived_class.index= [\"died\",\"survived\",\"coltotal\"]\n",
    "\n",
    "survived_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Stacked column chart`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacked column chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Chi_Square Test` is used to derive statistical significance of relationship between varables. It also tests whether the evidence in the sample is strong enough to generalize the relationship for a larger population. It returns probability of the computed chi-square distribution with the degree of freedom.\n",
    "- Probability of 0: both categorical variables are dependent\n",
    "- Probability of 1: independent\n",
    "- Probability less than 0.05: indicates that the relationship between the variables is significant at 95% of confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chi-Square Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other statistical measures used to analyze the power of relationship are:\n",
    "- Cramer's V for Nominal Categorical Variable\n",
    "- Mantel-Haenszed Chi-Square for ordinal categorical variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical and Continuous\n",
    "To explore relation between a categorical and continuous variables, we can draw box plots for each level of categorical variables. If levels are small in number, there is no statistical significance.\n",
    "#### Methods\n",
    "- Z-test - tests if means of two groups are statistically different from each other \n",
    "- T-test - like Z-test but for categories with less than 30 samples each\n",
    "- ANOVA - assesses if the average of more than two groups is statistically different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Values Handling\n",
    "Missing data in the training datase can reduce the power / fit of a model or can lead to a biased model as the data do not present relationships between variables correctly. Most libraries (including scikit-learn) will give you an error if you try to build a model using data with missing values.\n",
    "\n",
    "The most often reason for missing data are related to:\n",
    "- **Data collection**. Difficult and usually time consuming to correct.\n",
    "- **Data extraction**. Easy to find and corrected, mechanisms like hashing may help in ensuring that the data are extracted correctly.\n",
    "\n",
    "<img src=\"images/missing-values-handling.png\" alt=\"Correlations\" style=\"width: 600px;\"/>\n",
    "\n",
    "[source](https://towardsdatascience.com/how-to-handle-missing-data-8646b18db0d4)\n",
    "\n",
    "The missing values treatment can be as follows:\n",
    "- **Listwise or pairwise deletion**. It is important to understand that in the vast majority of cases, an important assumption to using either of these techniques is that your data is missing completely at random (MCAR). \n",
    "\n",
    "    In the `listwise deletion` we delete observations (or columns) where any of the variable is missing. It is simple method but reduces the power of model by reducing the sample size. \n",
    "    \n",
    "    `Pairwise deletion` occurs when the statistical procedure uses cases that contain some missing data. The procedure cannot include a particular variable when it has a missing value, but it can still use the case when analyzing other variables with non-missing values. Pairwise deletion allows you to use more of your data. However, each computed statistic may be based on a different subset of cases.\n",
    "\n",
    "    The choice between pairwise and listwise deletion of records is limited. The choice between these two types of deletion is not relevant when only one variable is being analyzed. In other situations, missing values may be treated as a valid category. \n",
    "    \n",
    "- **Mean / mode / median imputation**. Imputation means filling the missing values with estimated (most frequently used) ones. It consists of replacing the missing data for a given attribute quantitavely (mean, median) or qualitatively (mode). Mean imputation is one of the most ‘naive’ imputation methods because unlike more complex methods like k-nearest neighbors imputation, it does not use the information we have about an observation to estimate a value for it. The imputation can take forms of:\n",
    "\n",
    "    - `Generalized imputation`. We calculate mean, median or mode for all non missing values of a variable and replace all missing values of this variable with the result.\n",
    "    - `Similar case imputation`. We calculate mean, median or mode for similar cases only (looking at similarity of other variables of other cases without missing data for the variable in question).\n",
    "    \n",
    "\n",
    "- **Prediction model**. We create a predictive model (linear / logistic regression, tree, etc.) to estimate values that will substitute the missing data. To do this, we divide our dataset into two parts: one with no missing values for the variable (training dataset with a target variable), another with missing values for the variable (test dataset to pedict the target / missing variable). We populate the missing values with the predicted ones.\n",
    "\n",
    "- **KNN Imputation**. The missing values of a variable are imputed using the given number of variables that are most similar to the attribute whose values are missing. The similarity is defined as a distance function.\n",
    "\n",
    "    - Advantages: \n",
    "        - KNN can predict both qualitative and quantitative variables\n",
    "        - Creation of a predictive model for each variable with missing data is not required\n",
    "        - Variables with multiple missing values can be easily treated\n",
    "        - Correlation of data is taken into consideration\n",
    "    - Disadvantages:\n",
    "        - KNN is time consuming\n",
    "        - Choice of k-value is very critical "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the number of missing data points per column\n",
    "missing_values_count = nfl_data.isnull().sum()\n",
    "\n",
    "# look at the # of missing points in the first ten columns\n",
    "missing_values_count[0:10]\n",
    "\n",
    "# how many total missing values do we have?\n",
    "total_cells = np.product(nfl_data.shape)\n",
    "total_missing = missing_values_count.sum()\n",
    "\n",
    "# percent of data that is missing\n",
    "(total_missing/total_cells) * 100\n",
    "\n",
    "# remove all the rows that contain a missing value\n",
    "nfl_data.dropna()\n",
    "\n",
    "# remove all columns with at least one missing value\n",
    "columns_with_na_dropped = nfl_data.dropna(axis=1)\n",
    "columns_with_na_dropped.head()\n",
    "\n",
    "# just how much data did we lose?\n",
    "print(\"Columns in original dataset: %d \\n\" % nfl_data.shape[1])\n",
    "print(\"Columns with na's dropped: %d\" % columns_with_na_dropped.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'original_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-d407a9f6d99a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimpute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSimpleImputer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmy_imputer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSimpleImputer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdata_with_imputed_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmy_imputer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# make copy to avoid changing original data (when Imputing)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'original_data' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "my_imputer = SimpleImputer()\n",
    "data_with_imputed_values = my_imputer.fit_transform(original_data)\n",
    "\n",
    "# make copy to avoid changing original data (when Imputing)\n",
    "new_data = original_data.copy()\n",
    "\n",
    "# make new columns indicating what will be imputed\n",
    "cols_with_missing = (col for col in new_data.columns \n",
    "                                 if new_data[col].isnull().any())\n",
    "for col in cols_with_missing:\n",
    "    new_data[col + '_was_missing'] = new_data[col].isnull()\n",
    "\n",
    "# Imputation\n",
    "my_imputer = SimpleImputer()\n",
    "new_data = pd.DataFrame(my_imputer.fit_transform(new_data))\n",
    "new_data.columns = original_data.columns\n",
    "\n",
    "# example on how to use imputer: https://www.kaggle.com/dansbecker/handling-missing-values/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean imputation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "# Create an empty dataset\n",
    "df = pd.DataFrame()\n",
    "\n",
    "# Create two variables called x0 and x1. Make the first value of x1 a missing value\n",
    "df['x0'] = [0.3051,0.4949,0.6974,0.3769,0.2231,0.341,0.4436,0.5897,0.6308,0.5]\n",
    "df['x1'] = [np.nan,0.2654,0.2615,0.5846,0.4615,0.8308,0.4962,0.3269,0.5346,0.6731]\n",
    "\n",
    "# View the dataset\n",
    "df\n",
    "\n",
    "# Fit imputer\n",
    "# Create an imputer object that looks for 'Nan' values, then replaces them with the mean value of the feature by columns (axis=0)\n",
    "mean_imputer = Imputer(missing_values='NaN', strategy='mean', axis=0)\n",
    "\n",
    "# Train the imputor on the df dataset\n",
    "mean_imputer = mean_imputer.fit(df)\n",
    "\n",
    "# Apply the imputer to the df dataset\n",
    "imputed_df = mean_imputer.transform(df.values)\n",
    "\n",
    "# View the data\n",
    "imputed_df\n",
    "# Notice that 0.49273333 is the imputed value, replacing the np.NaN value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outliers\n",
    "\n",
    "An `outlier` is an observation that appears far away and diverges from an overall pattern in a sample. Outliers can be of two types: univariate and multivariate. `Univariate outliers` can be found while looking at distribution of a single variable data. `Multivariate outliers` are outstanding observations in an n-dimensional space.\n",
    "\n",
    "<img src=\"images/outlier.png\" alt=\"Correlations\" style=\"width: 300px;\"/>\n",
    "\n",
    "<img src=\"images/n-outlier.png\" alt=\"Correlations\" style=\"width: 600px;\"/>\n",
    "\n",
    "[source](https://www.analyticsvidhya.com/blog/2016/01/guide-data-exploration/)\n",
    "\n",
    "The method of dealing with outliers depend on the reason of their occurrence. Causes of outliers can be classified in two broad categories: artificial (error) / non-natural and natural:\n",
    "\n",
    "- artificial: data entry errors (human errors, experimental / sampling errors, intentional outlier), measurement errors (faulty instruments), data processing errors.\n",
    "- natural: not caused by error.\n",
    "\n",
    "Outliers can drastically change the results of the data analysis and statistical modeling. There are numerous unfavourable impacts of outliers in the data set:\n",
    "\n",
    "- They increase the error variance and reduces the power of statistical tests.\n",
    "- If the outliers are non-randomly distributed, they can decrease normality.\n",
    "- They can bias or influence estimates that may be of substantive interest.\n",
    "- They can also impact the basic assumption of Regression, ANOVA and other statistical model assumptions.\n",
    "\n",
    "<img src=\"images/outliers-impact.png\" alt=\"Correlations\" style=\"width: 600px;\"/>\n",
    "\n",
    "Most commonly used method to detect outliers is visualization (like box-plot, histogram, scatter plot). Some analysts use various thumb rules to detect outliers:\n",
    "\n",
    "- Any value, which is beyond the range of -1.5 x IQR to 1.5 x IQR.\n",
    "- Use capping methods. Any value which out of range of 5th and 95th percentile can be considered as outlier.\n",
    "- Data points, three or more standard deviation away from mean are considered outlier.\n",
    "- Outlier detection is merely a special case of the examination of data for influential data points and it also depends on the business understanding.\n",
    "- Bivariate and multivariate outliers are typically measured using either an index of influence or leverage, or distance. Popular indices such as `Mahalanobis’ distance` and `Cook’s D` are frequently used to detect outliers.\n",
    "\n",
    "Most of the ways to deal with outliers are similar to the methods of missing values like deleting observations, transforming them, binning them, treat them as a separate group, imputing values and other statistical methods:\n",
    "\n",
    "- **Deleting observations**: We delete outlier values if it is due to data entry error, data processing error or outlier observations are very small in numbers. We can also use trimming at both ends to remove outliers.\n",
    "\n",
    "- **Transforming and binning values**: Transforming variables can also eliminate outliers. `Natural log of a value` reduces the variation caused by extreme values. `Binning` is also a form of variable transformation. Decision Tree algorithm allows to deal with outliers well due to binning of variable. We can also use the process of assigning weights to different observations.\n",
    "\n",
    "<img src=\"images/log-transformation.png\" alt=\"Correlations\" style=\"width: 600px;\"/>\n",
    "\n",
    "- **Imputing**: We can use mean, median, mode to change outlier values but before that, we should analyse if it is natural outlier or artificial. If it is artificial, we can go with imputing values. We can also use statistical model to predict values of outlier observation and after that we can impute it with predicted values.\n",
    "\n",
    "- **Treat separately**: If there are significant number of outliers, we should treat them separately in the statistical model. One of the approach is to treat both groups as two different groups and build individual model for both groups and then combine the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering\n",
    "\n",
    "Feature engineering is the science (and art) of extracting more information from existing data. You are not adding any new data here, but you are actually making the data you already have more useful.\n",
    "\n",
    "### Variables transformation\n",
    "\n",
    "In data modelling, transformation refers to the replacement of a variable by a function. For instance, replacing a variable x by the square or cube root, or logarithm x is a transformation. In other words, transformation is a process that changes the distribution or relationship of a variable with others.\n",
    "\n",
    "Reasons for variable transformation\n",
    "\n",
    "- When we want to `change the scale` of a variable or `standardize the values` of a variable for better understanding. While this transformation is a must if you have data in different scales, this transformation does not change the shape of the variable distribution.\n",
    "\n",
    "    Most of the times, your dataset will contain features highly varying in magnitudes, units and range. But since, most of the machine learning algorithms use Eucledian distance between two data points in their computations, this is a problem. The features with high magnitudes will weigh in a lot more in the distance calculations than features with low magnitudes. To supress this effect, we need to bring all features to the same level of magnitudes. This can be acheived by scaling.\n",
    "\n",
    "    There are four common methods to perform Feature Scaling:\n",
    "    \n",
    "    - **Standardization**. Standardization replaces the values by their Z scores. \n",
    "    \n",
    "    <img src=\"images/standardization.png\" alt=\"Correlations\" style=\"width: 200px;\"/>\n",
    "\n",
    "    This redistributes the features with their mean μ = 0 and standard deviation σ =1. `sklearn.preprocessing.scale` helps us implementing standardisation in python.\n",
    "    \n",
    "    - **Mean Normalization**. This distribution will have values between -1 and 1 with μ=0.\n",
    "\n",
    "    <img src=\"images/mean-normalization.png\" alt=\"Correlations\" style=\"width: 200px;\"/>\n",
    "    \n",
    "    Standardisation and Mean Normalization can be used for algorithms that assumes zero centric data like `Principal Component Analysis (PCA)`.\n",
    "    \n",
    "    - **Min-Max Scaling**. This scaling brings the value between 0 and 1.\n",
    "    \n",
    "    <img src=\"images/min-max-scaling.png\" alt=\"Correlations\" style=\"width: 200px;\"/>\n",
    "    \n",
    "    - **Unit Vector**. Scaling is done considering the whole feature vecture to be of unit length.\n",
    "    \n",
    "    <img src=\"images/unit-vector.png\" alt=\"Correlations\" style=\"width: 200px;\"/>\n",
    "\n",
    "    `Min-Max Scaling` and `Unit Vector` techniques produces values of range [0,1]. When dealing with features with hard boundaries this is quite useful. For example, when dealing with image data, the colors can range from only 0 to 255.\n",
    "    \n",
    "    Scale variables if using any algorithm that computes distance or assumes normality. For example:\n",
    "\n",
    "    - `k-nearest neighbors` with an Euclidean distance measure is sensitive to magnitudes and hence should be scaled for all features to weigh in equally.\n",
    "    - Scaling is critical, while performing `Principal Component Analysis (PCA)`. PCA tries to get the features with maximum variance and the variance is high for high magnitude features. This skews the PCA towards high magnitude features.\n",
    "    - We can speed up `gradient descent` by scaling. This is because θ will descend quickly on small ranges and slowly on large ranges, and so will oscillate inefficiently down to the optimum when the variables are very uneven.\n",
    "    \n",
    "    But for below, scalling may not help:\n",
    "    \n",
    "    - `Tree based models` are not distance based models and can handle varying ranges of features. Hence, Scaling is not required while modelling trees.\n",
    "    - Algorithms like `Linear Discriminant Analysis (LDA)` or `Naive Bayes` are by design equipped to handle this and gives weights to the features accordingly. Performing a features scaling in these algorithms may not have much effect.\n",
    "\n",
    "- When we want to `transform complex non-linear relationships into linear relationships`. Existence of a linear relationship between variables is easier to comprehend compared to a non-linear or curved relation and also improves the prediction. `Log transformation` is one of the commonly used transformation technique used in these situations.\n",
    "\n",
    "- Symmetric distribution is preferred over skewed distribution as it is easier to interpret and generate inferences. Some modeling techniques requires `normal distribution` of variables. For `right skewed distribution`, we take square / cube root or logarithm of variable and for `left skewed distribution`, we take square / cube or exponential of variables.\n",
    "\n",
    "- When we want to group variables and represent their values in groups we use `binning`.\n",
    "\n",
    "There are various methods used to transform variables:\n",
    "\n",
    "- **Logarithm**: Log of a variable is a common transformation method used to change the shape of distribution of the variable. It is generally used for reducing right skewness of variables. Though, It can’t be applied to zero or negative values as well.\n",
    "- **Square / Cube root**: The square and cube root of a variable has a sound effect on variable distribution. However, it is not as significant as logarithmic transformation. Cube root has its own advantage. It can be applied to negative values including zero. Square root can be applied to positive values including zero.\n",
    "- **Binning**: It is used to categorize variables. It is performed on original values, percentile or frequency. Decision of categorization technique is based on business understanding. For example, we can categorize income in three categories, namely: High, Average and Low. We can also perform `co-variate binning` which depends on the value of more than one variables.\n",
    "\n",
    "### Variables creation\n",
    "\n",
    "Feature / Variable creation is a process to generate a new variables / features based on existing variable(s).\n",
    "\n",
    "There are various techniques to create new features:\n",
    "\n",
    "- **Creating derived variables**: This refers to creating new variables from existing variable(s) using set of functions or different methods.\n",
    "- **Creating dummy variables**: One of the most common application of dummy variable is to convert categorical variable into numerical variables (0-1 encoding, or `one-hot-encoding`). Dummy variables are also called `Indicator Variables`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference between `scaling` and `normalization` is that, in scaling, you're changing the range of your data while in normalization you're changing the shape of the distribution of your data. Scaling just changes the range of your data. Normalization is a more radical transformation. The point of normalization is to change your observations so that they can be described as a normal distribution.\n",
    "\n",
    "`Normal distribution`: Also known as the \"bell curve\", this is a specific statistical distribution where a roughly equal observations fall above and below the mean, the mean and the median are the same, and there are more observations closer to the mean. The normal distribution is also known as the Gaussian distribution.\n",
    "\n",
    "In general, you'll only want to normalize your data if you're going to be using a machine learning or statistics technique that assumes your data is normally distributed. Some examples of these include t-tests, ANOVAs, linear regression, linear discriminant analysis (LDA) and Gaussian naive Bayes. (Pro tip: any method with \"Gaussian\" in the name probably assumes normality.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale and normalize\n",
    "# modules we'll use\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# for Box-Cox Transformation\n",
    "from scipy import stats\n",
    "\n",
    "# for min_max scaling\n",
    "from mlxtend.preprocessing import minmax_scaling\n",
    "\n",
    "# plotting modules\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# read in all our data\n",
    "kickstarters_2017 = pd.read_csv(\"../input/kickstarter-projects/ks-projects-201801.csv\")\n",
    "\n",
    "# set seed for reproducibility\n",
    "np.random.seed(0)\n",
    "\n",
    "# Scaling\n",
    "# generate 1000 data points randomly drawn from an exponential distribution\n",
    "original_data = np.random.exponential(size = 1000)\n",
    "\n",
    "# mix-max scale the data between 0 and 1\n",
    "scaled_data = minmax_scaling(original_data, columns = [0])\n",
    "\n",
    "# plot both together to compare\n",
    "fig, ax=plt.subplots(1,2)\n",
    "sns.distplot(original_data, ax=ax[0])\n",
    "ax[0].set_title(\"Original Data\")\n",
    "sns.distplot(scaled_data, ax=ax[1])\n",
    "ax[1].set_title(\"Scaled data\")\n",
    "\n",
    "# Notice that the shape of the data doesn't change, but that instead of ranging from 0 to 8ish, it now ranges from 0 to 1.\n",
    "\n",
    "# Normalization\n",
    "# The method were using to normalize here is called the Box-Cox Transformation. \n",
    "# normalize the exponential data with boxcox\n",
    "normalized_data = stats.boxcox(original_data)\n",
    "\n",
    "# plot both together to compare\n",
    "fig, ax=plt.subplots(1,2)\n",
    "sns.distplot(original_data, ax=ax[0])\n",
    "ax[0].set_title(\"Original Data\")\n",
    "sns.distplot(normalized_data[0], ax=ax[1])\n",
    "ax[1].set_title(\"Normalized data\")\n",
    "\n",
    "# Notice that the shape of our data has changed. Before normalizing it was almost L-shaped. \n",
    "# But after normalizing it looks more like the outline of a bell (hence \"bell curve\").\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character encoding\n",
    "\n",
    "Character encodings are specific sets of rules for mapping from raw binary byte strings (that look like this: 0110100001101001) to characters that make up human-readable text (like \"hi\"). There are many different encodings, and if you tried to read in text with a different encoding that the one it was originally written in, you ended up with scrambled text called \"mojibake\" (said like mo-gee-bah-kay). Here's an example of mojibake:\n",
    "\n",
    "æ–‡å—åŒ–ã??\n",
    "\n",
    "You might also end up with a \"unknown\" characters. There are what gets printed when there's no mapping between a particular byte and a character in the encoding you're using to read your byte string in and they look like this:\n",
    "\n",
    "����������\n",
    "\n",
    "Character encoding mismatches are less common today than they used to be, but it's definitely still a problem. There are lots of different character encodings, but the main one you need to know is UTF-8.\n",
    "\n",
    "UTF-8 is the standard text encoding. All Python code is in UTF-8 and, ideally, all your data should be as well. It's when things aren't in UTF-8 that you run into trouble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modules we'll use\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# helpful character encoding module\n",
    "import chardet\n",
    "\n",
    "# set seed for reproducibility\n",
    "np.random.seed(0)\n",
    "\n",
    "# start with a string\n",
    "before = \"This is the euro symbol: €\"\n",
    "\n",
    "# check to see what datatype it is\n",
    "type(before)\n",
    "\n",
    "# encode it to a different encoding, replacing characters that raise errors\n",
    "after = before.encode(\"utf-8\", errors = \"replace\")\n",
    "\n",
    "# check the type\n",
    "type(after)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look at a bytes object, you'll see that it has a b in front of it, and then maybe some text after. That's because bytes are printed out as if they were characters encoded in ASCII. (ASCII is an older character encoding that doesn't really work for writing any language other than English.) Here you can see that our euro symbol has been replaced with some mojibake that looks like \"\\xe2\\x82\\xac\" when it's printed as if it were an ASCII string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at what the bytes look like\n",
    "after"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we convert our bytes back to a string with the correct encoding, we can see that our text is all there correctly, which is great! :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert it back to utf-8\n",
    "print(after.decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, when we try to use a different encoding to map our bytes into a string,, we get an error. This is because the encoding we're trying to use doesn't know what to do with the bytes we're trying to pass it. You need to tell Python the encoding that the byte string is actually supposed to be in.\n",
    "\n",
    "You can think of different encodings as different ways of recording music. You can record the same music on a CD, cassette tape or 8-track. While the music may sound more-or-less the same, you need to use the right equipment to play the music from each recording format. The correct decoder is like a cassette player or a cd player. If you try to play a cassette in a CD player, it just won't work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to decode our bytes with the ascii encoding\n",
    "print(after.decode(\"ascii\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also run into trouble if we try to use the wrong encoding to map from a string to bytes. Like I said earlier, strings are UTF-8 by default in Python 3, so if we try to treat them like they were in another encoding we'll create problems.\n",
    "\n",
    "For example, if we try to convert a string to bytes for ascii using encode(), we can ask for the bytes to be what they would be if the text was in ASCII. Since our text isn't in ASCII, though, there will be some characters it can't handle. We can automatically replace the characters that ASCII can't handle. If we do that, however, any characters not in ASCII will just be replaced with the unknown character. Then, when we convert the bytes back to a string, the character will be replaced with the unknown character. The dangerous part about this is that there's not way to tell which character it should have been. That means we may have just made our data unusable!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start with a string\n",
    "before = \"This is the euro symbol: €\"\n",
    "\n",
    "# encode it to a different encoding, replacing characters that raise errors\n",
    "after = before.encode(\"ascii\", errors = \"replace\")\n",
    "\n",
    "# convert it back to utf-8\n",
    "print(after.decode(\"ascii\"))\n",
    "\n",
    "# We've lost the original underlying byte string! It's been \n",
    "# replaced with the underlying byte string for the unknown character :("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is bad and we want to avoid doing it! It's far better to convert all our text to UTF-8 as soon as we can and keep it in that encoding. The best time to convert non UTF-8 input into UTF-8 is when you read in files, which we'll talk about next.\n",
    "\n",
    "First, however, try converting between bytes and strings with different encodings and see what happens. Notice what this does to your text. Would you want this to happen to data you were trying to analyze?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in files with encoding problems\n",
    "\n",
    "Most files you'll encounter will probably be encoded with UTF-8. This is what Python expects by default, so most of the time you won't run into problems. However, sometimes you'll get an error like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to read in a file not in UTF-8\n",
    "kickstarter_2016 = pd.read_csv(\"../input/kickstarter-projects/ks-projects-201612.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we get the same UnicodeDecodeError we got when we tried to decode UTF-8 bytes as if they were ASCII! This tells us that this file isn't actually UTF-8. We don't know what encoding it actually is though. One way to figure it out is to try and test a bunch of different character encodings and see if any of them work. A better way, though, is to use the chardet module to try and automatically guess what the right encoding is. It's not 100% guaranteed to be right, but it's usually faster than just trying to guess.\n",
    "\n",
    "I'm going to just look at the first ten thousand bytes of this file. This is usually enough for a good guess about what the encoding is and is much faster than trying to look at the whole file. (Especially with a large file this can be very slow.) Another reason to just look at the first part of the file is that we can see by looking at the error message that the first problem is the 11th character. So we probably only need to look at the first little bit of the file to figure out what's going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the first ten thousand bytes to guess the character encoding\n",
    "with open(\"../input/kickstarter-projects/ks-projects-201801.csv\", 'rb') as rawdata:\n",
    "    result = chardet.detect(rawdata.read(10000))\n",
    "\n",
    "# check what the character encoding might be\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So chardet is 73% confidence that the right encoding is \"Windows-1252\". Let's see if that's correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the file with the encoding detected by chardet\n",
    "kickstarter_2016 = pd.read_csv(\"../input/kickstarter-projects/ks-projects-201612.csv\", encoding='Windows-1252')\n",
    "\n",
    "# look at the first few lines\n",
    "kickstarter_2016.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yep, looks like chardet was right! The file reads in with no problem (although we do get a warning about datatypes) and when we look at the first few rows it seems to be be fine.\n",
    "\n",
    "What if the encoding chardet guesses isn't right? Since chardet is basically just a fancy guesser, sometimes it will guess the wrong encoding. One thing you can try is looking at more or less of the file and seeing if you get a different result and then try that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving your files with UTF-8 encoding\n",
    "\n",
    "Finally, once you've gone through all the trouble of getting your file into UTF-8, you'll probably want to keep it that way. The easiest way to do that is to save your files with UTF-8 encoding. The good news is, since UTF-8 is the standard encoding in Python, when you save a file it will be saved as UTF-8 by default:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save our file (will be saved as UTF-8 by default!)\n",
    "kickstarter_2016.to_csv(\"ks-projects-201801-utf8.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modules we'll use\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "\n",
    "# read in our data\n",
    "earthquakes = pd.read_csv(\"../input/earthquake-database/database.csv\")\n",
    "landslides = pd.read_csv(\"../input/landslide-events/catalog.csv\")\n",
    "volcanos = pd.read_csv(\"../input/volcanic-eruptions/database.csv\")\n",
    "\n",
    "# set seed for reproducibility\n",
    "np.random.seed(0)\n",
    "\n",
    "# print the first few rows of the date column\n",
    "print(landslides['date'].head())\n",
    "\n",
    "# Pandas uses the \"object\" dtype for storing various types of data types, \n",
    "# but most often when you see a column with the dtype \"object\" it will have strings in it.\n",
    "\n",
    "# check the data type of our date column\n",
    "landslides['date'].dtype\n",
    "\n",
    "# create a new column, date_parsed, with the parsed dates\n",
    "# http://strftime.org/\n",
    "landslides['date_parsed'] = pd.to_datetime(landslides['date'], format = \"%m/%d/%y\")\n",
    "\n",
    "# print the first few rows (the dtype is datetime64)\n",
    "landslides['date_parsed'].head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if I run into an error with multiple date formats? While we're specifying the date format here, sometimes you'll run into an error when there are multiple date formats in a single column. If that happens, you have have pandas try to infer what the right date format should be. You can do that like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landslides['date_parsed'] = pd.to_datetime(landslides['Date'], infer_datetime_format=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why don't you always use infer_datetime_format = True? There are two big reasons not to always have pandas guess the time format. The first is that pandas won't always been able to figure out the correct date format, especially if someone has gotten creative with data entry. The second is that it's much slower than specifying the exact format of the dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the day of the month from the date_parsed column\n",
    "day_of_month_landslides = landslides['date_parsed'].dt.day\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the biggest dangers in parsing dates is mixing up the months and days. The to_datetime() function does have very helpful error messages, but it doesn't hurt to double-check that the days of the month we've extracted make sense.\n",
    "\n",
    "To do this, let's plot a histogram of the days of the month. We expect it to have values between 1 and 31 and, since there's no reason to suppose the landslides are more common on some days of the month than others, a relatively even distribution. (With a dip on 31 because not all months have 31 days.) Let's see if that's the case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove na's\n",
    "day_of_month_landslides = day_of_month_landslides.dropna()\n",
    "\n",
    "# plot the day of the month\n",
    "sns.distplot(day_of_month_landslides, kde=False, bins=31)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inconsistent data entry\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "- xxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
