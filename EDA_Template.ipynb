{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration Data Analysis (EDA) and Cleaning\n",
    "\n",
    "- EDA - Data exploration analysis (ensure better information understanding, availability and accuracy)\n",
    "    - Variables Identification\n",
    "        - Naming convention consistency application\n",
    "        - De-duplication of observations\n",
    "        - (classification) ballanced/imbalance dataset handling\n",
    "    - Univariate Analysis\n",
    "    - Bi-variate Analysis\n",
    "    - Missing Values Handling\n",
    "    - Outliers\n",
    "- Data preprocessing, feature engineering (make data ready for ML, have a remarkable impact on the power of prediction)\n",
    "    - Variables transformation\n",
    "        - normalization\n",
    "        - standardization\n",
    "    - Variable creation\n",
    "        - character encoding\n",
    "    - Working with dates\n",
    "    - Inconsistent data entry\n",
    "- Re-assessment and iteration\n",
    "\n",
    "It is important to go into your initial data exploration with a big picture question in mind since the goal of your analysis should inform how you prepare the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful Python code snippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def template_function(arg1):\n",
    "    \"\"\"\n",
    "    Computes root mean squared error of two numpy ndarrays\n",
    "    \n",
    "    Args:\n",
    "        predicted: an ndarray of predictions\n",
    "        targets: an ndarray of target values\n",
    "    \n",
    "    Returns:\n",
    "        The root mean squared error as a float\n",
    "    \"\"\" \n",
    "    return arg1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unnamed (anonymous) functions called Lambda functions\n",
    "# The values x, y are the arguments of the function \n",
    "# and the code after the colon is the value that the function returns.\n",
    "lambda x, y: x + y\n",
    "\n",
    "# You can assign a lambda function a variable name and use it just like a normal function:\n",
    "my_function2 = lambda x, y: x + y\n",
    "my_function2(5, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "4\n",
      "9\n",
      "16\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "# map() takes a function and an iterable like a list as arguments \n",
    "# and applies the function to each item in the iterable. \n",
    "# Instead of defining a function and then passing that function to map(), \n",
    "# we can define a lambda function right in the call to map()\n",
    "my_map = map(lambda x: x**2, [1, 2, 3, 4, 5]) \n",
    "\n",
    "for item in my_map:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "# List comprehensions let you populate lists in one line of code \n",
    "# by taking the logic you would normally put a for loop \n",
    "# and moving it inside the list brackets.\n",
    "my_list2 = [number for number in range(0, 10)]\n",
    "print(my_list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 4, 6, 8]\n"
     ]
    }
   ],
   "source": [
    "my_list3 = [number for number in range(0, 10) if number % 2 == 0]\n",
    "print(my_list3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ls', 'lt', 'lu', 'ld', 'ly', 'is', 'it', 'iu', 'id', 'iy', 'fs', 'ft', 'fu', 'fd', 'fy', 'es', 'et', 'eu', 'ed', 'ey']\n"
     ]
    }
   ],
   "source": [
    "combined = [a + b  for a in \"life\" for b in \"study\"]\n",
    "print (combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'life': 4, 'is': 2, 'study': 5}\n"
     ]
    }
   ],
   "source": [
    "# Dictionary comprehension\n",
    "words = [\"life\",\"is\",\"study\"]\n",
    "word_length_dict2 = {word:len(word) for word in words}\n",
    "print(word_length_dict2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('life', 4)\n",
      "('is', 2)\n",
      "('study', 5)\n"
     ]
    }
   ],
   "source": [
    "# You can pair the items in two sequences into tuples \n",
    "# using the built in Python function zip():\n",
    "words = [\"life\",\"is\",\"study\"]\n",
    "word_lengths = [4, 2, 5]\n",
    "pairs = zip(words, word_lengths)\n",
    "\n",
    "for item in pairs:\n",
    "    print (item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(12, int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Tuples are Immutable', [1, 2, 3, 'But lists are mutable'])\n"
     ]
    }
   ],
   "source": [
    "list1 = [1,2,3]\n",
    "\n",
    "tuple1 = (\"Tuples are Immutable\", list1)\n",
    "\n",
    "tuple2 = tuple1[:]                       # Make a shallow copy\n",
    "\n",
    "list1.append(\"But lists are mutable\")\n",
    "\n",
    "print( tuple2 )                          # Print the copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Tuples are Immutable', [1, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "list1 = [1,2,3]\n",
    "\n",
    "tuple1 = (\"Tuples are Immutable\", list1)\n",
    "\n",
    "tuple2 = copy.deepcopy(tuple1)           # Make a deep copy\n",
    "\n",
    "list1.append(\"But lists are mutable\")\n",
    "\n",
    "print( tuple2 )                          # Print the copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "#import warnings\n",
    "#warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "seed = 1234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext version_information\n",
    "%version_information pandas, numpy, matplotlib, seaborn, sklearn, requests, beautifulsoup4, bokeh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/ksatola/Documents/git/Data-Science-Templates'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get current directory\n",
    "os.getcwd() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change current directory\n",
    "os.chdir('C:\\\\Users\\\\Greg\\\\Desktop\\\\intro_python10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Resources.ipynb',\n",
       " '.DS_Store',\n",
       " 'images',\n",
       " 'README.md',\n",
       " 'Data_Science_Process.ipynb',\n",
       " '.gitignore',\n",
       " 'EDA_Template.ipynb',\n",
       " '.gitattributes',\n",
       " '.ipynb_checkpoints',\n",
       " 'ML_Performance_Metrics.ipynb',\n",
       " '.git',\n",
       " 'data',\n",
       " 'Definitions.ipynb']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List all objects in a directory\n",
    "os.listdir(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read tabular data from the specified url\n",
    "url = \"http://www.basketball-reference.com/leagues/NBA_2015_totals.html\"\n",
    "BB_data = pd.read_html(url)\n",
    "# read_html() returns a list of DataFrames\n",
    "len(BB_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a dataset\n",
    "# https://www.kaggle.com/c/titanic/data\n",
    "df_titanic = pd.read_csv(\"data/titanic_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(891, 12)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_titanic.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 891 `observations` / `cases` in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>887</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Montvila, Rev. Juozas</td>\n",
       "      <td>male</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>211536</td>\n",
       "      <td>13.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>888</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Graham, Miss. Margaret Edith</td>\n",
       "      <td>female</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>112053</td>\n",
       "      <td>30.00</td>\n",
       "      <td>B42</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>889</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Johnston, Miss. Catherine Helen \"Carrie\"</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>W./C. 6607</td>\n",
       "      <td>23.45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>890</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Behr, Mr. Karl Howell</td>\n",
       "      <td>male</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>111369</td>\n",
       "      <td>30.00</td>\n",
       "      <td>C148</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>891</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Dooley, Mr. Patrick</td>\n",
       "      <td>male</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>370376</td>\n",
       "      <td>7.75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Survived  Pclass                                      Name  \\\n",
       "886          887         0       2                     Montvila, Rev. Juozas   \n",
       "887          888         1       1              Graham, Miss. Margaret Edith   \n",
       "888          889         0       3  Johnston, Miss. Catherine Helen \"Carrie\"   \n",
       "889          890         1       1                     Behr, Mr. Karl Howell   \n",
       "890          891         0       3                       Dooley, Mr. Patrick   \n",
       "\n",
       "        Sex   Age  SibSp  Parch      Ticket   Fare Cabin Embarked  \n",
       "886    male  27.0      0      0      211536  13.00   NaN        S  \n",
       "887  female  19.0      0      0      112053  30.00   B42        S  \n",
       "888  female   NaN      1      2  W./C. 6607  23.45   NaN        S  \n",
       "889    male  26.0      0      0      111369  30.00  C148        C  \n",
       "890    male  32.0      0      0      370376   7.75   NaN        Q  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_titanic.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>523</th>\n",
       "      <td>524</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Hippach, Mrs. Louis Albert (Ida Sophia Fischer)</td>\n",
       "      <td>female</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>111361</td>\n",
       "      <td>57.9792</td>\n",
       "      <td>B18</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>778</th>\n",
       "      <td>779</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Kilgannon, Mr. Thomas J</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>36865</td>\n",
       "      <td>7.7375</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>760</th>\n",
       "      <td>761</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Garfirth, Mr. John</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>358585</td>\n",
       "      <td>14.5000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>497</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Eustis, Miss. Elizabeth Mussey</td>\n",
       "      <td>female</td>\n",
       "      <td>54.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>36947</td>\n",
       "      <td>78.2667</td>\n",
       "      <td>D20</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>583</th>\n",
       "      <td>584</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Ross, Mr. John Hugo</td>\n",
       "      <td>male</td>\n",
       "      <td>36.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13049</td>\n",
       "      <td>40.1250</td>\n",
       "      <td>A10</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Survived  Pclass  \\\n",
       "523          524         1       1   \n",
       "778          779         0       3   \n",
       "760          761         0       3   \n",
       "496          497         1       1   \n",
       "583          584         0       1   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "523  Hippach, Mrs. Louis Albert (Ida Sophia Fischer)  female  44.0      0   \n",
       "778                          Kilgannon, Mr. Thomas J    male   NaN      0   \n",
       "760                               Garfirth, Mr. John    male   NaN      0   \n",
       "496                   Eustis, Miss. Elizabeth Mussey  female  54.0      1   \n",
       "583                              Ross, Mr. John Hugo    male  36.0      0   \n",
       "\n",
       "     Parch  Ticket     Fare Cabin Embarked  \n",
       "523      1  111361  57.9792   B18        C  \n",
       "778      0   36865   7.7375   NaN        Q  \n",
       "760      0  358585  14.5000   NaN        S  \n",
       "496      0   36947  78.2667   D20        C  \n",
       "583      0   13049  40.1250   A10        C  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_titanic.sample(5, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naming Convention and Consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename \"sex\" column\n",
    "table2.rename(columns={\"sex\":\"gender\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both data frames contain the column \"P_ID\" but the other columns are different. A unique identifier like an ID is usually a good key for joining two data frames together. You can combine two data frames by a common column with merge():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined1 = pd.merge(table1,       # First table\n",
    "                    table2,        # Second table\n",
    "                    how=\"inner\",   # Merge method\n",
    "                    on=\"P_ID\")     # Column(s) to join on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An inner join only merges records that appear in both columns used for the join. Inner joins ensure that we don't end up introducing missing values in our data. If you want to keep more of your data and don't mind introducing some missing values, you can use merge to perform other types of joins, such as left joins, right joins and outer joins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A left join keeps all key values in the first(left) data frame\n",
    "\n",
    "left_join = pd.merge(table1,       # First table\n",
    "                    table2,        # Second table\n",
    "                    how=\"left\",   # Merge method\n",
    "                    on=\"P_ID\")     # Column(s) to join on\n",
    "\n",
    "left_join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A right join keeps all key values in the second(right) data frame\n",
    "\n",
    "right_join = pd.merge(table1,       # First table\n",
    "                    table2,        # Second table\n",
    "                    how=\"right\",   # Merge method\n",
    "                    on=\"P_ID\")     # Column(s) to join on\n",
    "\n",
    "right_join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An outer join keeps all key values in both data frames\n",
    "\n",
    "outer_join = pd.merge(table1,      # First table\n",
    "                    table2,        # Second table\n",
    "                    how=\"outer\",   # Merge method\n",
    "                    on=\"P_ID\")     # Column(s) to join on\n",
    "\n",
    "outer_join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables Identification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables types\n",
    "- **input** (predictors, independent features / variables / observations / attributes)\n",
    "- **output** (target, dependent feature / variable / attribute, ground truth)\n",
    "\n",
    "### Variables categories\n",
    "- Continuous / Discrete\n",
    "- Categorical\n",
    "    - Nominal\n",
    "    - Ordinal\n",
    "    \n",
    "qualitative and quantitative\n",
    "\n",
    "### Data types\n",
    "- Numeric\n",
    "- Textual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "- wypisac z dataset - zob. opis w Kaggle\n",
    "- wypisac i sklasyfikowac\n",
    "- porownac zbior z modyfikacjami (predykcja) do zbioru bez modyfikacji (pousuwane missing i outlier) oraz do zbioru bez zadnych zmian\n",
    "- for normality tests for small dataset, I have observed from various literatures, if your n > 50 you should use kolmogorov-smirnov, and if n<50, should go for Shapiro-Wilk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(titanic_train[\"Name\"])[0:15]   # Check the first 15 sorted names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_train[\"Name\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_train[\"Cabin\"].unique()   # Check unique cabins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_cabin = titanic_train[\"Cabin\"].astype(str) # Convert data to str\n",
    "\n",
    "new_Cabin = np.array([cabin[0] for cabin in char_cabin]) # Take first letter\n",
    "\n",
    "new_Cabin = pd.Categorical(new_Cabin)\n",
    "\n",
    "new_Cabin .describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### De-duplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "df.duplicated()\n",
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ballance / Imbalanced Dataset Handling\n",
    "\n",
    "Imbalanced data typically refers to a problem with classification problems where the classes are not represented equally. You can have a class imbalance problem on two-class classification problems as well as multi-class classification problems. Most techniques can be used on either.\n",
    "\n",
    "Most classification data sets do not have exactly equal number of instances in each class, but a small difference often does not matter.\n",
    "\n",
    "There are problems where a class imbalance is not just common, it is expected. For example, in datasets like those that characterize fraudulent transactions are imbalanced. The vast majority of the transactions will be in the “Not-Fraud” class and a very small minority will be in the “Fraud” class.\n",
    "\n",
    "Another example is customer churn datasets, where the vast majority of customers stay with the service (the “No-Churn” class) and a small minority cancel their subscription (the “Churn” class).\n",
    "\n",
    "When there is a modest class imbalance like 4:1 in the example above it can cause problems.\n",
    "\n",
    "Methods of handling imbalanced datasets:\n",
    "\n",
    "- **Collect more data**. Collecting more data is almost always overlooked. A larger dataset might expose a different and perhaps more balanced perspective on the classes.\n",
    "- **Change Your Performance Metric**. `Accuracy` is not the metric to use when working with an imbalanced dataset. The following performance measures can give more insight into the accuracy of the model than traditional classification accuracy:\n",
    "\n",
    "    - **Confusion Matrix**: A breakdown of predictions into a table showing correct predictions (the diagonal) and the types of incorrect predictions made (what classes incorrect predictions were assigned).\n",
    "    - **Precision**: A measure of a classifiers exactness.\n",
    "    - **Recall**: A measure of a classifiers completeness.\n",
    "    - **F1 Score (or F-score)**: A weighted average of precision and recall.\n",
    "    - **Kappa (or Cohen’s kappa)**: Classification accuracy normalized by the imbalance of the classes in the data.\n",
    "    - **ROC Curves**: Like precision and recall, accuracy is divided into sensitivity and specificity and models can be chosen based on the balance thresholds of these values.\n",
    "   \n",
    "- **Resample Your Dataset**. Change the dataset that you use to build your predictive model to have more balanced data. This change is called sampling your dataset and there are two main methods that you can use to even-up the classes:\n",
    "\n",
    "    - You can add copies of instances from the under-represented class called `over-sampling` (or more formally `sampling with replacement`), or\n",
    "    - You can delete instances from the over-represented class, called `under-sampling`.\n",
    "\n",
    "    Some Rules of Thumb\n",
    "\n",
    "        - Consider testing under-sampling when you have an a lot data (tens- or hundreds of thousands of instances or more)\n",
    "        - Consider testing over-sampling when you don’t have a lot of data (tens of thousands of records or less)\n",
    "        - Consider testing random and non-random (e.g. stratified) sampling schemes.\n",
    "        - Consider testing different resampled ratios (e.g. you don’t have to target a 1:1 ratio in a binary classification problem, try other ratios)\n",
    "        \n",
    "- **Generate Synthetic Samples**. A simple way to generate synthetic samples is to randomly sample the attributes from instances in the minority class. There are systematic algorithms that you can use to generate synthetic samples. The most popular of such algorithms is called SMOTE or the Synthetic Minority Over-sampling Technique. It works by creating synthetic samples from the minor class instead of creating copies. The algorithm selects two or more similar instances (using a distance measure) and perturbing an instance one attribute at a time by a random amount within the difference to the neighboring instances.\n",
    "\n",
    "- **Try Different Algorithms**. For example, decision trees often perform well on imbalanced datasets. The splitting rules that look at the class variable used in the creation of the trees, can force both classes to be addressed.\n",
    "\n",
    "- **Penalized Models**. Penalized classification imposes an additional cost on the model for making classification mistakes on the minority class during training. These penalties can bias the model to pay more attention to the minority class. Often the handling of class penalties or weights are specialized to the learning algorithm. There are penalized versions of algorithms such as penalized-SVM and penalized-LDA. \n",
    "\n",
    "- **Different Perspective**. There are fields of study dedicated to imbalanced datasets. They have their own algorithms, measures and terminology. Two you might like to consider are `anomaly detection` and `change detection`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle highly unbalanced datasets, with a focus on resampling\n",
    "# Let's see how unbalanced the dataset is\n",
    "target_count = df_train.target.value_counts()\n",
    "print('Class 0:', target_count[0])\n",
    "print('Class 1:', target_count[1])\n",
    "print('Proportion:', round(target_count[0] / target_count[1], 2), ': 1')\n",
    "\n",
    "target_count.plot(kind='bar', title='Count (target)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the major issues that novice users fall into when dealing with unbalanced datasets relates to the metrics used to evaluate their model. Using simpler metrics like `accuracy_score` can be misleading. In a dataset with highly unbalanced classes, if the classifier always \"predicts\" the most common class without performing any analysis of the features, it will still have a high accuracy rate, obviously illusory.\n",
    "\n",
    "The `Normalized Gini Coefficient`, is arobust metric for imbalanced datasets, that ranges from approximately 0 for random guessing, to approximately 0.5 for a perfect score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A widely adopted technique for dealing with highly unbalanced datasets is called `resampling`. It consists of removing samples from the majority class (`under-sampling`) and / or adding more examples from the minority class (`over-sampling`).\n",
    "\n",
    "<img src=\"images/resampling.png\" alt=\"Correlations\" style=\"width: 800px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest implementation of over-sampling is to duplicate random records from the minority class, which can cause overfitting. In under-sampling, the simplest technique involves removing random records from the majority class, which can cause loss of information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random under and over sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class count\n",
    "count_class_0, count_class_1 = df_train.target.value_counts()\n",
    "\n",
    "# Divide by class\n",
    "df_class_0 = df_train[df_train['target'] == 0]\n",
    "df_class_1 = df_train[df_train['target'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random under-sampling\n",
    "df_class_0_under = df_class_0.sample(count_class_1)\n",
    "df_test_under = pd.concat([df_class_0_under, df_class_1], axis=0)\n",
    "\n",
    "print('Random under-sampling:')\n",
    "print(df_test_under.target.value_counts())\n",
    "\n",
    "df_test_under.target.value_counts().plot(kind='bar', title='Count (target)');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random over-sampling\n",
    "df_class_1_over = df_class_1.sample(count_class_0, replace=True)\n",
    "df_test_over = pd.concat([df_class_0, df_class_1_over], axis=0)\n",
    "\n",
    "print('Random over-sampling:')\n",
    "print(df_test_over.target.value_counts())\n",
    "\n",
    "df_test_over.target.value_counts().plot(kind='bar', title='Count (target)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A number of more sophisticated resapling techniques have been proposed in the scientific literature.\n",
    "\n",
    "For example, we can cluster the records of the majority class, and do the under-sampling by removing records from each cluster, thus seeking to preserve information. In over-sampling, instead of creating exact copies of the minority class records, we can introduce small variations into those copies, creating more diverse synthetic samples.\n",
    "\n",
    "Let's apply some of these resampling techniques, using the Python library [imbalanced-learn](https://imbalanced-learn.org/en/stable/index.html). It is compatible with scikit-learn and is part of scikit-learn-contrib projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://imbalanced-learn.org/en/stable/index.html\n",
    "import imblearn\n",
    "# For ease of visualization, let's create a small unbalanced sample dataset using the make_classification method:\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X, y = make_classification(\n",
    "    n_classes=2, class_sep=1.5, weights=[0.9, 0.1],\n",
    "    n_informative=3, n_redundant=1, flip_y=0,\n",
    "    n_features=20, n_clusters_per_class=1,\n",
    "    n_samples=100, random_state=10\n",
    ")\n",
    "\n",
    "df = pd.DataFrame(X)\n",
    "df['target'] = y\n",
    "df.target.value_counts().plot(kind='bar', title='Count (target)');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will also create a 2-dimensional plot function, plot_2d_space, to see the data distribution:\n",
    "def plot_2d_space(X, y, label='Classes'):   \n",
    "    colors = ['#1F77B4', '#FF7F0E']\n",
    "    markers = ['o', 's']\n",
    "    for l, c, m in zip(np.unique(y), colors, markers):\n",
    "        plt.scatter(\n",
    "            X[y==l, 0],\n",
    "            X[y==l, 1],\n",
    "            c=c, label=l, marker=m\n",
    "        )\n",
    "    plt.title(label)\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because the dataset has many dimensions (features) and our graphs will be 2D, \n",
    "# we will reduce the size of the dataset using Principal Component Analysis (PCA):\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X = pca.fit_transform(X)\n",
    "\n",
    "plot_2d_space(X, y, 'Imbalanced dataset (2 PCA components)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random under-sampling and over-sampling with imbalanced-learn\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "rus = RandomUnderSampler(return_indices=True)\n",
    "X_rus, y_rus, id_rus = rus.fit_sample(X, y)\n",
    "\n",
    "print('Removed indexes:', id_rus)\n",
    "\n",
    "plot_2d_space(X_rus, y_rus, 'Random under-sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random over-sampling and over-sampling with imbalanced-learn\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "ros = RandomOverSampler()\n",
    "X_ros, y_ros = ros.fit_sample(X, y)\n",
    "\n",
    "print(X_ros.shape[0] - X.shape[0], 'new random picked points')\n",
    "\n",
    "plot_2d_space(X_ros, y_ros, 'Random over-sampling')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Under-sampling: Tomek links\n",
    "Tomek links are pairs of very close instances, but of opposite classes. Removing the instances of the majority class of each pair increases the space between the two classes, facilitating the classification process.\n",
    "\n",
    "<img src=\"images/resampling-tomeklinks.png\" alt=\"Correlations\" style=\"width: 800px;\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Under-sampling: Tomek links\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "\n",
    "tl = TomekLinks(return_indices=True, ratio='majority')\n",
    "X_tl, y_tl, id_tl = tl.fit_sample(X, y)\n",
    "\n",
    "print('Removed indexes:', id_tl)\n",
    "\n",
    "plot_2d_space(X_tl, y_tl, 'Tomek links under-sampling')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Under-sampling: Cluster Centroids\n",
    "\n",
    "This technique performs under-sampling by generating centroids based on clustering methods. The data will be previously grouped by similarity, in order to preserve information.\n",
    "\n",
    "In this example we will pass the {0: 10} dict for the parameter ratio, to preserve 10 elements from the majority class (0), and all minority class (1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import ClusterCentroids\n",
    "\n",
    "cc = ClusterCentroids(ratio={0: 10})\n",
    "X_cc, y_cc = cc.fit_sample(X, y)\n",
    "\n",
    "plot_2d_space(X_cc, y_cc, 'Cluster Centroids under-sampling')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Over-sampling: SMOTE\n",
    "\n",
    "SMOTE (Synthetic Minority Oversampling TEchnique) consists of synthesizing elements for the minority class, based on those that already exist. It works randomly picingk a point from the minority class and computing the k-nearest neighbors for this point. The synthetic points are added between the chosen point and its neighbors.\n",
    "\n",
    "<img src=\"images/resampling-smote.png\" alt=\"Correlations\" style=\"width: 800px;\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote = SMOTE(ratio='minority')\n",
    "X_sm, y_sm = smote.fit_sample(X, y)\n",
    "\n",
    "plot_2d_space(X_sm, y_sm, 'SMOTE over-sampling')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Over-sampling followed by under-sampling\n",
    "\n",
    "Now, we will do a combination of over-sampling and under-sampling, using the SMOTE and Tomek links techniques:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "smt = SMOTETomek(ratio='auto')\n",
    "X_smt, y_smt = smt.fit_sample(X, y)\n",
    "\n",
    "plot_2d_space(X_smt, y_smt, 'SMOTE + Tomek links')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate Analysis\n",
    "Explore variables individually. In order to understand our data, we can look at each variable and try to understand their meaning and relevance to this problem.\n",
    "\n",
    "### Continuous Variables\n",
    "- Understand the central tendency and spread (dispersion).\n",
    "    - Central tendency\n",
    "        - Mean\n",
    "        - Median\n",
    "        - Mode\n",
    "        - Min\n",
    "        - Max\n",
    "    - Measures of dispersion\n",
    "        - Range\n",
    "        - Quartiles\n",
    "        - IQR\n",
    "        - Variance\n",
    "        - Standard deviation\n",
    "        - Skewness and Kurtosis\n",
    "- Identify missing values and outliers\n",
    "- Visualization methods\n",
    "    - Histogram\n",
    "    - BoxPlot\n",
    "    \n",
    "### Categorical Variables\n",
    "- Understand distribution of each category, get a sense of distribution of records across the categories\n",
    "    - One-way frequency or relative frequency table (count and percentage of values under a category).\n",
    "- Visualization methods\n",
    "    - Bar chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#histogram\n",
    "sns.distplot(df_train['SalePrice']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#skewness and kurtosis\n",
    "print(\"Skewness: %f\" % df_train['SalePrice'].skew())\n",
    "print(\"Kurtosis: %f\" % df_train['SalePrice'].kurt())\n",
    "# The kurtosis of any univariate normal distribution is 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Survived</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>342</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0     count\n",
       "Survived       \n",
       "0           549\n",
       "1           342"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One-way frequency table\n",
    "pd.crosstab(index=df_titanic[\"Survived\"], columns=\"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Survived</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.616162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.383838</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0     frequency\n",
       "Survived           \n",
       "0          0.616162\n",
       "1          0.383838"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One-way relative frequency table\n",
    "pd.crosstab(index=df_titanic[\"Survived\"], columns=\"frequency\", normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_Pclass = pd.Categorical(titanic_train[\"Pclass\"],\n",
    "                           ordered=True)\n",
    "\n",
    "new_Pclass = new_Pclass.rename_categories([\"Class1\",\"Class2\",\"Class3\"])     \n",
    "\n",
    "new_Pclass.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even these simple one-way tables give us some useful insight: we immediately get a sense of distribution of records across the categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tab = pd.crosstab(index=titanic_train[\"Survived\"],  # Make a crosstab\n",
    "                              columns=\"count\")      # Name the count column\n",
    "\n",
    "my_tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(index=titanic_train[\"Pclass\"],  # Make a crosstab\n",
    "                      columns=\"count\")      # Name the count column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(index=titanic_train[\"Sex\"],     # Make a crosstab\n",
    "                      columns=\"count\")      # Name the count column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cabin_tab = pd.crosstab(index=titanic_train[\"Cabin\"],  # Make a crosstab\n",
    "                        columns=\"count\")               # Name the count column\n",
    "\n",
    "cabin_tab "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most useful aspects of frequency tables is that they allow you to extract the proportion of the data that belongs to each category. With a one-way table, you can do this by dividing each table value by the total number of records in the table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cabin_tab/cabin_tab.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bi-variate Analysis\n",
    "Find relationships between any two variables, continuous and categorical.\n",
    "\n",
    "### Continuous and Continuous\n",
    "Identify any linear or non-linear relationship (pattern) between two variables.\n",
    "\n",
    "- Visualization methods\n",
    "    - Scatter plot\n",
    "\n",
    "<img src=\"images/correlations.png\" alt=\"Correlations\" style=\"width: 600px;\"/>\n",
    "\n",
    "Spearman, Pearson correlation\n",
    "Correlation matrix\n",
    "Correlation heatmap\n",
    "\n",
    "Metrics\n",
    "- Co-variance\n",
    "- Variance\n",
    "- Correlation coefficient\n",
    "\n",
    "### Categorical and Categorical\n",
    "#### Methods\n",
    "Two-way `frequency or relative frequency tables` also known as `crosstabs` or `contingency` tables. A frequency table is just a data table that shows the counts of one or more categorical variables. Relative frequency tables show what percent of data points fit in each category.\n",
    "\n",
    "In the example below ([source](https://www.khanacademy.org/math/statistics-probability/analyzing-categorical-data/two-way-tables-for-categorical-data/a/two-way-tables-review)) there are two variables - gender and preference - this is where the two in two-way frequency table comes from. Each cell tells us the number (or frequency).\n",
    "\n",
    "<img src=\"images/two-way-frequency-table.png\" alt=\"Correlations\" style=\"width: 300px;\"/>\n",
    "\n",
    "Two-way relative frequency tables show what percent of data points fit in each category. We can use row relative frequencies or column relative frequencies, it just depends on the context of the problem.\n",
    "\n",
    "<img src=\"images/two-way-relative-frequency-table.png\" alt=\"Correlations\" style=\"width: 600px;\"/>\n",
    "\n",
    "Sometimes your percentages won't add up to 100% even though we rounded properly. This is called `round-off error`, and we don't worry about it too much.\n",
    "\n",
    "Two-way relative frequency tables are useful when there are different sample sizes in a dataset. In this example, more females were surveyed than males, so using percentages makes it easier to compare the preferences of males and females. From the relative frequencies, we can see that a large majority of males preferred dogs (78%) compared to a minority of females (41%).\n",
    "    \n",
    "    - Stacked column chart\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two-way frequency tables, also called contingency tables, are tables of counts with two dimensions where each dimension is a different variable. Two-way tables can give you insight into the relationship between two variables. To create a two way table, pass two variables to the pd.crosstab() function instead of one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table of survival vs. sex\n",
    "survived_sex = pd.crosstab(index=titanic_train[\"Survived\"], \n",
    "                           columns=titanic_train[\"Sex\"])\n",
    "\n",
    "survived_sex.index= [\"died\",\"survived\"]\n",
    "\n",
    "survived_sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table of survival vs passenger class\n",
    "survived_class = pd.crosstab(index=titanic_train[\"Survived\"], \n",
    "                            columns=titanic_train[\"Pclass\"])\n",
    "\n",
    "survived_class.columns = [\"class1\",\"class2\",\"class3\"]\n",
    "survived_class.index= [\"died\",\"survived\"]\n",
    "\n",
    "survived_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table of survival vs passenger class\n",
    "survived_class = pd.crosstab(index=titanic_train[\"Survived\"], \n",
    "                            columns=titanic_train[\"Pclass\"],\n",
    "                             margins=True)   # Include row and column totals\n",
    "\n",
    "survived_class.columns = [\"class1\",\"class2\",\"class3\",\"rowtotal\"]\n",
    "survived_class.index= [\"died\",\"survived\",\"coltotal\"]\n",
    "\n",
    "survived_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get the total proportion of counts in each cell, \n",
    "# divide the table by the grand total:\n",
    "survived_class/survived_class.ix[\"coltotal\",\"rowtotal\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get the proportion of counts along each column \n",
    "# (in this case, the survival rate within each passenger class) \n",
    "# divide by the column totals:\n",
    "survived_class/survived_class.ix[\"coltotal\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the proportion of counts along each row divide by the row totals. The division operator functions on a row-by-row basis when used on DataFrames by default. In this case we want to divide each column by the rowtotals column. To get division to work on a column by column basis, use df.div() with the axis set to 0 (or \"index\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survived_class.div(survived_class[\"rowtotal\"],\n",
    "                   axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix (heatmap style)\n",
    "#correlation matrix\n",
    "corrmat = df_train.corr()\n",
    "f, ax = plt.subplots(figsize=(12, 9))\n",
    "sns.heatmap(corrmat, vmax=.8, square=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saleprice correlation matrix (dependent variable)\n",
    "k = 10 #number of variables for heatmap\n",
    "cols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index\n",
    "cm = np.corrcoef(df_train[cols].values.T)\n",
    "sns.set(font_scale=1.25)\n",
    "hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', \n",
    "                 annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scatterplot\n",
    "sns.set()\n",
    "cols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\n",
    "sns.pairplot(df_train[cols], size = 2.5)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relationship with numerical variables\n",
    "#scatter plot grlivarea/saleprice\n",
    "var = 'GrLivArea'\n",
    "data = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\n",
    "data.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relationship with categorical features\n",
    "#box plot overallqual/saleprice\n",
    "var = 'OverallQual'\n",
    "data = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\n",
    "f, ax = plt.subplots(figsize=(8, 6))\n",
    "fig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\n",
    "fig.axis(ymin=0, ymax=800000);\n",
    "plt.xticks(rotation=90);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Sex</th>\n",
       "      <th>female</th>\n",
       "      <th>male</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>died</th>\n",
       "      <td>81</td>\n",
       "      <td>468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>survived</th>\n",
       "      <td>233</td>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Sex       female  male\n",
       "died          81   468\n",
       "survived     233   109"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Two-way table\n",
    "# Table of survival vs. sex\n",
    "survived_sex = pd.crosstab(index=df_titanic[\"Survived\"], columns=df_titanic[\"Sex\"])\n",
    "survived_sex.index= [\"died\", \"survived\"]\n",
    "survived_sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class1</th>\n",
       "      <th>class2</th>\n",
       "      <th>class3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>died</th>\n",
       "      <td>80</td>\n",
       "      <td>97</td>\n",
       "      <td>372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>survived</th>\n",
       "      <td>136</td>\n",
       "      <td>87</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          class1  class2  class3\n",
       "died          80      97     372\n",
       "survived     136      87     119"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Table of survival vs passenger class\n",
    "survived_class = pd.crosstab(index=df_titanic[\"Survived\"], \n",
    "                            columns=df_titanic[\"Pclass\"])\n",
    "\n",
    "survived_class.columns = [\"class1\",\"class2\",\"class3\"]\n",
    "survived_class.index= [\"died\",\"survived\"]\n",
    "\n",
    "survived_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class1</th>\n",
       "      <th>class2</th>\n",
       "      <th>class3</th>\n",
       "      <th>rowtotal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>died</th>\n",
       "      <td>80</td>\n",
       "      <td>97</td>\n",
       "      <td>372</td>\n",
       "      <td>549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>survived</th>\n",
       "      <td>136</td>\n",
       "      <td>87</td>\n",
       "      <td>119</td>\n",
       "      <td>342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coltotal</th>\n",
       "      <td>216</td>\n",
       "      <td>184</td>\n",
       "      <td>491</td>\n",
       "      <td>891</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          class1  class2  class3  rowtotal\n",
       "died          80      97     372       549\n",
       "survived     136      87     119       342\n",
       "coltotal     216     184     491       891"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the marginal counts (totals for each row and column) by including the argument margins=True\n",
    "# Table of survival vs passenger class\n",
    "survived_class = pd.crosstab(index=df_titanic[\"Survived\"], \n",
    "                            columns=df_titanic[\"Pclass\"],\n",
    "                             margins=True)   # Include row and column totals\n",
    "\n",
    "survived_class.columns = [\"class1\",\"class2\",\"class3\",\"rowtotal\"]\n",
    "survived_class.index= [\"died\",\"survived\",\"coltotal\"]\n",
    "\n",
    "survived_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class1</th>\n",
       "      <th>class2</th>\n",
       "      <th>class3</th>\n",
       "      <th>rowtotal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>died</th>\n",
       "      <td>0.089787</td>\n",
       "      <td>0.108866</td>\n",
       "      <td>0.417508</td>\n",
       "      <td>0.616162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>survived</th>\n",
       "      <td>0.152637</td>\n",
       "      <td>0.097643</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>0.383838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coltotal</th>\n",
       "      <td>0.242424</td>\n",
       "      <td>0.206510</td>\n",
       "      <td>0.551066</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            class1    class2    class3  rowtotal\n",
       "died      0.089787  0.108866  0.417508  0.616162\n",
       "survived  0.152637  0.097643  0.133558  0.383838\n",
       "coltotal  0.242424  0.206510  0.551066  1.000000"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "survived_class = pd.crosstab(index=df_titanic[\"Survived\"], \n",
    "                            columns=df_titanic[\"Pclass\"],\n",
    "                             margins=True, normalize=True)   # Include row and column totals\n",
    "\n",
    "survived_class.columns = [\"class1\",\"class2\",\"class3\",\"rowtotal\"]\n",
    "survived_class.index= [\"died\",\"survived\",\"coltotal\"]\n",
    "\n",
    "survived_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The crosstab() function lets you create tables out of more than two categories. Higher dimensional tables can be a little confusing to look at, but they can also yield finer-grained insight into interactions between multiple variables. Let's create a 3-way table inspecting survival, sex and passenger class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surv_sex_class = pd.crosstab(index=titanic_train[\"Survived\"], \n",
    "                             columns=[titanic_train[\"Pclass\"],\n",
    "                                      titanic_train[\"Sex\"]],\n",
    "                             margins=True)   # Include row and column totals\n",
    "\n",
    "surv_sex_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that by passing a second variable to the columns argument, the resulting table has columns categorized by both Pclass and Sex. The outermost index (Pclass) returns sections of the table instead of individual columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surv_sex_class[2]        # Get the subtable under Pclass 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surv_sex_class[2][\"female\"]   # Get female column within Pclass 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the proportion of survival across each column\n",
    "surv_sex_class/surv_sex_class.ix[\"All\"]    # Divide by column totals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see something quite interesting: over 90% of women in first class and second class survived, but only 50% of women in third class survived. Men in first class also survived at a greater rate than men in lower classes. Passenger class seems to have a significant impact on survival, so it would likely be useful to include as a feature in a predictive model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Stacked column chart`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacked column chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Chi_Square Test` is used to derive statistical significance of relationship between varables. It also tests whether the evidence in the sample is strong enough to generalize the relationship for a larger population. It returns probability of the computed chi-square distribution with the degree of freedom.\n",
    "- Probability of 0: both categorical variables are dependent\n",
    "- Probability of 1: independent\n",
    "- Probability less than 0.05: indicates that the relationship between the variables is significant at 95% of confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chi-Square Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other statistical measures used to analyze the power of relationship are:\n",
    "- Cramer's V for Nominal Categorical Variable\n",
    "- Mantel-Haenszed Chi-Square for ordinal categorical variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical and Continuous\n",
    "To explore relation between a categorical and continuous variables, we can draw box plots for each level of categorical variables. If levels are small in number, there is no statistical significance.\n",
    "#### Methods\n",
    "- Z-test - tests if means of two groups are statistically different from each other \n",
    "- T-test - like Z-test but for categories with less than 30 samples each\n",
    "- ANOVA - assesses if the average of more than two groups is statistically different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Values Handling\n",
    "Missing data in the training datase can reduce the power / fit of a model or can lead to a biased model as the data do not present relationships between variables correctly. Most libraries (including scikit-learn) will give you an error if you try to build a model using data with missing values.\n",
    "\n",
    "The most often reason for missing data are related to:\n",
    "- **Data collection**. Difficult and usually time consuming to correct.\n",
    "- **Data extraction**. Easy to find and corrected, mechanisms like hashing may help in ensuring that the data are extracted correctly.\n",
    "\n",
    "<img src=\"images/missing-values-handling.png\" alt=\"Correlations\" style=\"width: 600px;\"/>\n",
    "\n",
    "[source](https://towardsdatascience.com/how-to-handle-missing-data-8646b18db0d4)\n",
    "\n",
    "The missing values treatment can be as follows:\n",
    "- **Listwise or pairwise deletion**. It is important to understand that in the vast majority of cases, an important assumption to using either of these techniques is that your data is missing completely at random (MCAR). \n",
    "\n",
    "    In the `listwise deletion` we delete observations (or columns) where any of the variable is missing. It is simple method but reduces the power of model by reducing the sample size. \n",
    "    \n",
    "    `Pairwise deletion` occurs when the statistical procedure uses cases that contain some missing data. The procedure cannot include a particular variable when it has a missing value, but it can still use the case when analyzing other variables with non-missing values. Pairwise deletion allows you to use more of your data. However, each computed statistic may be based on a different subset of cases.\n",
    "\n",
    "    The choice between pairwise and listwise deletion of records is limited. The choice between these two types of deletion is not relevant when only one variable is being analyzed. In other situations, missing values may be treated as a valid category. \n",
    "    \n",
    "- **Mean / mode / median imputation**. Imputation means filling the missing values with estimated (most frequently used) ones. It consists of replacing the missing data for a given attribute quantitavely (mean, median) or qualitatively (mode). Mean imputation is one of the most ‘naive’ imputation methods because unlike more complex methods like k-nearest neighbors imputation, it does not use the information we have about an observation to estimate a value for it. The imputation can take forms of:\n",
    "\n",
    "    - `Generalized imputation`. We calculate mean, median or mode for all non missing values of a variable and replace all missing values of this variable with the result.\n",
    "    - `Similar case imputation`. We calculate mean, median or mode for similar cases only (looking at similarity of other variables of other cases without missing data for the variable in question).\n",
    "    \n",
    "\n",
    "- **Prediction model**. We create a predictive model (linear / logistic regression, tree, etc.) to estimate values that will substitute the missing data. To do this, we divide our dataset into two parts: one with no missing values for the variable (training dataset with a target variable), another with missing values for the variable (test dataset to pedict the target / missing variable). We populate the missing values with the predicted ones.\n",
    "\n",
    "- **KNN Imputation**. The missing values of a variable are imputed using the given number of variables that are most similar to the attribute whose values are missing. The similarity is defined as a distance function.\n",
    "\n",
    "    - Advantages: \n",
    "        - KNN can predict both qualitative and quantitative variables\n",
    "        - Creation of a predictive model for each variable with missing data is not required\n",
    "        - Variables with multiple missing values can be easily treated\n",
    "        - Correlation of data is taken into consideration\n",
    "    - Disadvantages:\n",
    "        - KNN is time consuming\n",
    "        - Choice of k-value is very critical "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where function\n",
    "my_data = np.random.uniform(-1,1,25)  # Generate new random numbers\n",
    "\n",
    "my_data = np.where(my_data < 0,       # A logical test\n",
    "                   0,                 # Value to set if the test is true\n",
    "                   my_data)           # Value to set if the test is false\n",
    "\n",
    "print(my_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can get the row indexes of the missing values with np.where()\n",
    "missing = np.where(titanic_train[\"Age\"].isnull() == True)\n",
    "missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(missing[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_train.hist(column='Age',    # Column to plot\n",
    "                   figsize=(9,6),   # Plot size\n",
    "                   bins=20)         # Number of histogram bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NaNs in Age with 28\n",
    "new_age_var = np.where(titanic_train[\"Age\"].isnull(), # Logical check\n",
    "                       28,                       # Value if check is true\n",
    "                       titanic_train[\"Age\"])     # Value if check is false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#missing data\n",
    "total = df_train.isnull().sum().sort_values(ascending=False)\n",
    "percent = (df_train.isnull().sum()/df_train.isnull().count()).sort_values(ascending=False)\n",
    "missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "missing_data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dealing with missing data (removing)\n",
    "df_train = df_train.drop((missing_data[missing_data['Total'] > 1]).index,1)\n",
    "df_train = df_train.drop(df_train.loc[df_train['Electrical'].isnull()].index)\n",
    "df_train.isnull().sum().max() #just checking that there's no missing data missing..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the number of missing data points per column\n",
    "missing_values_count = nfl_data.isnull().sum()\n",
    "\n",
    "# look at the # of missing points in the first ten columns\n",
    "missing_values_count[0:10]\n",
    "\n",
    "# how many total missing values do we have?\n",
    "total_cells = np.product(nfl_data.shape)\n",
    "total_missing = missing_values_count.sum()\n",
    "\n",
    "# percent of data that is missing\n",
    "(total_missing/total_cells) * 100\n",
    "\n",
    "# remove all the rows that contain a missing value\n",
    "nfl_data.dropna()\n",
    "\n",
    "# remove all columns with at least one missing value\n",
    "columns_with_na_dropped = nfl_data.dropna(axis=1)\n",
    "columns_with_na_dropped.head()\n",
    "\n",
    "# just how much data did we lose?\n",
    "print(\"Columns in original dataset: %d \\n\" % nfl_data.shape[1])\n",
    "print(\"Columns with na's dropped: %d\" % columns_with_na_dropped.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'original_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-d407a9f6d99a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimpute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSimpleImputer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmy_imputer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSimpleImputer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdata_with_imputed_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmy_imputer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# make copy to avoid changing original data (when Imputing)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'original_data' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "my_imputer = SimpleImputer()\n",
    "data_with_imputed_values = my_imputer.fit_transform(original_data)\n",
    "\n",
    "# make copy to avoid changing original data (when Imputing)\n",
    "new_data = original_data.copy()\n",
    "\n",
    "# make new columns indicating what will be imputed\n",
    "cols_with_missing = (col for col in new_data.columns \n",
    "                                 if new_data[col].isnull().any())\n",
    "for col in cols_with_missing:\n",
    "    new_data[col + '_was_missing'] = new_data[col].isnull()\n",
    "\n",
    "# Imputation\n",
    "my_imputer = SimpleImputer()\n",
    "new_data = pd.DataFrame(my_imputer.fit_transform(new_data))\n",
    "new_data.columns = original_data.columns\n",
    "\n",
    "# example on how to use imputer: https://www.kaggle.com/dansbecker/handling-missing-values/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean imputation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "# Create an empty dataset\n",
    "df = pd.DataFrame()\n",
    "\n",
    "# Create two variables called x0 and x1. Make the first value of x1 a missing value\n",
    "df['x0'] = [0.3051,0.4949,0.6974,0.3769,0.2231,0.341,0.4436,0.5897,0.6308,0.5]\n",
    "df['x1'] = [np.nan,0.2654,0.2615,0.5846,0.4615,0.8308,0.4962,0.3269,0.5346,0.6731]\n",
    "\n",
    "# View the dataset\n",
    "df\n",
    "\n",
    "# Fit imputer\n",
    "# Create an imputer object that looks for 'Nan' values, then replaces them with the mean value of the feature by columns (axis=0)\n",
    "mean_imputer = Imputer(missing_values='NaN', strategy='mean', axis=0)\n",
    "\n",
    "# Train the imputor on the df dataset\n",
    "mean_imputer = mean_imputer.fit(df)\n",
    "\n",
    "# Apply the imputer to the df dataset\n",
    "imputed_df = mean_imputer.transform(df.values)\n",
    "\n",
    "# View the data\n",
    "imputed_df\n",
    "# Notice that 0.49273333 is the imputed value, replacing the np.NaN value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outliers\n",
    "\n",
    "An `outlier` is an observation that appears far away and diverges from an overall pattern in a sample. Outliers can be of two types: univariate and multivariate. `Univariate outliers` can be found while looking at distribution of a single variable data. `Multivariate outliers` are outstanding observations in an n-dimensional space.\n",
    "\n",
    "<img src=\"images/outlier.png\" alt=\"Correlations\" style=\"width: 300px;\"/>\n",
    "\n",
    "<img src=\"images/n-outlier.png\" alt=\"Correlations\" style=\"width: 600px;\"/>\n",
    "\n",
    "[source](https://www.analyticsvidhya.com/blog/2016/01/guide-data-exploration/)\n",
    "\n",
    "The method of dealing with outliers depend on the reason of their occurrence. Causes of outliers can be classified in two broad categories: artificial (error) / non-natural and natural:\n",
    "\n",
    "- artificial: data entry errors (human errors, experimental / sampling errors, intentional outlier), measurement errors (faulty instruments), data processing errors.\n",
    "- natural: not caused by error.\n",
    "\n",
    "Outliers can drastically change the results of the data analysis and statistical modeling. There are numerous unfavourable impacts of outliers in the data set:\n",
    "\n",
    "- They increase the error variance and reduces the power of statistical tests.\n",
    "- If the outliers are non-randomly distributed, they can decrease normality.\n",
    "- They can bias or influence estimates that may be of substantive interest.\n",
    "- They can also impact the basic assumption of Regression, ANOVA and other statistical model assumptions.\n",
    "\n",
    "<img src=\"images/outliers-impact.png\" alt=\"Correlations\" style=\"width: 600px;\"/>\n",
    "\n",
    "Most commonly used method to detect outliers is visualization (like box-plot, histogram, scatter plot). Some analysts use various thumb rules to detect outliers:\n",
    "\n",
    "- Any value, which is beyond the range of -1.5 x IQR to 1.5 x IQR.\n",
    "- Use capping methods. Any value which out of range of 5th and 95th percentile can be considered as outlier.\n",
    "- Data points, three or more standard deviation away from mean are considered outlier.\n",
    "- Outlier detection is merely a special case of the examination of data for influential data points and it also depends on the business understanding.\n",
    "- Bivariate and multivariate outliers are typically measured using either an index of influence or leverage, or distance. Popular indices such as `Mahalanobis’ distance` and `Cook’s D` are frequently used to detect outliers.\n",
    "\n",
    "Most of the ways to deal with outliers are similar to the methods of missing values like deleting observations, transforming them, binning them, treat them as a separate group, imputing values and other statistical methods:\n",
    "\n",
    "- **Deleting observations**: We delete outlier values if it is due to data entry error, data processing error or outlier observations are very small in numbers. We can also use trimming at both ends to remove outliers.\n",
    "\n",
    "- **Transforming and binning values**: Transforming variables can also eliminate outliers. `Natural log of a value` reduces the variation caused by extreme values. `Binning` is also a form of variable transformation. Decision Tree algorithm allows to deal with outliers well due to binning of variable. We can also use the process of assigning weights to different observations.\n",
    "\n",
    "<img src=\"images/log-transformation.png\" alt=\"Correlations\" style=\"width: 600px;\"/>\n",
    "\n",
    "- **Imputing**: We can use mean, median, mode to change outlier values but before that, we should analyse if it is natural outlier or artificial. If it is artificial, we can go with imputing values. We can also use statistical model to predict values of outlier observation and after that we can impute it with predicted values.\n",
    "\n",
    "- **Treat separately**: If there are significant number of outliers, we should treat them separately in the statistical model. One of the approach is to treat both groups as two different groups and build individual model for both groups and then combine the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boxplots are designed to show the spread of the data and help identify outliers\n",
    "# In a boxplot, the central box represents 50% of the data and the central bar represents the median. \n",
    "# The dotted lines with bars on the ends are \"whiskers\" which encompass the great majority of the data \n",
    "# and points beyond the whiskers indicate uncommon values.\n",
    "titanic_train[\"Fare\"].plot(kind=\"box\",\n",
    "                           figsize=(9,9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deleting points (outliars)\n",
    "df_train.sort_values(by = 'GrLivArea', ascending = False)[:2]\n",
    "df_train = df_train.drop(df_train[df_train['Id'] == 1299].index)\n",
    "df_train = df_train.drop(df_train[df_train['Id'] == 524].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering\n",
    "\n",
    "Feature engineering is the science (and art) of extracting more information from existing data. You are not adding any new data here, but you are actually making the data you already have more useful.\n",
    "\n",
    "The trick here seems to be the choice of the right features (feature selection) and not the definition of complex relationships between them (feature engineering).\n",
    "\n",
    "### Variables transformation\n",
    "\n",
    "In data modelling, transformation refers to the replacement of a variable by a function. For instance, replacing a variable x by the square or cube root, or logarithm x is a transformation. In other words, transformation is a process that changes the distribution or relationship of a variable with others.\n",
    "\n",
    "Reasons for variable transformation\n",
    "\n",
    "- When we want to `change the scale` of a variable or `standardize the values` of a variable for better understanding. While this transformation is a must if you have data in different scales, this transformation does not change the shape of the variable distribution.\n",
    "\n",
    "    Most of the times, your dataset will contain features highly varying in magnitudes, units and range. But since, most of the machine learning algorithms use Eucledian distance between two data points in their computations, this is a problem. The features with high magnitudes will weigh in a lot more in the distance calculations than features with low magnitudes. To supress this effect, we need to bring all features to the same level of magnitudes. This can be acheived by scaling.\n",
    "\n",
    "    There are four common methods to perform Feature Scaling:\n",
    "    \n",
    "    - **Standardization**. Standardization replaces the values by their Z scores. \n",
    "    \n",
    "    <img src=\"images/standardization.png\" alt=\"Correlations\" style=\"width: 200px;\"/>\n",
    "\n",
    "    This redistributes the features with their mean μ = 0 and standard deviation σ =1. `sklearn.preprocessing.scale` helps us implementing standardisation in python.\n",
    "    \n",
    "    - **Mean Normalization**. This distribution will have values between -1 and 1 with μ=0.\n",
    "\n",
    "    <img src=\"images/mean-normalization.png\" alt=\"Correlations\" style=\"width: 200px;\"/>\n",
    "    \n",
    "    Standardisation and Mean Normalization can be used for algorithms that assumes zero centric data like `Principal Component Analysis (PCA)`.\n",
    "    \n",
    "    - **Min-Max Scaling**. This scaling brings the value between 0 and 1.\n",
    "    \n",
    "    <img src=\"images/min-max-scaling.png\" alt=\"Correlations\" style=\"width: 200px;\"/>\n",
    "    \n",
    "    - **Unit Vector**. Scaling is done considering the whole feature vecture to be of unit length.\n",
    "    \n",
    "    <img src=\"images/unit-vector.png\" alt=\"Correlations\" style=\"width: 200px;\"/>\n",
    "\n",
    "    `Min-Max Scaling` and `Unit Vector` techniques produces values of range [0,1]. When dealing with features with hard boundaries this is quite useful. For example, when dealing with image data, the colors can range from only 0 to 255.\n",
    "    \n",
    "    Scale variables if using any algorithm that computes distance or assumes normality. For example:\n",
    "\n",
    "    - `k-nearest neighbors` with an Euclidean distance measure is sensitive to magnitudes and hence should be scaled for all features to weigh in equally.\n",
    "    - Scaling is critical, while performing `Principal Component Analysis (PCA)`. PCA tries to get the features with maximum variance and the variance is high for high magnitude features. This skews the PCA towards high magnitude features.\n",
    "    - We can speed up `gradient descent` by scaling. This is because θ will descend quickly on small ranges and slowly on large ranges, and so will oscillate inefficiently down to the optimum when the variables are very uneven.\n",
    "    \n",
    "    But for below, scalling may not help:\n",
    "    \n",
    "    - `Tree based models` are not distance based models and can handle varying ranges of features. Hence, Scaling is not required while modelling trees.\n",
    "    - Algorithms like `Linear Discriminant Analysis (LDA)` or `Naive Bayes` are by design equipped to handle this and gives weights to the features accordingly. Performing a features scaling in these algorithms may not have much effect.\n",
    "\n",
    "- When we want to `transform complex non-linear relationships into linear relationships`. Existence of a linear relationship between variables is easier to comprehend compared to a non-linear or curved relation and also improves the prediction. `Log transformation` is one of the commonly used transformation technique used in these situations.\n",
    "\n",
    "- Symmetric distribution is preferred over skewed distribution as it is easier to interpret and generate inferences. Some modeling techniques requires `normal distribution` of variables. For `right skewed distribution`, we take square / cube root or logarithm of variable and for `left skewed distribution`, we take square / cube or exponential of variables.\n",
    "\n",
    "- When we want to group variables and represent their values in groups we use `binning`.\n",
    "\n",
    "There are various methods used to transform variables:\n",
    "\n",
    "- **Logarithm**: Log of a variable is a common transformation method used to change the shape of distribution of the variable. It is generally used for reducing right skewness of variables. Though, It can’t be applied to zero or negative values as well.\n",
    "- **Square / Cube root**: The square and cube root of a variable has a sound effect on variable distribution. However, it is not as significant as logarithmic transformation. Cube root has its own advantage. It can be applied to negative values including zero. Square root can be applied to positive values including zero.\n",
    "- **Binning**: It is used to categorize variables. It is performed on original values, percentile or frequency. Decision of categorization technique is based on business understanding. For example, we can categorize income in three categories, namely: High, Average and Low. We can also perform `co-variate binning` which depends on the value of more than one variables.\n",
    "\n",
    "### Variables creation\n",
    "\n",
    "Feature / Variable creation is a process to generate a new variables / features based on existing variable(s).\n",
    "\n",
    "There are various techniques to create new features:\n",
    "\n",
    "- **Creating derived variables**: This refers to creating new variables from existing variable(s) using set of functions or different methods.\n",
    "- **Creating dummy variables**: One of the most common application of dummy variable is to convert categorical variable into numerical variables (0-1 encoding, or `one-hot-encoding`). Dummy variables are also called `Indicator Variables`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a new variable, Family, that combines SibSp and Parch \n",
    "# to indicate the total number of family members \n",
    "# (siblings, spouses, parents and children) a passenger has on board:\n",
    "titanic_train[\"Family\"] = titanic_train[\"SibSp\"] + titanic_train[\"Parch\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_cabin = titanic_train[\"Cabin\"].astype(str)    # Convert cabin to str\n",
    "\n",
    "new_Cabin = np.array([cabin[0] for cabin in char_cabin]) # Take first letter\n",
    "\n",
    "titanic_train[\"Cabin\"] = pd.Categorical(new_Cabin)  # Save the new cabin var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference between `scaling` and `normalization` is that, in scaling, you're changing the range of your data while in normalization you're changing the shape of the distribution of your data. Scaling just changes the range of your data. Normalization is a more radical transformation. The point of normalization is to change your observations so that they can be described as a normal distribution.\n",
    "\n",
    "`Normal distribution`: Also known as the \"bell curve\", this is a specific statistical distribution where a roughly equal observations fall above and below the mean, the mean and the median are the same, and there are more observations closer to the mean. The normal distribution is also known as the Gaussian distribution.\n",
    "\n",
    "In general, you'll only want to normalize your data if you're going to be using a machine learning or statistics technique that assumes your data is normally distributed. Some examples of these include t-tests, ANOVAs, linear regression, linear discriminant analysis (LDA) and Gaussian naive Bayes. (Pro tip: any method with \"Gaussian\" in the name probably assumes normality.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale and normalize\n",
    "# modules we'll use\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# for Box-Cox Transformation\n",
    "from scipy import stats\n",
    "\n",
    "# for min_max scaling\n",
    "from mlxtend.preprocessing import minmax_scaling\n",
    "\n",
    "# plotting modules\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# read in all our data\n",
    "kickstarters_2017 = pd.read_csv(\"../input/kickstarter-projects/ks-projects-201801.csv\")\n",
    "\n",
    "# set seed for reproducibility\n",
    "np.random.seed(0)\n",
    "\n",
    "# Scaling\n",
    "# generate 1000 data points randomly drawn from an exponential distribution\n",
    "original_data = np.random.exponential(size = 1000)\n",
    "\n",
    "# mix-max scale the data between 0 and 1\n",
    "scaled_data = minmax_scaling(original_data, columns = [0])\n",
    "\n",
    "# plot both together to compare\n",
    "fig, ax=plt.subplots(1,2)\n",
    "sns.distplot(original_data, ax=ax[0])\n",
    "ax[0].set_title(\"Original Data\")\n",
    "sns.distplot(scaled_data, ax=ax[1])\n",
    "ax[1].set_title(\"Scaled data\")\n",
    "\n",
    "# Notice that the shape of the data doesn't change, but that instead of ranging from 0 to 8ish, it now ranges from 0 to 1.\n",
    "\n",
    "# Normalization\n",
    "# The method were using to normalize here is called the Box-Cox Transformation. \n",
    "# normalize the exponential data with boxcox\n",
    "normalized_data = stats.boxcox(original_data)\n",
    "\n",
    "# plot both together to compare\n",
    "fig, ax=plt.subplots(1,2)\n",
    "sns.distplot(original_data, ax=ax[0])\n",
    "ax[0].set_title(\"Original Data\")\n",
    "sns.distplot(normalized_data[0], ax=ax[1])\n",
    "ax[1].set_title(\"Normalized data\")\n",
    "\n",
    "# Notice that the shape of our data has changed. Before normalizing it was almost L-shaped. \n",
    "# But after normalizing it looks more like the outline of a bell (hence \"bell curve\").\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram - Kurtosis and skewness.\n",
    "# Normal probability plot - Data distribution should closely follow the diagonal that represents the normal distribution.\n",
    "#histogram and normal probability plot\n",
    "sns.distplot(df_train['SalePrice'], fit=norm);\n",
    "fig = plt.figure()\n",
    "res = stats.probplot(df_train['SalePrice'], plot=plt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple data transformation can solve the problem. This is one of the awesome things you can learn in statistical books: in case of positive skewness, log transformations usually works well. When I discovered this, I felt like an Hogwarts' student discovering a new cool spell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying log transformation\n",
    "df_train['SalePrice'] = np.log(df_train['SalePrice'])\n",
    "\n",
    "#transformed histogram and normal probability plot\n",
    "sns.distplot(df_train['SalePrice'], fit=norm);\n",
    "fig = plt.figure()\n",
    "res = stats.probplot(df_train['SalePrice'], plot=plt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character encoding\n",
    "\n",
    "Character encodings are specific sets of rules for mapping from raw binary byte strings (that look like this: 0110100001101001) to characters that make up human-readable text (like \"hi\"). There are many different encodings, and if you tried to read in text with a different encoding that the one it was originally written in, you ended up with scrambled text called \"mojibake\" (said like mo-gee-bah-kay). Here's an example of mojibake:\n",
    "\n",
    "æ–‡å—åŒ–ã??\n",
    "\n",
    "You might also end up with a \"unknown\" characters. There are what gets printed when there's no mapping between a particular byte and a character in the encoding you're using to read your byte string in and they look like this:\n",
    "\n",
    "����������\n",
    "\n",
    "Character encoding mismatches are less common today than they used to be, but it's definitely still a problem. There are lots of different character encodings, but the main one you need to know is UTF-8.\n",
    "\n",
    "UTF-8 is the standard text encoding. All Python code is in UTF-8 and, ideally, all your data should be as well. It's when things aren't in UTF-8 that you run into trouble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modules we'll use\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# helpful character encoding module\n",
    "import chardet\n",
    "\n",
    "# set seed for reproducibility\n",
    "np.random.seed(0)\n",
    "\n",
    "# start with a string\n",
    "before = \"This is the euro symbol: €\"\n",
    "\n",
    "# check to see what datatype it is\n",
    "type(before)\n",
    "\n",
    "# encode it to a different encoding, replacing characters that raise errors\n",
    "after = before.encode(\"utf-8\", errors = \"replace\")\n",
    "\n",
    "# check the type\n",
    "type(after)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look at a bytes object, you'll see that it has a b in front of it, and then maybe some text after. That's because bytes are printed out as if they were characters encoded in ASCII. (ASCII is an older character encoding that doesn't really work for writing any language other than English.) Here you can see that our euro symbol has been replaced with some mojibake that looks like \"\\xe2\\x82\\xac\" when it's printed as if it were an ASCII string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at what the bytes look like\n",
    "after"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we convert our bytes back to a string with the correct encoding, we can see that our text is all there correctly, which is great! :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert it back to utf-8\n",
    "print(after.decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, when we try to use a different encoding to map our bytes into a string,, we get an error. This is because the encoding we're trying to use doesn't know what to do with the bytes we're trying to pass it. You need to tell Python the encoding that the byte string is actually supposed to be in.\n",
    "\n",
    "You can think of different encodings as different ways of recording music. You can record the same music on a CD, cassette tape or 8-track. While the music may sound more-or-less the same, you need to use the right equipment to play the music from each recording format. The correct decoder is like a cassette player or a cd player. If you try to play a cassette in a CD player, it just won't work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to decode our bytes with the ascii encoding\n",
    "print(after.decode(\"ascii\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also run into trouble if we try to use the wrong encoding to map from a string to bytes. Like I said earlier, strings are UTF-8 by default in Python 3, so if we try to treat them like they were in another encoding we'll create problems.\n",
    "\n",
    "For example, if we try to convert a string to bytes for ascii using encode(), we can ask for the bytes to be what they would be if the text was in ASCII. Since our text isn't in ASCII, though, there will be some characters it can't handle. We can automatically replace the characters that ASCII can't handle. If we do that, however, any characters not in ASCII will just be replaced with the unknown character. Then, when we convert the bytes back to a string, the character will be replaced with the unknown character. The dangerous part about this is that there's not way to tell which character it should have been. That means we may have just made our data unusable!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start with a string\n",
    "before = \"This is the euro symbol: €\"\n",
    "\n",
    "# encode it to a different encoding, replacing characters that raise errors\n",
    "after = before.encode(\"ascii\", errors = \"replace\")\n",
    "\n",
    "# convert it back to utf-8\n",
    "print(after.decode(\"ascii\"))\n",
    "\n",
    "# We've lost the original underlying byte string! It's been \n",
    "# replaced with the underlying byte string for the unknown character :("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is bad and we want to avoid doing it! It's far better to convert all our text to UTF-8 as soon as we can and keep it in that encoding. The best time to convert non UTF-8 input into UTF-8 is when you read in files, which we'll talk about next.\n",
    "\n",
    "First, however, try converting between bytes and strings with different encodings and see what happens. Notice what this does to your text. Would you want this to happen to data you were trying to analyze?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in files with encoding problems\n",
    "\n",
    "Most files you'll encounter will probably be encoded with UTF-8. This is what Python expects by default, so most of the time you won't run into problems. However, sometimes you'll get an error like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to read in a file not in UTF-8\n",
    "kickstarter_2016 = pd.read_csv(\"../input/kickstarter-projects/ks-projects-201612.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we get the same UnicodeDecodeError we got when we tried to decode UTF-8 bytes as if they were ASCII! This tells us that this file isn't actually UTF-8. We don't know what encoding it actually is though. One way to figure it out is to try and test a bunch of different character encodings and see if any of them work. A better way, though, is to use the chardet module to try and automatically guess what the right encoding is. It's not 100% guaranteed to be right, but it's usually faster than just trying to guess.\n",
    "\n",
    "I'm going to just look at the first ten thousand bytes of this file. This is usually enough for a good guess about what the encoding is and is much faster than trying to look at the whole file. (Especially with a large file this can be very slow.) Another reason to just look at the first part of the file is that we can see by looking at the error message that the first problem is the 11th character. So we probably only need to look at the first little bit of the file to figure out what's going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the first ten thousand bytes to guess the character encoding\n",
    "with open(\"../input/kickstarter-projects/ks-projects-201801.csv\", 'rb') as rawdata:\n",
    "    result = chardet.detect(rawdata.read(10000))\n",
    "\n",
    "# check what the character encoding might be\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So chardet is 73% confidence that the right encoding is \"Windows-1252\". Let's see if that's correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the file with the encoding detected by chardet\n",
    "kickstarter_2016 = pd.read_csv(\"../input/kickstarter-projects/ks-projects-201612.csv\", encoding='Windows-1252')\n",
    "\n",
    "# look at the first few lines\n",
    "kickstarter_2016.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yep, looks like chardet was right! The file reads in with no problem (although we do get a warning about datatypes) and when we look at the first few rows it seems to be be fine.\n",
    "\n",
    "What if the encoding chardet guesses isn't right? Since chardet is basically just a fancy guesser, sometimes it will guess the wrong encoding. One thing you can try is looking at more or less of the file and seeing if you get a different result and then try that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving your files with UTF-8 encoding\n",
    "\n",
    "Finally, once you've gone through all the trouble of getting your file into UTF-8, you'll probably want to keep it that way. The easiest way to do that is to save your files with UTF-8 encoding. The good news is, since UTF-8 is the standard encoding in Python, when you save a file it will be saved as UTF-8 by default:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save our file (will be saved as UTF-8 by default!)\n",
    "kickstarter_2016.to_csv(\"ks-projects-201801-utf8.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modules we'll use\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "\n",
    "# read in our data\n",
    "earthquakes = pd.read_csv(\"../input/earthquake-database/database.csv\")\n",
    "landslides = pd.read_csv(\"../input/landslide-events/catalog.csv\")\n",
    "volcanos = pd.read_csv(\"../input/volcanic-eruptions/database.csv\")\n",
    "\n",
    "# set seed for reproducibility\n",
    "np.random.seed(0)\n",
    "\n",
    "# print the first few rows of the date column\n",
    "print(landslides['date'].head())\n",
    "\n",
    "# Pandas uses the \"object\" dtype for storing various types of data types, \n",
    "# but most often when you see a column with the dtype \"object\" it will have strings in it.\n",
    "\n",
    "# check the data type of our date column\n",
    "landslides['date'].dtype\n",
    "\n",
    "# create a new column, date_parsed, with the parsed dates\n",
    "# http://strftime.org/\n",
    "landslides['date_parsed'] = pd.to_datetime(landslides['date'], format = \"%m/%d/%y\")\n",
    "\n",
    "# print the first few rows (the dtype is datetime64)\n",
    "landslides['date_parsed'].head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if I run into an error with multiple date formats? While we're specifying the date format here, sometimes you'll run into an error when there are multiple date formats in a single column. If that happens, you have have pandas try to infer what the right date format should be. You can do that like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landslides['date_parsed'] = pd.to_datetime(landslides['Date'], infer_datetime_format=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why don't you always use infer_datetime_format = True? There are two big reasons not to always have pandas guess the time format. The first is that pandas won't always been able to figure out the correct date format, especially if someone has gotten creative with data entry. The second is that it's much slower than specifying the exact format of the dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the day of the month from the date_parsed column\n",
    "day_of_month_landslides = landslides['date_parsed'].dt.day \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the biggest dangers in parsing dates is mixing up the months and days. The to_datetime() function does have very helpful error messages, but it doesn't hurt to double-check that the days of the month we've extracted make sense.\n",
    "\n",
    "To do this, let's plot a histogram of the days of the month. We expect it to have values between 1 and 31 and, since there's no reason to suppose the landslides are more common on some days of the month than others, a relatively even distribution. (With a dip on 31 because not all months have 31 days.) Let's see if that's the case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove na's\n",
    "day_of_month_landslides = day_of_month_landslides.dropna()\n",
    "\n",
    "# plot the day of the month\n",
    "sns.distplot(day_of_month_landslides, kde=False, bins=31)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inconsistent data entry\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting with Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizations are one of the most powerful tools at your disposal for exploring data and communicating data insights. The pandas library includes basic plotting capabilities that let you create a variety of plots from DataFrames. Plots in pandas are built on top of a popular Python plotting library called matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ggplot'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-6ffce713f83a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# conda search ggplot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# conda install -c conda-forge ggplot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mggplot\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdiamonds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstyle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ggplot'\u001b[0m\u001b[0;34m)\u001b[0m       \u001b[0;31m# Use ggplot style plots*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ggplot'"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "\n",
    "# conda search ggplot\n",
    "# conda install -c conda-forge ggplot\n",
    "#from ggplot import diamonds\n",
    "#matplotlib.style.use('ggplot')       # Use ggplot style plots*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `histogram` is a univariate plot (a plot that displays one variable) that groups a numeric variable into bins and displays the number of observations that fall within each bin. A histogram is a useful tool for getting a sense of the distribution of a numeric variable. Let's create a histogram of diamond carat weight with the df.hist() function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diamonds.hist(column=\"carat\",        # Column to plot\n",
    "              figsize=(8,8),         # Plot size\n",
    "              color=\"blue\")          # Plot color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diamonds.hist(column=\"carat\",        # Column to plot\n",
    "              figsize=(8,8),         # Plot size\n",
    "              color=\"blue\",          # Plot color\n",
    "              bins=50,               # Use 50 bins\n",
    "              range= (0,3.5))        # Limit x-axis range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Boxplots` are another type of univariate plot for summarizing distributions of numeric data graphically. Let's make a boxplot of carat using the pd.boxplot() function. the central box of the boxplot represents the middle 50% of the observations, the central bar is the median and the bars at the end of the dotted lines (whiskers) encapsulate the great majority of the observations. Circles that lie beyond the end of the whiskers are data points that may be outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diamonds.boxplot(column=\"carat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most useful features of a boxplot is the ability to make side-by-side boxplots. A side-by-side boxplot takes a numeric variable and splits it on based on some categorical variable, drawing a different boxplot for each level of the categorical variable. Let's make a side-by-side boxplot of diamond price split by diamond clarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diamonds.boxplot(column=\"price\",        # Column to plot\n",
    "                 by= \"clarity\",         # Column to split upon\n",
    "                 figsize= (8,8))        # Figure size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `density plot` shows the distribution of a numeric variable with a continuous curve. It is similar to a histogram but without discrete bins, a density plot gives a better picture of the underlying shape of a distribution. Create a density plot with series.plot(kind=\"density\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diamonds[\"carat\"].plot(kind=\"density\",  # Create density plot\n",
    "                      figsize=(8,8),    # Set figure size\n",
    "                      xlim= (0,5))      # Limit x axis values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Barplots` are graphs that visually display counts of categorical variables. We can create a barplot by creating a table of counts for a certain variable using the pd.crosstab() function and then passing the counts to df.plot(kind=\"bar\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "carat_table = pd.crosstab(index=diamonds[\"clarity\"], columns=\"count\")\n",
    "carat_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "carat_table.plot(kind=\"bar\",\n",
    "                 figsize=(8,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use a two dimensional table to create a `stacked barplot`. Stacked barplots show the distribution of a second categorical variable within each bar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "carat_table = pd.crosstab(index=diamonds[\"clarity\"], \n",
    "                          columns=diamonds[\"color\"])\n",
    "\n",
    "carat_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "carat_table.plot(kind=\"bar\", \n",
    "                 figsize=(8,8),\n",
    "                 stacked=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `grouped barplot` is an alternative to a stacked barplot that gives each stacked section its own bar. To make a grouped barplot, do not include the stacked argument (or set stacked=False):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "carat_table.plot(kind=\"bar\", \n",
    "                 figsize=(8,8),\n",
    "                 stacked=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Scatterplots` are bivariate (two variable) plots that take two numeric variables and plot data points on the x/y plane. To create a single scatterplot, use df.plot(kind=\"scatter\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diamonds.plot(kind=\"scatter\",     # Create a scatterplot\n",
    "              x=\"carat\",          # Put carat on the x axis\n",
    "              y=\"price\",          # Put price on the y axis\n",
    "              figsize=(10,10),\n",
    "              ylim=(0,20000))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Line plots` are charts used to show the change in a numeric variable based on some other ordered variable. Line plots are often used to plot time series data to show the evolution of a variable over time. Line plots are the default plot type when using df.plot() so you don't have to specify the kind argument when making a line plot in pandas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some data\n",
    "years = [y for y in range(1950,2016)]\n",
    "\n",
    "readings = [(y+np.random.uniform(0,20)-1900) for y in years]\n",
    "\n",
    "time_df = pd.DataFrame({\"year\":years,\n",
    "                        \"readings\":readings})\n",
    "\n",
    "# Plot the data\n",
    "time_df.plot(x=\"year\",\n",
    "             y=\"readings\",\n",
    "             figsize=(9,9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to save plots for later use, you can export the plot figure (plot information) to a file. First get the plot figure with plot.get_figure() and then save it to a file with figure.savefig(\"filename\"). You can save plots to a variety of common image file formats, such as png, jpeg and pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_plot = time_df.plot(x=\"year\",     # Create the plot and save to a variable\n",
    "             y=\"readings\",\n",
    "             figsize=(9,9))\n",
    "\n",
    "my_fig = my_plot.get_figure()            # Get the figure\n",
    "\n",
    "my_fig.savefig(\"line_plot_example.png\")  # Save to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "- [Kaggle Data Cleaning Challenge](https://www.kaggle.com/rtatman/data-cleaning-challenge-inconsistent-data-entry/)\n",
    "- [Kaggle Resampling strategies for imbalanced datasets](https://www.kaggle.com/rafjaa/resampling-strategies-for-imbalanced-datasets)\n",
    "- [8 Tactics to Combat Imbalanced Classes in Your Machine Learning Dataset](https://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
