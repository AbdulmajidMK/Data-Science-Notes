{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An **artificial neural network** (or neural network for short) is a predictive model motivated by the way the brain operates. Think of the brain as a collection of neurons wired together. Each neuron looks at the outputs of the other neurons that feed into it, does a calculation, and then either fires (if the calculation exceeds some threshhold) or doesn’t (if it doesn’t).\n",
    "\n",
    "Accordingly, artificial neural networks consist of artificial neurons, which perform similar calculations over their inputs. Neural networks can solve a wide variety of problems like handwriting recognition and face detection, and they are used heavily in `deep learning`, one of the trendiest subfields of data science. However, most neural networks are `“black boxes”` — inspecting their details doesn’t give you much understanding of how they’re solving a problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron\n",
    "\n",
    "Pretty much the simplest neural network is the **perceptron**, which approximates a sin‐ gle neuron with n binary inputs. It computes a weighted sum of its inputs and “fires” if that weighted sum is zero or greater."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scratch.linear_algebra import Vector, dot\n",
    "\n",
    "def step_function(x: float) -> float:\n",
    "    return 1.0 if x >= 0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptron_output(weights: Vector, bias: float, x: Vector) -> float:\n",
    "    \"\"\"Returns 1 if the perceptron 'fires', 0 if not\"\"\"\n",
    "    calculation = dot(weights, x) + bias\n",
    "    return step_function(calculation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With properly chosen weights, perceptrons can solve a number of simple problems. For example, we can create an `AND gate` (which returns 1 if both its inputs are 1 but returns 0 if one of its inputs is 0):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "and_weights = [2., 2]\n",
    "and_bias = -3.\n",
    "\n",
    "assert perceptron_output(and_weights, and_bias, [1, 1]) == 1\n",
    "assert perceptron_output(and_weights, and_bias, [0, 1]) == 0\n",
    "assert perceptron_output(and_weights, and_bias, [1, 0]) == 0\n",
    "assert perceptron_output(and_weights, and_bias, [0, 0]) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we could build an `OR gate`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "or_weights = [2., 2]\n",
    "or_bias = -1.\n",
    "\n",
    "assert perceptron_output(or_weights, or_bias, [1, 1]) == 1\n",
    "assert perceptron_output(or_weights, or_bias, [0, 1]) == 1\n",
    "assert perceptron_output(or_weights, or_bias, [1, 0]) == 1\n",
    "assert perceptron_output(or_weights, or_bias, [0, 0]) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we could build a `NOT gate` (which has one input and converts 1 to 0 and 0 to 1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_weights = [-2.]\n",
    "not_bias = 1.\n",
    "\n",
    "assert perceptron_output(not_weights, not_bias, [0]) == 1\n",
    "assert perceptron_output(not_weights, not_bias, [1]) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/neural_networks1.png\" alt=\"\" style=\"width: 600px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, there are some problems that simply can’t be solved by a single perceptron. For example, no matter how hard you try, you cannot use a perceptron to build an `XOR gate` that outputs 1 if exactly one of its inputs is 1 and 0 otherwise. This is where we start needing more-complicated neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XOR logic gate without artificial neurons\n",
    "and_gate = min\n",
    "or_gate = max\n",
    "xor_gate = lambda x, y: 0 if x == y else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert xor_gate(0, 1) == 1\n",
    "assert xor_gate(1, 1) == 0\n",
    "assert xor_gate(1, 0) == 1\n",
    "assert xor_gate(0, 0) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed-Forward Neural Network\n",
    "\n",
    "The topology of the brain is enormously complicated, so it’s common to approximate it with an idealized **feed-forward** neural network that consists of discrete layers of neurons, each connected to the next. This typically entails an `input layer` (which receives inputs and feeds them forward unchanged), one or more `“hidden layers”` (each of which consists of neurons that take the outputs of the previous layer, performs some calculation, and passes the result to the next layer), and an `output layer` (which produces the final outputs).\n",
    "\n",
    "Just like the perceptron, each (noninput) neuron has a` weight` corresponding to each of its inputs and a `bias`. To make our representation simpler, we’ll add the bias to the end of our weights vector and give each neuron a bias input that always equals 1.\n",
    "\n",
    "As with the `perceptron`, for each neuron we’ll sum up the products of its inputs and its weights. But here, rather than outputting the step_function applied to that prod‐ uct, we’ll output `a smooth approximation of the step function`. In particular, we’ll use the `sigmoid function`:\n",
    "\n",
    "<img src=\"images/neural_networks2.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "See below for other activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def sigmoid(t: float) -> float:\n",
    "    return 1 / (1 + math.exp(-t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0066928509242848554"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8807970779778823"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Why use sigmoid instead of the simpler step_function?` In order to train a neural network, we’ll need to use calculus, and in order to use calculus, we need smooth functions. The step function isn’t even continuous, and sigmoid is a good smooth approximation of it.\n",
    "\n",
    "Technically `sigmoid` refers to the shape of the function, `logistic` to this particular function although people often use the terms interchangeably."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neuron_output(weights: Vector, inputs: Vector) -> float:\n",
    "    # weights includes the bias term, inputs includes a 1\n",
    "    return sigmoid(dot(weights, inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given this function, `we can represent a neuron` simply as a list of weights whose length is one more than the number of inputs to that neuron (because of the bias weight). Then `we can represent a neural network` as a list of (noninput) layers, where each layer is just a list of the neurons in that layer.\n",
    "\n",
    "That is, we’ll represent a neural network as a list (layers) of lists (neurons) of lists (weights)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def feed_forward(neural_network: List[List[Vector]], input_vector: Vector) -> List[Vector]:\n",
    "    '''\n",
    "    Feeds the input vector through the neural network.\n",
    "    Returns the outputs of all layers (not just the last one)\n",
    "    '''\n",
    "    outputs: List[Vector] = []\n",
    "        \n",
    "    for layer in neural_network:\n",
    "        # Add a constant\n",
    "        input_with_bias = input_vector + [1]\n",
    "        # Compute the output for each neuron\n",
    "        output = [neuron_output(neuron, input_with_bias) for neuron in layer]\n",
    "        # Add to results\n",
    "        outputs.append(output)\n",
    "        \n",
    "        # Then the input to the next layer is the output of this one\n",
    "        input_vector = output\n",
    "        \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it’s easy to build the `XOR gate` that we couldn’t build with a single perceptron. We just need to scale the weights up so that the neuron_outputs are either really close to 0 or really close to 1.\n",
    "\n",
    "<img src=\"images/neural_networks3.png\" alt=\"\" style=\"width: 600px;\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "xor_network = [      # hidden layer \n",
    "    [[20, 20, -30],  # 'and' neuron\n",
    "     [20, 20, -10]], # 'or' neuron\n",
    "                     # output layer \n",
    "    [[-60, 60, -30]]]# '2nd input but not 1st input' neuron\n",
    "\n",
    "# feed_forward returns the outputs of all layers, so the [-1] get the final output,\n",
    "# and the[0] gets the value of the resulting vector\n",
    "\n",
    "assert 0.000 < feed_forward(xor_network, [0, 0])[-1][0] < 0.001 # 0\n",
    "assert 0.999 < feed_forward(xor_network, [1, 0])[-1][0] < 1.000 # 1\n",
    "assert 0.999 < feed_forward(xor_network, [0, 1])[-1][0] < 1.000 # 1\n",
    "assert 0.000 < feed_forward(xor_network, [1, 1])[-1][0] < 0.001 # 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "\n",
    "Usually we don't build neural networks by hand, because we use them to solve bigger problems (image recognition) and because we usually won't be able to 'reason out' what the neurons should be. Instead, we use data to `train neural networks`. The typical approach is an algorithm called **backpropagation**, which uses gradient descent or one of its variants.\n",
    "\n",
    "Imagine we have a training set that consists of input vectors and corresponding target output vectors. For example, in our previous xor_network example, the input vector [1, 0] corresponded to the target output [1]. And imagine that our network has some set of weights. We then `adjust the weights using the following algorithm`:\n",
    "\n",
    "1. Run feed_forward on an input vector to produce the outputs of all the neurons in the network.\n",
    "2. We know the target output, so we can compute a **loss** that's the sum of the squared errors.\n",
    "3. Compute the gradient of this **loss as a function** of the output neuron’s weights (to adjust its weights in the direction that most decreases the error).\n",
    "4. “Propagate” the gradients and errors backward to compute the gradients with respect to the hidden neurons' weights.\n",
    "5. Take a gradient descent step.\n",
    "\n",
    "Typically we run this algorithm many times for our entire training set until the net‐ work converges.\n",
    "\n",
    "- [Gradients and Graphs](https://revisionmaths.com/gcse-maths-revision/algebra/gradients-and-graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sqerror_gradients(network: List[List[Vector]],\n",
    "                      input_vector: Vector,\n",
    "                      target_vector: Vector) -> List[List[Vector]]:\n",
    "    \"\"\"\n",
    "    Given a neural network, an input vector, and a target vector,\n",
    "    make a prediction and compute the gradient of the squared error\n",
    "    loss with respect to the neuron weights.\n",
    "    \"\"\"\n",
    "    # forward pass\n",
    "    hidden_outputs, outputs = feed_forward(network, input_vector)\n",
    "\n",
    "    # gradients with respect to output neuron pre-activation outputs\n",
    "    output_deltas = [output * (1 - output) * (output - target)\n",
    "                     for output, target in zip(outputs, target_vector)]\n",
    "\n",
    "    # gradients with respect to output neuron weights\n",
    "    output_grads = [[output_deltas[i] * hidden_output\n",
    "                     for hidden_output in hidden_outputs + [1]]\n",
    "                    for i, output_neuron in enumerate(network[-1])]\n",
    "\n",
    "    # gradients with respect to hidden neuron pre-activation outputs\n",
    "    hidden_deltas = [hidden_output * (1 - hidden_output) *\n",
    "                         dot(output_deltas, [n[i] for n in network[-1]])\n",
    "                     for i, hidden_output in enumerate(hidden_outputs)]\n",
    "\n",
    "    # gradients with respect to hidden neuron weights\n",
    "    hidden_grads = [[hidden_deltas[i] * input for input in input_vector + [1]]\n",
    "                    for i, hidden_neuron in enumerate(network[0])]\n",
    "\n",
    "    return [hidden_grads, output_grads]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let' train `XOR network`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(0)\n",
    "\n",
    "# training data\n",
    "xs = [[0., 0], [0., 1], [1., 0], [1., 1]]\n",
    "ys = [[0.], [1.], [1.], [0.]]\n",
    "\n",
    "# start with random weights\n",
    "network = [ # hidden layer: 2 inputs -> 2 outputs\n",
    "            [[random.random() for _ in range(2 + 1)],   # 1st hidden neuron\n",
    "             [random.random() for _ in range(2 + 1)]],  # 2nd hidden neuron\n",
    "            # output layer: 2 inputs -> 1 output\n",
    "            [[random.random() for _ in range(2 + 1)]]   # 1st output neuron\n",
    "          ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "neural net for xor: 100%|██████████| 20000/20000 [00:01<00:00, 16798.84it/s]\n"
     ]
    }
   ],
   "source": [
    "from scratch.gradient_descent import gradient_step\n",
    "import tqdm\n",
    "\n",
    "learning_rate = 1.0\n",
    "\n",
    "for epoch in tqdm.trange(20000, desc=\"neural net for xor\"):\n",
    "    for x, y in zip(xs, ys):\n",
    "        gradients = sqerror_gradients(network, x, y)\n",
    "\n",
    "        # Take a gradient step for each neuron in each layer\n",
    "        network = [[gradient_step(neuron, grad, -learning_rate)\n",
    "                    for neuron, grad in zip(layer, layer_grad)]\n",
    "                   for layer, layer_grad in zip(network, gradients)]\n",
    "        \n",
    "# check that it learned XOR\n",
    "assert feed_forward(network, [0, 0])[-1][0] < 0.01\n",
    "assert feed_forward(network, [0, 1])[-1][0] > 0.99\n",
    "assert feed_forward(network, [1, 0])[-1][0] > 0.99\n",
    "assert feed_forward(network, [1, 1])[-1][0] < 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[7.311939329608502, 7.311431630672686, -3.3465806415545014], [5.516539031565446, 5.5163240259285296, -8.444348631309365]], [[12.314993642624385, -12.96068246983362, -5.834848937963502]]]\n"
     ]
    }
   ],
   "source": [
    "print(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[   # hidden layer\n",
    "    [[7, 7, -3],     # computes OR\n",
    "     [5, 5, -8]],    # computes AND\n",
    "    # output layer\n",
    "    [[11, -12, -5]]  # computes \"first but not second\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Activation Functions\n",
    "\n",
    "The **sigmoid function** function has fallen out of favor for a couple of reasons. One reason is that sigmoid(0) equals 1/2, which means that a neuron whose inputs sum is 0 has a positive output. Another is that its gradient is very close to 0 for very large and very small inputs, which means that its gratient can get \"saturated\" and its weights can get stuck.\n",
    "\n",
    "One popular replacement is `tanh (hyperbolic tangent)`, which is a different sigmoid-shaped function that ranges from -1 to 1 and outputs 0 if its input is 0. The derivative of tanh(x) is just 1 - tanh(x) ** 2.\n",
    "\n",
    "<img src=\"images/neural_networks4.png\" alt=\"\" style=\"width: 600px;\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def tanh(x: float) -> float:\n",
    "    # if x is very large or very small, tanh is close to 1 or -1\n",
    "    \n",
    "    # we check for this because, e.g., math.exp(1000) raises an error\n",
    "    if x < -100: return -1\n",
    "    elif x > 100: return 1\n",
    "    \n",
    "    em2x = math.exp(-2 * x)\n",
    "    return (1 - em2x) / (1 + em2x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "OverflowError",
     "evalue": "math range error",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOverflowError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-e0c11add5017>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mOverflowError\u001b[0m: math range error"
     ]
    }
   ],
   "source": [
    "math.exp(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert tanh(0) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.7615941559557649"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tanh(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46211715726000974"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tanh(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In larger networks another popular replacement is **Relu function**, which is 0 for negative inputs and the identity for positive inputs.\n",
    "\n",
    "<img src=\"images/neural_networks5.png\" alt=\"\" style=\"width: 600px;\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x: float) -> float:\n",
    "    return max(x, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relu(-0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relu(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relu(0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
