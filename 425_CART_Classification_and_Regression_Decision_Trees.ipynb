{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "From: https://github.com/ksatola\n",
    "Version: 0.5.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "\n",
    "**CART** - Classification and Regression Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "- [Introduction](#intro)\n",
    "- [Entropy](#entro)\n",
    "- [Example 1: Identify which candidates will interview well](#ex1)\n",
    "- [Example 2: More general approach](#ex2)\n",
    "- [CART - Classification and Regression Trees](#cart)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='intro'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `decision tree` is a predictive modeling tool which uses a tree structure to represent a number of possible `decision paths` and an outcome for each path.\n",
    "\n",
    "<img src=\"images/tree1.png\" alt=\"\" style=\"width: 600px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision trees have a lot to recommend them. They’re `very easy to understand and interpret`, and the process by which they reach a prediction is completely transparent. Unlike the other models, decision trees `can easily handle a mix of numeric` (e.g., number of legs) `and categorical` (e.g., delicious/not delicious) `attributes and can even classify data for which attributes are missing`.\n",
    "\n",
    "At the same time, finding an “optimal” decision tree for a set of training data is computationally a very hard problem. (We will get around this by trying to build a goodenough tree rather than an optimal one, although for large data sets this can still be a lot of work.) More important, `it is very easy (and very bad) to build decision trees that are overfitted to the training data`, and that don’t generalize well to unseen data. We’ll look at ways to address this.\n",
    "\n",
    "Most people divide decision trees into `classification trees` (which produce categorical outputs) and `regression trees` (which produce numeric outputs). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='entro'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy\n",
    "\n",
    "In order to build a `decision tree`, we will need to decide what questions to ask and in what order. At each stage of the tree there are some possibilities we’ve eliminated and some that we haven’t. Every possible question partitions the remaining possibilities according to their answers.\n",
    "\n",
    "Ideally, `we’d like to choose questions whose answers give a lot of information about what our tree should predict`. If there’s a single yes/no question for which “yes” answers always correspond to True outputs and “no” answers to False outputs (or vice versa), this would be an awesome question to pick.\n",
    "\n",
    "We capture this notion of “how much information” with `entropy` (think \"uncertainty\"). You have probably heard this used to mean disorder. We use it to represent the uncertainty associated with data.\n",
    "\n",
    "Imagine that we have a set S of data, each member of which is labeled as belonging to one of a finite number of classes C1, ..., Cn. If all the data points belong to a single class, then there is no real uncertainty, which means we’d like there to be `low entropy`. If the data points are evenly spread across the classes, there is a lot of uncertainty and we’d like there to be `high entropy`.\n",
    "\n",
    "<img src=\"images/tree2.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "Each term `−pi log2 pi` is non-negative and is close to zero precisely when pi is either close to zero or close to one.\n",
    "\n",
    "<img src=\"images/tree3.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "This means the `entropy will be small` when every `pi` is close to 0 or 1 (i.e., when most of the data is in a single class), and it `will be larger` when many of the `pi`’s are not close to 0 (i.e., when the data is spread across multiple classes). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import math\n",
    "\n",
    "def entropy(class_probabilities: List[float]) -> float:\n",
    "    \"\"\"given a list of class probabilities, compute the entropy\"\"\"\n",
    "    return sum(-p * math.log(p,2) \n",
    "               for p in class_probabilities\n",
    "              if p > 0) # ignore zero probabilities\n",
    "\n",
    "assert entropy([1.0]) == 0\n",
    "assert entropy([0.5, 0.5]) == 1\n",
    "assert 0.81 < entropy([0.25, 0.75]) < 0.82\n",
    "assert entropy([0.25, 0.75]) == entropy([0.75, 0.25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5834674497121084"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy([0.33, 0.33, 0.33])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.321928094887362"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy([0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.1609640474436813"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy([0.1, 0.1, 0.1, 0.1, 0.1, 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3219280948873626"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy([0.1, 0.1, 0.2, 0.1, 0.1, 0.4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "from collections import Counter\n",
    "\n",
    "def class_probabilities(labels: List[Any]) -> List[float]:\n",
    "    \"\"\"return the frewquency of class appearances in a list\"\"\"\n",
    "    total_count = len(labels)\n",
    "    return [count / total_count for count in Counter(labels).values()]\n",
    "\n",
    "def data_entropy(labels: List[Any]) -> float:\n",
    "    return entropy(class_probabilities(labels))\n",
    "\n",
    "assert data_entropy(['a']) == 0\n",
    "assert data_entropy(['True', 'False']) == 1\n",
    "assert data_entropy([3, 4, 4, 4]) == entropy([0.25, 0.75])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each stage of a decision tree involves asking a question whose answer partitions data into one or (hopefully) more subsets. We’d like some notion of the `entropy` that results from partitioning a set of data in a certain way. \n",
    "\n",
    "We want a partition to have:\n",
    "- **low entropy** if it splits the data into subsets that themselves have low entropy (i.e., are highly certain), \n",
    "- and **high entropy** if it contains subsets that (are large and) have high entropy (i.e., are highly uncertain)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_entropy(subsets: List[List[Any]]) -> float:\n",
    "    \"\"\"returns the entropy from this partition of data into subsets\"\"\"\n",
    "    total_count = sum(len(subset) for subset in subsets)\n",
    "    return sum(data_entropy(subset) * len(subset) / total_count\n",
    "              for subset in subsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One problem with this approach is that partitioning by an attribute with many different values will result in a `very low entropy due to overfitting`. For example, imagine you work for a bank and are trying to build a decision tree to predict which of your customers are likely to default on their mortgages, using some historical data as your training set. Imagine further that the data set contains each customer’s Social Security number. Partitioning on SSN will produce one-person subsets, each of which necessarily has zero entropy. But a model that relies on SSN is certain not to generalize beyond the training set. For this reason, you should probably `try to avoid (or bucket, if appropriate) attributes with large numbers of possible values when creating decision trees`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='ex1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Identify which candidates will interview well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple, Optional\n",
    "\n",
    "class Candidate(NamedTuple):\n",
    "    level: str\n",
    "    lang: str\n",
    "    tweets: bool\n",
    "    phd: bool\n",
    "    did_well: Optional[bool] = None # allow unlabeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "                  #  level     lang     tweets  phd  did_well\n",
    "inputs = [Candidate('Senior', 'Java',   False, False, False),\n",
    "          Candidate('Senior', 'Java',   False, True,  False),\n",
    "          Candidate('Mid',    'Python', False, False, True),\n",
    "          Candidate('Junior', 'Python', False, False, True),\n",
    "          Candidate('Junior', 'R',      True,  False, True),\n",
    "          Candidate('Junior', 'R',      True,  True,  False),\n",
    "          Candidate('Mid',    'R',      True,  True,  True),\n",
    "          Candidate('Senior', 'Python', False, False, False),\n",
    "          Candidate('Senior', 'R',      True,  False, True),\n",
    "          Candidate('Junior', 'Python', True,  False, True),\n",
    "          Candidate('Senior', 'Python', True,  True,  True),\n",
    "          Candidate('Mid',    'Python', False, True,  True),\n",
    "          Candidate('Mid',    'Java',   True,  False, True),\n",
    "          Candidate('Junior', 'Python', False, True,  False)\n",
    "         ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our tree will consist of `decision nodes` (which ask a question and direct us differently depending on the answer) and `leaf nodes` (which give us a prediction). We will build it using the relatively simple `ID3 algorithm`, which operates in the following manner. Let’s say we’re given some labeled data, and a list of attributes to consider branching on.\n",
    "\n",
    "- If the data all have the same label, then create a `leaf node` that predicts that label and then stop.\n",
    "- If the list of attributes is empty (i.e., there are no more possible questions to ask), then create a `leaf node` that predicts the most common label and then stop.\n",
    "- Otherwise, try partitioning the data by each of the attributes\n",
    "- Choose the partition with the lowest partition entropy\n",
    "- Add a `decision node` based on the chosen attribute\n",
    "- Recur on each partitioned subset using the remaining attributes\n",
    "\n",
    "This is what’s known as a `“greedy” algorithm` because, at each step, it chooses the most immediately best option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "level 0.6935361388961919\n",
      "lang 0.8601317128547441\n",
      "tweets 0.7884504573082896\n",
      "phd 0.8921589282623617\n"
     ]
    }
   ],
   "source": [
    "# Find the partition with the least entropy\n",
    "# first create partitioning function\n",
    "from typing import Dict, TypeVar\n",
    "from collections import defaultdict\n",
    "\n",
    "T = TypeVar('T') # generic type for inputs\n",
    "\n",
    "def partition_by(inputs: List[T], attribute: str) -> Dict[Any, List[T]]:\n",
    "    \"\"\"Partition the inputs into lists based on the specified attribute.\"\"\"\n",
    "    partitions: Dict[Any, List[T]] = defaultdict(list)\n",
    "    for input in inputs:\n",
    "        key = getattr(input, attribute)  # value of the specified attribute\n",
    "        partitions[key].append(input)    # add input to the correct partition\n",
    "    return partitions\n",
    "\n",
    "# then another for computing entropy\n",
    "def partition_entropy_by(inputs: List[Any],\n",
    "                         attribute: str,\n",
    "                         label_attribute: str) -> float:\n",
    "    \"\"\"Compute the entropy corresponding to the given partition\"\"\"\n",
    "    # partitions consist of our inputs\n",
    "    partitions = partition_by(inputs, attribute)\n",
    "\n",
    "    # but partition_entropy needs just the class labels\n",
    "    labels = [[getattr(input, label_attribute) for input in partition]\n",
    "              for partition in partitions.values()]\n",
    "\n",
    "    return partition_entropy(labels)\n",
    "\n",
    "# Then we just find the minimum-entropy partition for the whole data set:\n",
    "for key in ['level','lang','tweets','phd']: \n",
    "    print(key, partition_entropy_by(inputs, key, 'did_well'))\n",
    "    \n",
    "assert 0.69 < partition_entropy_by(inputs, 'level', 'did_well')  < 0.70\n",
    "assert 0.86 < partition_entropy_by(inputs, 'lang', 'did_well')   < 0.87\n",
    "assert 0.78 < partition_entropy_by(inputs, 'tweets', 'did_well') < 0.79\n",
    "assert 0.89 < partition_entropy_by(inputs, 'phd', 'did_well')    < 0.90"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`The lowest entropy` comes from splitting on `level`, so we need to make a subtree for each possible `level` value. The next step is to follow the `Senior` branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "level 0.9709505944546686\n",
      "lang 0.4\n",
      "tweets 0.0\n",
      "phd 0.9509775004326938\n"
     ]
    }
   ],
   "source": [
    "senior_inputs = [input for input in inputs if input.level == 'Senior']\n",
    "#print(senior_inputs)\n",
    "\n",
    "for key in ['level','lang','tweets','phd']: \n",
    "    print(key, partition_entropy_by(senior_inputs, key, 'did_well'))\n",
    "\n",
    "assert 0.4 == partition_entropy_by(senior_inputs, 'lang', 'did_well')\n",
    "assert 0.0 == partition_entropy_by(senior_inputs, 'tweets', 'did_well')\n",
    "assert 0.95 < partition_entropy_by(senior_inputs, 'phd', 'did_well') < 0.96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "level 0\n",
      "lang 0\n",
      "tweets 0\n",
      "phd 0\n"
     ]
    }
   ],
   "source": [
    "# tweets have the lowest entropy\n",
    "tweets_inputs = [input for input in senior_inputs if input.level == 'True']\n",
    "print(tweets_inputs)\n",
    "\n",
    "for key in ['level','lang','tweets','phd']: \n",
    "    print(key, partition_entropy_by(tweets_inputs, key, 'did_well'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "level 0.0\n",
      "lang 0.0\n",
      "tweets 0.0\n",
      "phd 0.0\n"
     ]
    }
   ],
   "source": [
    "senior_inputs = [input for input in inputs if input.level == 'Mid']\n",
    "#print(senior_inputs)\n",
    "\n",
    "for key in ['level','lang','tweets','phd']: \n",
    "    print(key, partition_entropy_by(senior_inputs, key, 'did_well'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "level 0.9709505944546686\n",
      "lang 0.9509775004326938\n",
      "tweets 0.9509775004326938\n",
      "phd 0.0\n"
     ]
    }
   ],
   "source": [
    "senior_inputs = [input for input in inputs if input.level == 'Junior']\n",
    "#print(senior_inputs)\n",
    "\n",
    "for key in ['level','lang','tweets','phd']: \n",
    "    print(key, partition_entropy_by(senior_inputs, key, 'did_well'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "level 0\n",
      "lang 0\n",
      "tweets 0\n",
      "phd 0\n"
     ]
    }
   ],
   "source": [
    "phd_inputs = [input for input in inputs if input.level == 'phd']\n",
    "print(phd_inputs)\n",
    "\n",
    "for key in ['level','lang','tweets','phd']: \n",
    "    print(key, partition_entropy_by(phd_inputs, key, 'did_well'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/tree4.png\" alt=\"\" style=\"width: 600px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='ex2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: More general approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple, Union, Any\n",
    "\n",
    "class Leaf(NamedTuple):\n",
    "    value: Any\n",
    "\n",
    "class Split(NamedTuple):\n",
    "    attribute: str\n",
    "    subtrees: dict\n",
    "    default_value: Any = None\n",
    "\n",
    "DecisionTree = Union[Leaf, Split]\n",
    "\n",
    "hiring_tree = Split('level', {   # First, consider \"level\".\n",
    "    'Junior': Split('phd', {     # if level is \"Junior\", next look at \"phd\"\n",
    "        False: Leaf(True),       #   if \"phd\" is False, predict True\n",
    "        True: Leaf(False)        #   if \"phd\" is True, predict False\n",
    "    }),\n",
    "    'Mid': Leaf(True),           # if level is \"Mid\", just predict True\n",
    "    'Senior': Split('tweets', {  # if level is \"Senior\", look at \"tweets\"\n",
    "        False: Leaf(False),      #   if \"tweets\" is False, predict False\n",
    "        True: Leaf(True)         #   if \"tweets\" is True, predict True\n",
    "    })\n",
    "})\n",
    "\n",
    "def classify(tree: DecisionTree, input: Any) -> Any:\n",
    "    \"\"\"classify the input using the given decision tree\"\"\"\n",
    "\n",
    "    # If this is a leaf node, return its value\n",
    "    if isinstance(tree, Leaf):\n",
    "        return tree.value\n",
    "\n",
    "    # Otherwise this tree consists of an attribute to split on\n",
    "    # and a dictionary whose keys are values of that attribute\n",
    "    # and whose values of are subtrees to consider next\n",
    "    subtree_key = getattr(input, tree.attribute)\n",
    "\n",
    "    if subtree_key not in tree.subtrees:   # If no subtree for key,\n",
    "        return tree.default_value          # return the default value.\n",
    "\n",
    "    subtree = tree.subtrees[subtree_key]   # Choose the appropriate subtree\n",
    "    return classify(subtree, input)        # and use it to classify the input.\n",
    "\n",
    "def build_tree_id3(inputs: List[Any],\n",
    "                   split_attributes: List[str],\n",
    "                   target_attribute: str) -> DecisionTree:\n",
    "    # Count target labels\n",
    "    label_counts = Counter(getattr(input, target_attribute)\n",
    "                           for input in inputs)\n",
    "    most_common_label = label_counts.most_common(1)[0][0]\n",
    "\n",
    "    # If there's a unique label, predict it\n",
    "    if len(label_counts) == 1:\n",
    "        return Leaf(most_common_label)\n",
    "\n",
    "    # If no split attributes left, return the majority label\n",
    "    if not split_attributes:\n",
    "        return Leaf(most_common_label)\n",
    "\n",
    "    # Otherwise split by the best attribute\n",
    "\n",
    "    def split_entropy(attribute: str) -> float:\n",
    "        \"\"\"Helper function for finding the best attribute\"\"\"\n",
    "        return partition_entropy_by(inputs, attribute, target_attribute)\n",
    "\n",
    "    best_attribute = min(split_attributes, key=split_entropy)\n",
    "\n",
    "    partitions = partition_by(inputs, best_attribute)\n",
    "    new_attributes = [a for a in split_attributes if a != best_attribute]\n",
    "\n",
    "    # recursively build the subtrees\n",
    "    subtrees = {attribute_value : build_tree_id3(subset,\n",
    "                                                 new_attributes,\n",
    "                                                 target_attribute)\n",
    "                for attribute_value, subset in partitions.items()}\n",
    "\n",
    "    return Split(best_attribute, subtrees, default_value=most_common_label)\n",
    "\n",
    "tree = build_tree_id3(inputs,\n",
    "                      ['level', 'lang', 'tweets', 'phd'],\n",
    "                      'did_well')\n",
    "\n",
    "# Should predict True\n",
    "assert classify(tree, Candidate(\"Junior\", \"Java\", True, False))\n",
    "\n",
    "# Should predict False\n",
    "assert not classify(tree, Candidate(\"Junior\", \"Java\", True, True))\n",
    "\n",
    "# Should predict True\n",
    "assert classify(tree, Candidate(\"Intern\", \"Java\", True, True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='cart'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CART - Classification and Regression Trees\n",
    "\n",
    "The CART acronym was introduced by Leo Breiman to refer to Decision Tree algorithms that can be used for classification or regression predictive modeling problems. In this example we use a greedy approach called `recursive binary splitting`. This is a numerical procedure where all the values are lined up and different split points are tried and tested using a cost function. The split with the lowest cost is selected. All input variables and all split points are evaluated and chosen in a greedy manner based on the cost function.\n",
    "\n",
    "- **Regression** - the cost function that is minimized to choose split points is the `Sum Squared Error (SSE)` accross all training samples that fall within the rectangle.\n",
    "- **Classification** - the `Gini Index` cost function is used which provides an indication of how pure the nodes are, where the purity refers to how mixed the training data assigned to each node is.\n",
    "\n",
    "Splitting continues until nodes contain a minimum number of training examples or a maximum tree depth is reached."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Banknote Dataset: http://goo.gl/rLPrHO\n",
    "# More information: http://archive.ics.uci.edu/ml/datasets/banknote+authentication\n",
    "path = '../data/jbmla/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gini Index\n",
    "\n",
    "The `Gini Index` is the name of the cost function used to evaluate splits in the dataset. A Gini score gives an idea of how good a split is by how mixed the classes are in the two groups created by the split. A perfect separation results in a Gini score of 0, whereas the worst case split that results in 50/50 classes in each group results in a Gini score of 0.5 (for a 2 class problem).\n",
    "\n",
    "<img src=\"images/jb-gini-weighted.png\" alt=\"\" style=\"width: 400px;\"/>\n",
    "\n",
    "Where `n` is number of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def gini_index(groups:List, classes: List):\n",
    "    '''\n",
    "    Calculate the Gini index for a split dataset.\n",
    "    :param A list of groups of observations.\n",
    "    :param A list of known class values.\n",
    "    '''\n",
    "    # Count all samples at split point\n",
    "    n_instances = float(sum([len(group) for group in groups]))\n",
    "    #print(f\"n_instances: {n_instances}\")\n",
    "    \n",
    "    # Sum weighted Gini index for each group\n",
    "    gini = 0.0\n",
    "    \n",
    "    for group in groups:\n",
    "        #print(f\"group: {group}\")\n",
    "        \n",
    "        size = float(len(group))\n",
    "        #print(f\"size: {size}\")\n",
    "        \n",
    "        # Avoid divide by zero\n",
    "        if size == 0:\n",
    "            continue\n",
    "        score = 0.0\n",
    "        \n",
    "        # Score the group based on the score for each class\n",
    "        for class_val in classes:\n",
    "            #print(f\"class_val: {class_val}\")\n",
    "            \n",
    "            # Calculate the proportion of classes in each group\n",
    "            # row[-1] - get the last element of a list, it gets a class assigned to an attribute\n",
    "            proportion = [row[-1] for row in group].count(class_val) / size\n",
    "            #print(f\"proportion: {proportion}\")\n",
    "            \n",
    "            score += proportion*proportion\n",
    "            #print(f\"score: {score}\")\n",
    "        \n",
    "        gini += (1.0 - score) * (size / n_instances)\n",
    "        #print(f\"gini: {gini}\")\n",
    "            \n",
    "    return gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_attr = 1\n",
    "attr_value = 0 # to which class the attribute belongs\n",
    "row1 = [input_attr, attr_value]\n",
    "row2 = [1, 0]\n",
    "group1 = [row1, \n",
    "          row2] # 2 first rows\n",
    "group2 = [[0, 0], \n",
    "          [0, 0]] # 2 last rows\n",
    "groups = [group1, group2]\n",
    "clases = [0, 1]\n",
    "gini_index(groups, clases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# The worst Gini score is 0.5 (for binary classification)\n",
    "print(gini_index([[[1, 1], [1, 0]], [[1, 1], [1, 0]]], [0, 1]))\n",
    "\n",
    "# The best Gini score is 0\n",
    "print(gini_index([[[1, 0], [1, 0]], [[1, 1], [1, 1]]], [0, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Split\n",
    "\n",
    "Now that we know how to evaluate the results of a split, ;et's look at creating splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
