{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Features** are whatever inputs we provide to our model.\n",
    "\n",
    "In the simplest case, features are simply given to you. You can transform them (adding their squared, cubed... versions if that helps you build a better model). In other cases, features can be extracted (for example from text like in NLP) and then potentially encoded:\n",
    "\n",
    "- `The Naive Bayes classifier` is suited to yes-or-no features.\n",
    "- `Regression models` require numeric features (which could include dummy variables that are 0s and 1s).\n",
    "- `Decision trees` can deal with numeric or categorical data.\n",
    "\n",
    "In some cases we may also want to remove features or reduce them into a handful important dimensions (`dimensionality reduction`). We can also use a `regularization` technique that penalizes models the more features they use.\n",
    "\n",
    "While choosing features, a combination of experience and domain expertise comes into play, but in general you will have to try different things, which is part of the fun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
