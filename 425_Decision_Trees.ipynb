{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `decision tree` is a predictive modeling tool which uses a tree structure to represent a number of possible `decision paths` and an outcome for each path.\n",
    "\n",
    "<img src=\"images/tree1.png\" alt=\"\" style=\"width: 600px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision trees have a lot to recommend them. They’re `very easy to understand and interpret`, and the process by which they reach a prediction is completely transparent. Unlike the other models, decision trees `can easily handle a mix of numeric` (e.g., number of legs) `and categorical` (e.g., delicious/not delicious) `attributes and can even classify data for which attributes are missing`.\n",
    "\n",
    "At the same time, finding an “optimal” decision tree for a set of training data is computationally a very hard problem. (We will get around this by trying to build a goodenough tree rather than an optimal one, although for large data sets this can still be a lot of work.) More important, `it is very easy (and very bad) to build decision trees that are overfitted to the training data`, and that don’t generalize well to unseen data. We’ll look at ways to address this.\n",
    "\n",
    "Most people divide decision trees into `classification trees` (which produce categorical outputs) and `regression trees` (which produce numeric outputs). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy\n",
    "\n",
    "In order to build a `decision tree`, we will need to decide what questions to ask and in what order. At each stage of the tree there are some possibilities we’ve eliminated and some that we haven’t. Every possible question partitions the remaining possibilities according to their answers.\n",
    "\n",
    "Ideally, `we’d like to choose questions whose answers give a lot of information about what our tree should predict`. If there’s a single yes/no question for which “yes” answers always correspond to True outputs and “no” answers to False outputs (or vice versa), this would be an awesome question to pick.\n",
    "\n",
    "We capture this notion of “how much information” with `entropy` (think \"uncertainty\"). You have probably heard this used to mean disorder. We use it to represent the uncertainty associated with data.\n",
    "\n",
    "Imagine that we have a set S of data, each member of which is labeled as belonging to one of a finite number of classes C1, ..., Cn. If all the data points belong to a single class, then there is no real uncertainty, which means we’d like there to be `low entropy`. If the data points are evenly spread across the classes, there is a lot of uncertainty and we’d like there to be `high entropy`.\n",
    "\n",
    "<img src=\"images/tree2.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "Each term `−pi log2 pi` is non-negative and is close to zero precisely when pi is either close to zero or close to one.\n",
    "\n",
    "<img src=\"images/tree3.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "This means the `entropy will be small` when every `pi` is close to 0 or 1 (i.e., when most of the data is in a single class), and it `will be larger` when many of the `pi`’s are not close to 0 (i.e., when the data is spread across multiple classes). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import math\n",
    "\n",
    "def entropy(class_probabilities: List[float]) -> float:\n",
    "    \"\"\"given a list of class probabilities, compute the entropy\"\"\"\n",
    "    return sum(-p * math.log(p,2) \n",
    "               for p in class_probabilities\n",
    "              if p > 0) # ignore zero probabilities\n",
    "\n",
    "assert entropy([1.0]) == 0\n",
    "assert entropy([0.5, 0.5]) == 1\n",
    "assert 0.81 < entropy([0.25, 0.75]) < 0.82\n",
    "assert entropy([0.25, 0.75]) == entropy([0.75, 0.25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5834674497121084"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy([0.33, 0.33, 0.33])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.321928094887362"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy([0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.1609640474436813"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy([0.1, 0.1, 0.1, 0.1, 0.1, 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3219280948873626"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy([0.1, 0.1, 0.2, 0.1, 0.1, 0.4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "from collections import Counter\n",
    "\n",
    "def class_probabilities(labels: List[Any]) -> List[float]:\n",
    "    \"\"\"return the frewquency of class appearances in a list\"\"\"\n",
    "    total_count = len(labels)\n",
    "    return [count / total_count for count in Counter(labels).values()]\n",
    "\n",
    "def data_entropy(labels: List[Any]) -> float:\n",
    "    return entropy(class_probabilities(labels))\n",
    "\n",
    "assert data_entropy(['a']) == 0\n",
    "assert data_entropy(['True', 'False']) == 1\n",
    "assert data_entropy([3, 4, 4, 4]) == entropy([0.25, 0.75])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each stage of a decision tree involves asking a question whose answer partitions data into one or (hopefully) more subsets. We’d like some notion of the `entropy` that results from partitioning a set of data in a certain way. \n",
    "\n",
    "We want a partition to have:\n",
    "- **low entropy** if it splits the data into subsets that themselves have low entropy (i.e., are highly certain), \n",
    "- and **high entropy** if it contains subsets that (are large and) have high entropy (i.e., are highly uncertain)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_entropy(subsets: List[List[Any]]) -> float:\n",
    "    \"\"\"returns the entropy from this partition of data into subsets\"\"\"\n",
    "    total_count = sum(len(subset) * len(subset) / total_count\n",
    "                     for subset in subsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One problem with this approach is that partitioning by an attribute with many different values will result in a `very low entropy due to overfitting`. For example, imagine you work for a bank and are trying to build a decision tree to predict which of your customers are likely to default on their mortgages, using some historical data as your training set. Imagine further that the data set contains each customer’s Social Security number. Partitioning on SSN will produce one-person subsets, each of which necessarily has zero entropy. But a model that relies on SSN is certain not to generalize beyond the training set. For this reason, you should probably `try to avoid (or bucket, if appropriate) attributes with large numbers of possible values when creating decision trees`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Identify which candidates will interview well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple, Optional\n",
    "\n",
    "class Candidate(NamedTuple):\n",
    "    level: str\n",
    "    lang: str\n",
    "    tweets: bool\n",
    "    phd: bool\n",
    "    did_well: Optional[bool] = None # allow unlabeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
